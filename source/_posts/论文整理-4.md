---
title: 论文整理
date: 2023-04-06 19:06:12
tags: 深度学习基础
categories: 论文整理
---

# 综述

## NatureDeepReview                                  doi:10.1038/nature14539

深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应该如何改变其内部参数。深度卷积网络在图像、视频、语音和音频处理方面取得了突破，而循环网络则在文本和语音等顺序数据方面大出风头。

机器学习最常见的形式，无论深度与否，都是监督学习。

我们计算一个目标函数来测量输出分数和期望分数模式之间的误差。然后，机器修改其内部可调参数，以减少这种误差。这些可调参数，通常称为权重。

在实践中，大多数从业者使用一种称为随机梯度下降(SGD)的程序。这包括显示几个例子的输入向量，计算输出和误差，计算这些例子的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复这个过程，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本集对所有样本的平均梯度给出了一个有噪声的估计。与复杂得多的优化技术相比，这个简单的程序通常能惊人地快速找到一组良好的权重。

<img title="" src="file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-16-39-5a49a75876192c9a1ea19b821a5521c.png" alt="" width="303">     <img title="" src="file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-17-24-3a42b78e655362bd8b6b86aeab1d0bb.png" alt="" width="314">

c，用于计算具有两个隐藏层和一个输出层的神经网络中的前向传递的方程，每个输出层构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入z，这是下一层单元输出的加权和。然后对z应用非线性函数f(.)来得到单位的输出。为简单起见，我们省略了偏差项。

神经网络中使用的非线性函数包括近年来常用的整流线性单元(ReLU) f(z) = max(0,z)，以及更传统的sigmoids，如双曲正切，f(z) =(exp(z)−exp(−z))/(exp(z)+exp(−z))和logistic函数logistic, f(z) =1/(1 +exp(−z))。

d，用于计算向后传递的方程。在每个隐藏层，我们计算关于每个单元输出的误差导数，这是关于上面一层中单元的总输入的误差导数的加权和。然后通过乘以f(z)的梯度，将输出的误差导数转换为输入的误差导数。

在输出层，相对于单位输出的误差导数通过对成本函数求导来计算。如果单位l的成本函数为0.5(yl−tl) 2，则得到yl−tl，其中tl是目标值。一旦∂E/∂zk是已知的，从下面一层的单位j开始的连接的权值wjk的误差导数就是yj∂E/∂zk。

使用通用的学习过程自动学习好的特性

隐藏层可以被视为以非线性方式扭曲输入

There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.

深度神经网络利用了许多自然信号都是组合层次结构的特性，其中高级特征是通过组合低级特征来获得的。

深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络具有两种不同的指数优势。这两个优点都来自组合的强大功能，并依赖于底层数据生成分布具有适当的组件结构。首先，学习分布式表示能够泛化到学习到的特征值的新组合，而不是在训练中看到的那些(例如，n个二进制特征可以有2^n个组合)。其次，在一个深度网络中组成表示层带来了另一个指数优势(深度指数)的潜力。

# 历史

## Reducing the Dimensionality of Data with Neural Networks

## 28 JULY 2006 VOL 313 SCIENCE www.sciencemag.org

降维有利于高维数据的分类、可视化、通信和存储。一种简单而广泛使用的方法是主成分分析。本文提出了。。。。。

在具有多个隐藏层(2-4层)的非线性自编码器中，权值的优化非常困难。只有初始权重接近一个好的解，梯度下降才能很好地工作，但找到这样的初始权重需要一种非常不同的算法，需要每次学习一层特征。我们介绍了这个用于二进制数据的预训练过程，并将其推广到实值数据，并表明它适用于各种数据集。
