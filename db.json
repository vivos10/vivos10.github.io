{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/dist/APlayer.min.css","path":"dist/APlayer.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/dist/APlayer.min.css.map","path":"dist/APlayer.min.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/dist/APlayer.min.js","path":"dist/APlayer.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/dist/APlayer.min.js.map","path":"dist/APlayer.min.js.map","modified":1,"renderable":1},{"_id":"themes/next/source/dist/music.js","path":"dist/music.js","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/back.jpg","path":"images/back.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/background.jpg","path":"images/background.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/music.jpg","path":"images/music.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/music2.jpg","path":"images/music2.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/wechatpay.jpg","path":"images/wechatpay.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/wechatpay.png","path":"images/wechatpay.png","modified":1,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","path":"lib/canvas-nest/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/README.md","path":"lib/canvas-nest/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest-nomobile.min.js","path":"lib/canvas-nest/canvas-nest-nomobile.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/LICENSE","path":"lib/three/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/README.md","path":"lib/three/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/gulpfile.js","path":"lib/three/gulpfile.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/package.json","path":"lib/three/package.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/renovate.json","path":"lib/three/renovate.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/lib/CanvasRenderer.js","path":"lib/three/lib/CanvasRenderer.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/lib/Projector.js","path":"lib/three/lib/Projector.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/src/canvas_lines.js","path":"lib/three/src/canvas_lines.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/src/canvas_sphere.js","path":"lib/three/src/canvas_sphere.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/src/three-waves.js","path":"lib/three/src/three-waves.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_data/styles.styl","hash":"a0c70497ab52b330a1acb06a81465dfc7d110280","modified":1675423913207},{"_id":"source/tags/index.md","hash":"d636041d423f91e046d5e025ca67ee472f41f4e9","modified":1677571643659},{"_id":"source/_posts/Bilibili-yolo基础.md","hash":"3ce0c42260466b24a6c1d1def6549625be5c8da1","modified":1682509112408},{"_id":"source/_posts/PLC.md","hash":"c7e815841354525e163c6b7170dfdbf3f9633842","modified":1681476804940},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1674638753676},{"_id":"source/_posts/产品表面缺陷视觉检测数据处理关键技术研究.md","hash":"79a02a0c986494445313517b30e4ce6a48315fca","modified":1688479109725},{"_id":"source/_posts/yolov5代码分析.md","hash":"2da2d5ead8a29b6b15bf30346205566338abc3cb","modified":1684502136109},{"_id":"source/_posts/去看论文吧.md","hash":"33884f3d42dc88577fb515818d65f4d4341cab9b","modified":1678616039341},{"_id":"source/_posts/基于图像的三维重建——基于图像的立体视觉研究.md","hash":"a14e6d35619335a44c650a9275c8130a59d82a40","modified":1686403274938},{"_id":"source/_posts/基于深度学习的图像识别.md","hash":"bc30ce3663ef8dd974945d262b45ad218ea17749","modified":1680061405230},{"_id":"source/_posts/基于结构光扫描的三维点云数据重构算法研究.md","hash":"e131438e5f469d858152f55ceab42a17951fe45e","modified":1686404039428},{"_id":"source/_posts/搭建深度学习环境.md","hash":"ff93dfca73edca5cc7eac9bfd05085cce97f1f70","modified":1682508784065},{"_id":"source/_posts/搭建yolov5框架.md","hash":"34c4d785505efa69a0baa9d61526a98e3a9f5ac7","modified":1682508784064},{"_id":"source/_posts/方向研究.md","hash":"de91e5e4238d35e7f40c2a6e4979416e4236a9b1","modified":1684501890752},{"_id":"source/_posts/时间序列.md","hash":"88e27d509422d16187e5b92f0f1039683bb88aa2","modified":1684755784272},{"_id":"source/_posts/添加音乐.md","hash":"86ac7375b0908c4d4cbfde4a3c3956154dcdc193","modified":1677910264523},{"_id":"source/_posts/论文整理-10.md","hash":"1497f639c51d6f3a908027caf117b6d91d6048aa","modified":1688528541711},{"_id":"source/_posts/论文整理-2.md","hash":"f509aa5ad527bc275cc195f9e9763d8f6d0a9f61","modified":1682508784159},{"_id":"source/_posts/论文整理-3.md","hash":"a132f1e0a580214af2ee28b6336246d9a8ab28b4","modified":1682508784159},{"_id":"source/_posts/论文整理-4.md","hash":"40520895de58e6be93b4b0ec02392a6dee35c835","modified":1681993741822},{"_id":"source/_posts/论文整理-5.md","hash":"df115cad42ead56038322459e646d5c745252f92","modified":1685966869073},{"_id":"source/_posts/论文整理-6.md","hash":"74897b059f20819c342f7073a86fd24ebc9dbe76","modified":1685966866655},{"_id":"source/_posts/论文整理-8.md","hash":"2ae6d70c9496bb5a93242a4148bbd02970d27344","modified":1684069713282},{"_id":"source/_posts/论文整理-7.md","hash":"b41b6240d3c938dbb7bafd251bc664bbbb35fe5b","modified":1683258051124},{"_id":"source/_posts/论文整理-9.md","hash":"b7fa7a96e4c5c931de7956009cece747f9f7051f","modified":1686988216492},{"_id":"source/_posts/论文整理.md","hash":"5df93d9b0ec760382acaff8c938a5d0ac5dbdacc","modified":1682508784158},{"_id":"source/categories/index.md","hash":"328d4be2fa82130fb2d0787eb163cb0a8fb3bb11","modified":1678181089751},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1675259119176},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1675259119177},{"_id":"themes/next/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1675259119177},{"_id":"themes/next/.gitignore","hash":"83418530da80e6a78501e1d62a89c3bf5cbaec3d","modified":1675259119184},{"_id":"themes/next/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1675259119184},{"_id":"themes/next/.travis.yml","hash":"379f31a140ce41e441442add6f673bf397d863ea","modified":1675259119184},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1675259119185},{"_id":"themes/next/README.md","hash":"7d56751b580d042559b2acf904fca4b42bcb30a7","modified":1675259119185},{"_id":"themes/next/_config.yml","hash":"e69e13f0340dab9cc2421c42b9ee19a2dfc5e3f8","modified":1677572636713},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1675259119186},{"_id":"themes/next/gulpfile.js","hash":"0c76a1ac610ee8cbe8e2cc9cca1c925ffd0edf98","modified":1675259119201},{"_id":"themes/next/package.json","hash":"b099e7cea4406e209130410d13de87988ba37b2a","modified":1675259119253},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"778b7e052993ed59f21ed266ba7119ee2e5253fb","modified":1675259119178},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ddde54fb50d11dc08cec899a3588addb56aa386","modified":1675259119178},{"_id":"themes/next/.github/config.yml","hash":"df3d970700e6b409edc3d23be8d553db78d5ba3f","modified":1675259119180},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"d2f8e6b65783e31787feb05d2ccea86151f53f35","modified":1675259119180},{"_id":"themes/next/.github/issue-close-app.yml","hash":"b14756e65546eb9ecc9d4393f0c9a84a3dac1824","modified":1675259119182},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1675259119182},{"_id":"themes/next/.github/lock.yml","hash":"3ce3d0a26030a1cd52b273cc6a6d444d7c8d85c2","modified":1675259119182},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1675259119182},{"_id":"themes/next/.github/stale.yml","hash":"590b65aca710e0fba75d3cf5361a64d13b6b0f63","modified":1675259119183},{"_id":"themes/next/.github/release-drafter.yml","hash":"09c3352b2d643acdc6839601ceb38abc38ab97c5","modified":1675259119183},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1675259119183},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1675259119187},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"60c7e9ef0c578deebad43e9395c958fa61096baf","modified":1675259119187},{"_id":"themes/next/docs/AUTHORS.md","hash":"cde7cc095ac31b421a573042cf61060f90d9ad0d","modified":1675259119187},{"_id":"themes/next/docs/INSTALLATION.md","hash":"07ea00bee149a1bdc9073e903ee6b411e9f2f818","modified":1675259119189},{"_id":"themes/next/docs/DATA-FILES.md","hash":"980fb8d37701f7fd96b30bb911519de3bbb473d1","modified":1675259119188},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"6cc663db5e99fd86bb993c10d446ad26ada88e58","modified":1675259119192},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1675259119194},{"_id":"themes/next/languages/de.yml","hash":"15078b7ede1b084e8a6a15d271f0db9c325bd698","modified":1675259119202},{"_id":"themes/next/docs/MATH.md","hash":"f56946053ade0915ff7efa74d43c38b8dd9e63bb","modified":1675259119195},{"_id":"themes/next/languages/ar.yml","hash":"abcf220bd615cec0dd50e4d98da56580169d77e1","modified":1675259119201},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"1e86d32063b490d204baa9d45d8d3cb22c24a37d","modified":1675259119195},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1675259119202},{"_id":"themes/next/languages/es.yml","hash":"f064c793d56a5e0f20cda93b6f0e355044efc7d8","modified":1675259119202},{"_id":"themes/next/languages/en.yml","hash":"dbb64776f9c001c54d0058256c415a9a0724ed5d","modified":1675259119202},{"_id":"themes/next/languages/fa.yml","hash":"6c0a7d5bcc26eb45a9f3e02f13117c668e77fffd","modified":1675259119202},{"_id":"themes/next/languages/fr.yml","hash":"3e2f89d4bb4441d33ecc7b5a4ee114f627603391","modified":1675259119204},{"_id":"themes/next/languages/hu.yml","hash":"0ea89ffaefd02a10494995f05a2a59d5e5679a28","modified":1675259119204},{"_id":"themes/next/languages/id.yml","hash":"7599bb0ecf278beb8fde3d17bfc148a3241aef82","modified":1675259119204},{"_id":"themes/next/languages/it.yml","hash":"46222f468e66789e9ba13095809eb5e5b63edf30","modified":1675259119205},{"_id":"themes/next/languages/ja.yml","hash":"bf279d0eb1911806d01a12f27261fbc76a3bb3f9","modified":1675259119208},{"_id":"themes/next/languages/ko.yml","hash":"af4be6cb394abd4e2e9a728418897d2ed4cc5315","modified":1675259119210},{"_id":"themes/next/languages/nl.yml","hash":"9749cf90b250e631dd550a4f32ada3bb20f66dd0","modified":1675259119211},{"_id":"themes/next/languages/pt.yml","hash":"f6606dd0b916a465c233f24bd9a70adce34dc8d6","modified":1675259119212},{"_id":"themes/next/languages/pt-BR.yml","hash":"69aa3bef5710b61dc9a0f3b3a8f52f88c4d08c00","modified":1675259119212},{"_id":"themes/next/languages/ru.yml","hash":"012abc694cf9de281a0610f95f79c594f0a16562","modified":1675259119212},{"_id":"themes/next/languages/tr.yml","hash":"c4e9ab7e047ae13a19f147c6bec163c3ba2c6898","modified":1675259119212},{"_id":"themes/next/languages/zh-CN.yml","hash":"81d73e21402dad729053a3041390435f43136a68","modified":1675259119214},{"_id":"themes/next/languages/uk.yml","hash":"69ef00b1b8225920fcefff6a6b6f2f3aad00b4ce","modified":1675259119212},{"_id":"themes/next/languages/vi.yml","hash":"6a578cc28773bd764f4418110500478f185d6efa","modified":1675259119212},{"_id":"themes/next/languages/zh-HK.yml","hash":"92ccee40c234626bf0142152949811ebe39fcef2","modified":1675259119214},{"_id":"themes/next/languages/zh-TW.yml","hash":"cf0740648725983fb88409d6501876f8b79db41d","modified":1675259119214},{"_id":"themes/next/layout/_layout.swig","hash":"f72459f0ae963872276345cacada8f5f501e7ad9","modified":1677906380584},{"_id":"themes/next/layout/category.swig","hash":"c546b017a956faaa5f5643c7c8a363af7ac9d6b9","modified":1675259119251},{"_id":"themes/next/layout/archive.swig","hash":"d9bca77f6dcfef71e300a294f731bead11ce199f","modified":1675259119250},{"_id":"themes/next/layout/page.swig","hash":"357d916694d4c9a0fd1140fa56d3d17e067d8b52","modified":1675259119252},{"_id":"themes/next/layout/index.swig","hash":"8dfd96fb6f833dd5d037de800813105654e8e8e6","modified":1675259119251},{"_id":"themes/next/layout/tag.swig","hash":"d44ff8755727f6532e86fc9fc8dc631200ffe161","modified":1675259119253},{"_id":"themes/next/layout/post.swig","hash":"5f0b5ba2e0a5b763be5e7e96611865e33bba24d7","modified":1675259119252},{"_id":"themes/next/scripts/renderer.js","hash":"e3658eea97b1183ee2e9f676231e53f7994741f6","modified":1675259119267},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"6beeca0f45a429cd932b6e648617f548ff64c27c","modified":1675259119179},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"e67146befddec3a0dc47dc80d1109070c71d5d04","modified":1675259119179},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d5aa1a3323639a36bcd9a401484b67537043cd3c","modified":1675259119179},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"59275aa0582f793fee7be67904dcf52ad33a7181","modified":1675259119180},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"54e6a067ed95268eab6be2ba040a7e9b1907928e","modified":1675259119196},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"a9cfe5ac9ef727a8650b2b6584482751a26b1460","modified":1675259119196},{"_id":"themes/next/docs/ru/README.md","hash":"1e5ddb26ad6f931f8c06ce2120f257ff38b74fdf","modified":1675259119197},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"cb8e39c377fc4a14aaf133b4d1338a48560e9e65","modified":1675259119197},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"3202be9a8d31986caac640e7a4c7ce22e99917eb","modified":1675259119197},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7e6f227f2aaf30f400d4c065650a4e3d0d61b9e1","modified":1675259119198},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"611f2930c2b281b80543531b1bf33d082531456a","modified":1675259119198},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"2d868cd271d78b08775e28c5b976de8836da4455","modified":1675259119199},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"716111dd36d276f463c707dfcc9937fea2a1cf7a","modified":1675259119199},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"50ab381c27611d5bf97bb3907b5ca9998f28187d","modified":1675259119199},{"_id":"themes/next/docs/zh-CN/README.md","hash":"8f7c0d0b766024152591d4ccfac715c8e18b37f3","modified":1675259119199},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"0d46f9f50cf2e4183970adce705d1041155b0d37","modified":1675259119199},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"b3201934b966bc731eaf8a4dad4ba4bdcd300c10","modified":1675259119201},{"_id":"themes/next/layout/_partials/comments.swig","hash":"142efb4c6b73d8f736f6784804b40d5871333172","modified":1675259119215},{"_id":"themes/next/layout/_partials/footer.swig","hash":"c006374733c76f3602795a8a12c6a6fdddba5d99","modified":1675412608981},{"_id":"themes/next/layout/_partials/languages.swig","hash":"c3ea82604a5853fb44c5f4e4663cbe912aa5dcf8","modified":1675259119218},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1675259119220},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"5392dcbb504266f0f61d5b8219914068ef9cdc25","modified":1675259119229},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"30ade8c806d7826cc50a4a3e46a9e6213fddf333","modified":1675259119214},{"_id":"themes/next/layout/_macro/post.swig","hash":"c3fd56bac90ce45a0c79ddfe68beb223ad0d72b4","modified":1675259119215},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"5bffdb1448caca7db7b1f84e1693e6657a106d50","modified":1675259119215},{"_id":"themes/next/layout/_scripts/index.swig","hash":"1822eaf55bbb4bec88871c324fc18ad95580ccb4","modified":1675259119229},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"7b9e0f776a5be6c3f95bc7f394e1424ba02ba93b","modified":1675259119229},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"ccff5a773644d33ff22f6b45b6734f52b048f22b","modified":1675259119230},{"_id":"themes/next/layout/_scripts/three.swig","hash":"6b092c6d882b2dfa5273e1b3f60b244cb7c29fcd","modified":1675259119232},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"244ca2d74ee0d497c87572c6a26b43c62a952673","modified":1675259119232},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"28b0a7e843ec4365db1963646659a153753cd746","modified":1675259119234},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"5ae5adcd6f63ed98b2071e4f7e5e38c4d7d24e1b","modified":1675259119246},{"_id":"themes/next/layout/_third-party/index.swig","hash":"c6b63cbc80938e6e09578b8c67e01adf13a9e3bd","modified":1675259119244},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"269102fc5e46bd1ce75abdcce161f0570ae70e2f","modified":1675259119246},{"_id":"themes/next/scripts/events/index.js","hash":"5c355f10fe8c948a7f7cd28bd8120adb7595ebde","modified":1675259119254},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"ad321db012cea520066deb0639335e9bc0dcc343","modified":1675259119265},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"305d03c1e45782988809298c3e3b3c5d5ee438aa","modified":1675259119265},{"_id":"themes/next/scripts/filters/locals.js","hash":"a5e7d05d3bd2ae6dcffad5a8ea0f72c6e55dbd02","modified":1675259119265},{"_id":"themes/next/scripts/filters/minify.js","hash":"21196a48cb127bf476ce598f25f24e8a53ef50c2","modified":1675259119265},{"_id":"themes/next/scripts/helpers/engine.js","hash":"eb6b8bbc1dce4846cd5e0fac0452dbff56d07b5d","modified":1675259119266},{"_id":"themes/next/scripts/filters/post.js","hash":"57f2d817578dd97e206942604365e936a49854de","modified":1675259119266},{"_id":"themes/next/scripts/helpers/font.js","hash":"8fb1c0fc745df28e20b96222974402aab6d13a79","modified":1675259119266},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"b8d7ddfa4baa9b8d6b9066a634aa81c6243beec9","modified":1675259119267},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"4044129368d0e2811859a9661cad8ab47118bc32","modified":1675259119267},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"840536754121e0da5968f5ad235f29200fc5d769","modified":1675259119268},{"_id":"themes/next/scripts/tags/button.js","hash":"bb0e8abbc0a6d5b3a1a75a23976f2ac3075aab31","modified":1675259119268},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"93ccd3f99d3cb42674f29183c756df63acb5d7f8","modified":1675259119269},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"e2d0184bc4a557e1017395b80ff46880078d8537","modified":1675259119268},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1675259119269},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1675259119269},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1675259119270},{"_id":"themes/next/scripts/tags/pdf.js","hash":"37b53661ad00a01a2ca7d2e4a5ad3a926073f8e2","modified":1675259119270},{"_id":"themes/next/scripts/tags/tabs.js","hash":"c70a4a66fd0c28c98ccb6c5d5f398972e5574d28","modified":1675259119270},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1675259119270},{"_id":"themes/next/source/css/_mixins.styl","hash":"072a3fa473c19b20ccd7536a656cda044dbdae0a","modified":1675259119311},{"_id":"themes/next/source/css/_colors.styl","hash":"11aef31a8e76f0f332a274a8bfd4537b73d4f88f","modified":1675259119271},{"_id":"themes/next/source/css/main.styl","hash":"e618122a80b542228a9faaf899fec82b1b96b3ff","modified":1675419748251},{"_id":"themes/next/source/dist/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1677905168635},{"_id":"themes/next/source/dist/APlayer.min.css.map","hash":"c59d2bc9472922cf6ef9a99e052dbee6cc7e6b36","modified":1677905168645},{"_id":"themes/next/source/dist/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1677905168665},{"_id":"themes/next/source/dist/music.js","hash":"00ff71bd9093a1d70562a75f506b9e8ed39e870b","modified":1679465296366},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1675259119320},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1675259119320},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1675259119320},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1675259119321},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1675259119321},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1675259119325},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1675259119327},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1675259119328},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1675259119329},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1675259119329},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1675259119330},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1675259119330},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1675259119330},{"_id":"themes/next/source/images/music.jpg","hash":"a2e7cc2fdfebf19b6de5a4260b631d090fb3bc25","modified":1679464792079},{"_id":"themes/next/source/images/music2.jpg","hash":"4a78bdd0a73584ba1ee2597d801c26ec9d8db1cc","modified":1679465181044},{"_id":"themes/next/source/images/wechatpay.png","hash":"e38612cefdc0c24d2d78bd2c65e0516c7cde2b93","modified":1675316447129},{"_id":"themes/next/source/js/algolia-search.js","hash":"6a813410e33824d7acc65a369a2983912bb3420c","modified":1675259119331},{"_id":"themes/next/source/js/bookmark.js","hash":"9f05fd3672789311dc0cf5b37e40dc654cb04a2a","modified":1675259119331},{"_id":"themes/next/source/js/local-search.js","hash":"cfa6a0f3f9c2bc759ee507668a21f4e8f250f42a","modified":1675259119331},{"_id":"themes/next/source/js/motion.js","hash":"d5aa1a08cdf3c8d1d8d550fb1801274cc41e5874","modified":1675259119331},{"_id":"themes/next/source/js/next-boot.js","hash":"250d8dcd6322e69e3fbadd0f3e37081c97b47c52","modified":1675259119331},{"_id":"themes/next/source/js/utils.js","hash":"26a82e46fdcadc7c3c2c56a7267284b61a26f7f3","modified":1675259119333},{"_id":"themes/next/source/lib/anime.min.js","hash":"960be51132134acd65c2017cc8a5d69cb419a0cd","modified":1675259119333},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"91056a6c98cca63ff8cc6956e531ee3faf4b8ad9","modified":1675259119218},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"7d638e413f2548fc990c4a467dd03de6c81fc960","modified":1675259119217},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"90cce9f407e9490756ba99580e3eb09f55b05eaa","modified":1675412140284},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"0dd316f153c492c0a03bd0273d50fa322bc81f11","modified":1675259119218},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"4baa86ca631168fc6388d27f4b1b501b40c877a8","modified":1675259119218},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"90d3eaba6fbe69bee465ddd67c467fd2c0239dc4","modified":1675259119218},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"bed6cc2b48cf2655036ba39c9bae73a295228a4d","modified":1675259119218},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"91c0addb33006619faa4c32e5d66874e25f1e9b3","modified":1675259119219},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"8d4e3dd0d3631ce0b21bc15c259f6ac886de631d","modified":1675259119219},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"f2eb455c8bf13533427254f0c9b4b17b2498168b","modified":1675259119220},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"d8f785c062c6b0763a778bd4a252e6f5fee0e432","modified":1675259119220},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"ce712c110b5ce8aacba7a86b0558ff89700675c9","modified":1675259119222},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"bc7b047a6246df07767373644b1637d91c3a88b1","modified":1675259119224},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"f349a226e5370075bb6924e60da8b0170c7cfcc1","modified":1675259119226},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"98fd1f5df044f4534e1d4ca9ab092ba5761739a9","modified":1675259119227},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"128f7d679bb4d53b29203d598d217f029a66dee7","modified":1675259119228},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a6c761d5193cb6f22e9422dbbcf209e05471b0ed","modified":1675259119228},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"7b2ef5db9615267a24b884388925de1e9b447c1f","modified":1675259119228},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1675259119231},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"34c05e9d73b0f081db70990c296b6d6a0f8ea2ca","modified":1675259119230},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1675259119231},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1675259119231},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1675259119231},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"84adaadd83ce447fa9da2cff19006334c9fcbff9","modified":1675259119233},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b8819bd056f8a580c5556d4415836a906ed5d7a4","modified":1675259119233},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"91c2cb900c76224c5814eeb842d1d5f517f9bf05","modified":1675259119234},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"85b60e222712ca3b2c4dc2039de2dc36b8d82940","modified":1675259119234},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"2642e8aef5afbe23a2a76efdc955dab2ee04ed48","modified":1675259119235},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"fb94ee487d75e484e59b7fba96e989f699ff8a83","modified":1675259119235},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1b29b99fa921f12c25d3dc95facdf84ef7bb1b5c","modified":1675259119236},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"a42f97eda3748583bac2253c47fe5dfa54f07b8f","modified":1675259119237},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"9298e6d6c4a62a0862fc0f4060ed99779d7b68cb","modified":1675259119236},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"606ad14a29320157df9b8f33738282c51bb393d9","modified":1675259119238},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"3d91899ca079e84d95087b882526d291e6f53918","modified":1675259119241},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"ae2707d6e47582bb470c075649ec7bad86a6d5a9","modified":1675259119243},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"59df21fcfe9d0ada8cee3188cb1075529c1c3eb8","modified":1675259119244},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"276f523e414d4aa7f350a8f2fd3df8a3d8ea9656","modified":1675259119245},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"1f34b2d3c753a3589ab6c462880bd4eb7df09914","modified":1675259119245},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"fd726aad77a57b288f07d6998ec29291c67c7cbb","modified":1675259119246},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"58296a5c1883f26464c2a5ccf734c19f5fbf395a","modified":1675259119247},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"aa6ab95b8b76611694613defb4bf25003d1b927f","modified":1675259119247},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d2f0e4c598410ec33785abe302c7ea7492bb791a","modified":1675259119247},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"53a0760c75d5aaabb3ce8e8aa8e003510d59807f","modified":1675259119247},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"01d94354d07e72cad47100482068b6be69fcc033","modified":1675259119248},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"964cd6bac668cf6d211a2624fbef3948cfdece55","modified":1675259119248},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"619338ddacf01e3df812e66a997e778f672f4726","modified":1675259119249},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"c171ea94e9afbba97f06856904264da331559463","modified":1675259119248},{"_id":"themes/next/scripts/events/lib/config.js","hash":"aefe3b38a22bc155d485e39187f23e4f2ee5680a","modified":1675259119254},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"5a223b60406cee7438cfe3a5e41d1284425aa7a5","modified":1675259119250},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"e73f697bb160b223fdde783237148be5f41c1d78","modified":1675259119260},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"08496b71c9939718e7955704d219e44d7109247b","modified":1675259119255},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"2f22f48f7370470cef78561a47c2a47c78035385","modified":1675259119262},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1675259119262},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"0c3bea89d64bc12c1bbe6f208a83773c6fb5375a","modified":1675259119263},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3a80559df0b670ccb065ea9d3bb587d0b61be3a4","modified":1675259119263},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"67cf90d9a2428c14eb113a64bdd213c22a019aef","modified":1675259119263},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"323a47df6ded894944a2647db44556d6163e67c4","modified":1675259119263},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"a4f3153ac76a7ffdf6cc70f52f1b2cc218ed393e","modified":1675259119264},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"851359f5ff90f733a9bd7fe677edbee8b8ac714c","modified":1675259119264},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"583ff1e7a2ca889f1f54eb0ca793894466823c7c","modified":1675259119318},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"5980abbbbeacd8541121f436fa414d24ad5e97c2","modified":1675259119318},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c22b58af3327236ec54d5706501aa5a20e15012e","modified":1675259119318},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4e33774b1fe6d0a51f3a428c54c5e600e83bf154","modified":1675259119319},{"_id":"themes/next/source/js/schemes/muse.js","hash":"a18559a9c332199efad0100cf84bb0c23fc0f17a","modified":1675259119331},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ad680efdfb2f86546182bf3f59886efbcf3c1b2d","modified":1675259119319},{"_id":"themes/next/source/lib/canvas-nest/LICENSE","hash":"336611e76f0638d3d8aeca6b1b97138d2a07523f","modified":1675422185121},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"b85a6e2af1387fe64b51e7cd3e2da8616e6f5a3f","modified":1675259119332},{"_id":"themes/next/source/lib/canvas-nest/README.md","hash":"6964aecdc62466e6b529f6887558669925a36bc6","modified":1675422185121},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1675422185121},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest-nomobile.min.js","hash":"824acc97dace301d1d083d7a97b8b374c1654031","modified":1675673687680},{"_id":"themes/next/source/lib/three/.gitignore","hash":"5767276045f60da2350895a59aa6e138b0e83294","modified":1675423491754},{"_id":"themes/next/source/lib/three/LICENSE","hash":"336611e76f0638d3d8aeca6b1b97138d2a07523f","modified":1675423491754},{"_id":"themes/next/source/lib/three/README.md","hash":"76071f107e07113ccb23192680782577a15c5350","modified":1675423491754},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"558838e0821f76c1e6d58add25116853caa1976c","modified":1675423491754},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"1b371d908c6729f15877601792583e25cc4e6091","modified":1675423491754},{"_id":"themes/next/source/lib/three/package.json","hash":"ff493a283c04f87a272abdffa8aa29fb4368b803","modified":1675423491754},{"_id":"themes/next/source/lib/three/gulpfile.js","hash":"8667c49d81292b55e4ec8c0ec9f8eb42f4ababab","modified":1675423491754},{"_id":"themes/next/source/lib/three/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1675423491754},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"62fe85b767c525a07bbec1193c0840ee924dad9a","modified":1675423491754},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1675259119337},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1675259119338},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"510a6f0ba7485dd54ce347cca890ab52c4957081","modified":1675259119275},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"0534b329d279a6f255112b3305ff92c810f31724","modified":1675259119277},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"d17236df3b4d6def1e4e81133ef4729c390de3ac","modified":1675259119278},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"c52648a7b09f9fe37858f5694fcc1ffc709ad147","modified":1675259119285},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"a2ee16cac29a82cfce26804c160286fcbee94161","modified":1675259119294},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7a95c27762e1303bf06ee808c63f616cb192fcaf","modified":1675259119295},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5540c9259cb7895a5f10a289c7937e5470a7c134","modified":1675259119299},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"45f4badac6ec45cf24355f6157aece1d4d3f1134","modified":1675259119299},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"4b068d0d898f4e624937503f0e1428993050bd65","modified":1675259119300},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1675259119301},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"b619f39e18398422e0ac4999d8f042a5eaebe9cd","modified":1675259119302},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1675259119302},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"f317d2e3886e94f5fbb8781c2e68edd19669ff58","modified":1675259119303},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"20e0e3e3eba384930c022e21511214d244b4c9e7","modified":1675259119310},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"e342b8f8e11a3a6aa5a029912c9778c25bf5d135","modified":1675259119312},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"4fe031f5557a2ce1fc9bff18f27b64f129c71a2f","modified":1675421697662},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"12b265f82840f27112ca2b1be497677f20f87545","modified":1675259119313},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"716e8b0f056bf6393e6bc6969ac84598ab8e7a6f","modified":1675259119313},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"e1c29b81a32273a0dedd926cda199a71aea72624","modified":1675259119314},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"c5142739e01e9f25c8b32b2209af85c787bb2b42","modified":1675259119314},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"b70581fc964dcc2e6723e4ca8143997affd4cdc6","modified":1675430615845},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"7d3263177cf7bf23dbf2666802363afbbd7e832b","modified":1675431048179},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4b7f057dbb53efd7cbe7eac7835a793ab3cbb135","modified":1675430284375},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"e2e4d12dfb26d0e530cd4af6b269c492f51246ff","modified":1675432920683},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1675259119315},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"558794fced306339b98dc2b0ee7f0576802f1355","modified":1675259119316},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"9ea428b6fa37d0d7dedb5c45276b104da293c485","modified":1675429754697},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0a9f0d9eb042595502d200fb8c65efb0e6c89aa9","modified":1675259119317},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5de34e1d8a290751641ae456c942410852d5e809","modified":1675259119316},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"dc9318992ce2eb086ebaa2fe56b325e56d24098b","modified":1675259119317},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b69ac38b9da8c9c1b7de696fdeea7f9d7705213a","modified":1675259119317},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1675259119317},{"_id":"themes/next/source/lib/canvas-nest/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1675422185111},{"_id":"themes/next/source/lib/canvas-nest/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/config","hash":"78c4459d066ad795856608d603d780b53488073d","modified":1675422185111},{"_id":"themes/next/source/lib/canvas-nest/.git/index","hash":"045fefcc50c8b2c1a0d59e9ddb9966ffc02e765e","modified":1675422185121},{"_id":"themes/next/source/lib/canvas-nest/.git/packed-refs","hash":"80eecf0c5c7f21b2678dc1c329f74de19b6a3a67","modified":1675422185090},{"_id":"themes/next/source/lib/canvas-nest/.github/stale.yml","hash":"dbd5e6bf89b76ad1f2b081578b239c7ae32755af","modified":1675422185121},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"82e34d28f8a1169b20b60101d5bb0446deba3514","modified":1675259119334},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1675259119335},{"_id":"themes/next/source/lib/three/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1675423491744},{"_id":"themes/next/source/lib/three/.git/config","hash":"c9c13087a3f33e6f46cd35a93c8b9bdf27618852","modified":1675423491744},{"_id":"themes/next/source/lib/three/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1675423484529},{"_id":"themes/next/source/lib/three/.git/index","hash":"fbff5a7b60dda5bd90887d58c2698d1cc0b25fe5","modified":1675423491764},{"_id":"themes/next/source/lib/three/.git/packed-refs","hash":"51439496ae6791f8b002584e71f5d10f4f1a1f71","modified":1675423491734},{"_id":"themes/next/source/lib/three/lib/CanvasRenderer.js","hash":"71141daa39bbcedcf14ae95c05023a57828a5a43","modified":1675423491754},{"_id":"themes/next/source/lib/three/.github/stale.yml","hash":"dbd5e6bf89b76ad1f2b081578b239c7ae32755af","modified":1675423491754},{"_id":"themes/next/source/lib/three/src/canvas_lines.js","hash":"10795d7f1e5393b2b5e1529b017ee4e0ffe82ac9","modified":1675423491754},{"_id":"themes/next/source/lib/three/lib/Projector.js","hash":"69725cd0af6868c5aa059343cc6e18e0c10b2f2e","modified":1675423491754},{"_id":"themes/next/source/lib/three/src/canvas_sphere.js","hash":"8381c792b161001a1b5cf39613c6d48e2588b3ce","modified":1675423491754},{"_id":"themes/next/source/lib/three/src/three-waves.js","hash":"ac382962d408f16acf07b925b94bb15495b5207c","modified":1675423491754},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"236a039b0900f4267de566b46f62314ad967d30f","modified":1675259119279},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"18edddb2ffb3f85a68e4367f81e06c461e07bc25","modified":1675259119280},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1675259119280},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"f6f05f02d50f742c84ee5122016c0563a8bb2cf9","modified":1675259119280},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"97974c231b4659b8aa5e9321c4d54db5c816d0db","modified":1675259119281},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"a52f8cae599099231866298ed831fdf76c9b6717","modified":1675259119281},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"9af620eba5ccceea21a0e3bc69f6f1fa7637c2f3","modified":1675259119281},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"70b3eb9d36543ab92796ac163544e9cf51b7c1e6","modified":1675259119282},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"97dec98d0403097d66822f1c90b50b2890c84698","modified":1675259119282},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0dfb97703a519d9438f64f9e41ab1dd37381f733","modified":1675259119283},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"57b9a179675f1536e017cba457b6ac575e397c4f","modified":1675259119282},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"93ba8172c0d2c37d738e6dbd44fcd5a2e23b92f3","modified":1675259119283},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"2c24829d95c742eb9e8316ebf2fbe9f2c168b59a","modified":1675259119283},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"66fc406796b6efe6cea76550573b7a632112406a","modified":1675259119283},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"09dda2667628d1f91b2e37d8fc6df1413f961b64","modified":1675259119284},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5cc9e7394c927065c688cba5edd6e0a27587f1d8","modified":1675259119284},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"fcd64c23d17775b3635325f6758b648d932e79b5","modified":1675259119284},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"b266d2ce5e2b117be01537889e839a69004dc0bb","modified":1675259119284},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"b87f4a06c0db893df4f756f24be182e1a4751f24","modified":1675259119285},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"d83102771df652769e51ddfd041cf5f4ca1a041d","modified":1675259119286},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"8ed7a9d5dfac592de703421b543978095129aa5b","modified":1675259119286},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"bad99f4cccb93b3cefe990a2c85124e60698d32e","modified":1675259119286},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1f6b0d3ab227697ca115e57fd61122ea7950e19d","modified":1675259119286},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"7eeb22c5696f8e0c95161dc57703973cf81c8c12","modified":1675259119287},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"b4f4bae437d4f994af93cf142494ffcd86bae46b","modified":1675259119287},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"b31c86d1a4f89837f9187bed646bda96b2cd286c","modified":1675259119287},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"300058ca12e81013e77ba01fe66ac210525768b6","modified":1675259119289},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"6d5f26646e2914474f295de8bf6dc327d4acd529","modified":1675259119291},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"7a3a56b10ab714c0e2ed240d0939deeecdcad167","modified":1675259119291},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"3d16ac0f4ccaeed868c246d4d49bde543d1f62cb","modified":1675259119293},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b8c816fba0a9b4a35fbae03ba5b1b2da96ba2687","modified":1675259119294},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"49722d555a2edb18094bb2cb3d7336dd72051b93","modified":1675259119295},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"357f825f0a649b2e28cba1481d4c9a0cb402e43a","modified":1675259119295},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"096f908c08ce553e482aadfd3e767a0145191093","modified":1675259119297},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"525242ce9e912c4adfe5134347c67dbdb9e98e3d","modified":1675259119297},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"12f7eaf6b56624cbc411528562d6bb848ff97039","modified":1675259119297},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"fa0a2ea57b7b4ce75b5d18c264af2d92ea3192f9","modified":1675259119298},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"b11b04737a1a0fea3bd9f0081d96ee6c015358d4","modified":1675259119298},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"098b4bdf49c7300490f959386d5d1185a32543f6","modified":1675259119298},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"5d540f683018745a5ed1d6f635df28ea610c1244","modified":1675259119298},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"67a1fcb33535122d41acd24f1f49cf02c89b88fa","modified":1675259119299},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4079e616fbf36112dec0674c1e0713d1d9769068","modified":1675259119300},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"83bd737f663a8461e66985af8ddbfc0a731fc939","modified":1675259119300},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"80488259271bcfe38031f4c2e902463daba9336b","modified":1675259119301},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"c911045b2ce9a66e38d9dd30c7ed078abbc10cbf","modified":1675259119301},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"ceacfa6218f6084c71a230b086e5d2708d29927e","modified":1675259119303},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"aca7bb220fc14ef2a8f96282d2a95a96a9238d46","modified":1675259119303},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"adaf0f580fccf4158169eeaf534a18005b39a760","modified":1675259119304},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"8b7aafb911850c73074cdb6cc87abe4ac8c12e99","modified":1675259119304},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"03a5bcecc0b12231462ef6ffe432fa77ee71beff","modified":1675259119304},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"3256e39f281f06751a1c0145d9806a0e56d68170","modified":1675259119305},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"51d46fa3c7c6b691c61a2c2b0ac005c97cfbf72b","modified":1675259119308},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/fsmonitor-watchman.sample","hash":"0ec0ec9ac11111433d17ea79e0ae8cec650dcfa4","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1675422182680},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-commit.sample","hash":"a79d057388ee2c2fe6561d7697f1f5efcff96f23","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-merge-commit.sample","hash":"04c64e58bc25c149482ed45dbd79e40effb89eb7","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-push.sample","hash":"a599b773b930ca83dbc3a5c7c13059ac4a6eaedc","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/push-to-checkout.sample","hash":"508240328c8b55f8157c93c43bf5e291e5d2fbcb","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/hooks/update.sample","hash":"730e6bd5225478bab6147b7a62a6e2ae21d40507","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1675422182690},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/HEAD","hash":"17a9273894de5d7f5bb2adc4eb2ae905867a85bf","modified":1675422185111},{"_id":"themes/next/source/lib/three/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1675423484529},{"_id":"themes/next/source/lib/three/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1675423484529},{"_id":"themes/next/source/lib/three/.git/hooks/fsmonitor-watchman.sample","hash":"0ec0ec9ac11111433d17ea79e0ae8cec650dcfa4","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-commit.sample","hash":"a79d057388ee2c2fe6561d7697f1f5efcff96f23","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-merge-commit.sample","hash":"04c64e58bc25c149482ed45dbd79e40effb89eb7","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-push.sample","hash":"a599b773b930ca83dbc3a5c7c13059ac4a6eaedc","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/push-to-checkout.sample","hash":"508240328c8b55f8157c93c43bf5e291e5d2fbcb","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/hooks/update.sample","hash":"730e6bd5225478bab6147b7a62a6e2ae21d40507","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1675423484539},{"_id":"themes/next/source/lib/three/.git/logs/HEAD","hash":"732a9b67dab29e09bccdd9093e0a30b3b7a31618","modified":1675423491744},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/pack/pack-8614499ac34603e5f62a25e1ed7138332befa370.idx","hash":"7b050e7c7c4ed82ff61ea69f13085b11bac423cf","modified":1675422185000},{"_id":"themes/next/source/lib/canvas-nest/.git/objects/pack/pack-8614499ac34603e5f62a25e1ed7138332befa370.pack","hash":"455c5c532eee08dd88c8295639309b05fccbb64c","modified":1675422184990},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/heads/master","hash":"473e30291eac5f6d120dfe823b29ad4b2218f05a","modified":1675422185111},{"_id":"themes/next/source/lib/three/.git/objects/pack/pack-75f0bd54ccc830622c5523e90e168e5d930b759d.idx","hash":"5f4f97f13dbe353456fa6978bc089b4320f79272","modified":1675423491524},{"_id":"themes/next/source/lib/three/.git/refs/heads/master","hash":"dcb9d54aa64c8a682ca5ca209370022b8a8dc843","modified":1675423491744},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/heads/master","hash":"17a9273894de5d7f5bb2adc4eb2ae905867a85bf","modified":1675422185111},{"_id":"themes/next/source/lib/canvas-nest/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1675422185100},{"_id":"themes/next/source/lib/three/.git/logs/refs/heads/master","hash":"732a9b67dab29e09bccdd9093e0a30b3b7a31618","modified":1675423491744},{"_id":"themes/next/source/lib/three/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1675423491744},{"_id":"themes/next/source/lib/canvas-nest/.git/logs/refs/remotes/origin/HEAD","hash":"17a9273894de5d7f5bb2adc4eb2ae905867a85bf","modified":1675422185100},{"_id":"themes/next/source/lib/three/.git/logs/refs/remotes/origin/HEAD","hash":"732a9b67dab29e09bccdd9093e0a30b3b7a31618","modified":1675423491744},{"_id":"themes/next/source/images/wechatpay.jpg","hash":"f7a2fc86255f2a010851618293777a7ccc8b5aad","modified":1675315398911},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1675259119335},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1675259119336},{"_id":"themes/next/source/dist/APlayer.min.js.map","hash":"31a19da0f0cb6b00ec212eafa847f31af86788df","modified":1677905168675},{"_id":"themes/next/source/lib/three/.git/objects/pack/pack-75f0bd54ccc830622c5523e90e168e5d930b759d.pack","hash":"89e534013b432027b94500494ad5573cd869ce00","modified":1675423491514},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1675423491764},{"_id":"themes/next/source/images/background.jpg","hash":"2409adab05a1ca5dd6a2287b742c97f9fc409ea1","modified":1675413229110},{"_id":"themes/next/source/images/back.jpg","hash":"cb01783caab9279a61bfcf4d4f07317cf528d2ce","modified":1675425034582},{"_id":"public/atom.xml","hash":"7ffab2496b5b75640b1c774a9babfce1d3a854b4","modified":1700831729134},{"_id":"public/search.xml","hash":"d11a0076f9d20bf81178229c36428d9a97f76654","modified":1700831729134},{"_id":"public/sitemap.xml","hash":"ab7a18a1330789f28632f6622dd3a3d0420b2734","modified":1700831729134},{"_id":"public/sitemap.txt","hash":"94add2a070c74dd3300f0686ba1bd3308526b70d","modified":1700831729134},{"_id":"public/tags/index.html","hash":"ab6ef7681afbeb65ed0ac8d19f90b888a2ddde8e","modified":1700831729134},{"_id":"public/categories/index.html","hash":"2fa454710eb4245f6c8a6216ce0cbef000aab131","modified":1700831729134},{"_id":"public/2023/07/05/论文整理-10/index.html","hash":"97b5bbf0eef27d8860076fae17e9bea5fad0058c","modified":1700831729134},{"_id":"public/2023/07/03/产品表面缺陷视觉检测数据处理关键技术研究/index.html","hash":"1bb23ea9fbd2fe57d6a2752ce3039537cc999365","modified":1700831729134},{"_id":"public/2023/06/17/论文整理-9/index.html","hash":"058d9a2e93ddf3438d044e6ab3bdf73e516008be","modified":1700831729134},{"_id":"public/2023/06/10/基于结构光扫描的三维点云数据重构算法研究/index.html","hash":"bf692b4b7eb2c0d34bb3bd30b2e62fb00efd49ee","modified":1700831729134},{"_id":"public/2023/05/22/时间序列/index.html","hash":"1102a2c17457625223d294edb926e3a10f9c5162","modified":1700831729134},{"_id":"public/2023/05/19/方向研究/index.html","hash":"1fab427377b1ba2caa6e433e5a1b8a7f5cb90da4","modified":1700831729134},{"_id":"public/2023/05/18/yolov5代码分析/index.html","hash":"83007218fa31ad856478a02b8f676665bd4d8d08","modified":1700831729134},{"_id":"public/2023/05/05/论文整理-7/index.html","hash":"7e5d45fb57dd80ee30a51d425ef9ad62b1e16796","modified":1700831729134},{"_id":"public/2023/05/01/论文整理-6/index.html","hash":"61f23628e99323a066cecd3e0c6bb3525c9d394b","modified":1700831729134},{"_id":"public/2023/04/26/Bilibili-yolo基础/index.html","hash":"b917b718a938b293b25f4643520ebe042c96efbd","modified":1700831729134},{"_id":"public/2023/03/29/基于深度学习的图像识别/index.html","hash":"14ca34c0c6131da803795178b5e7a6d1cab45195","modified":1700831729134},{"_id":"public/2023/03/23/论文整理-3/index.html","hash":"8f43ac88dfe3af6c682e7872e91e70d07e5ae37b","modified":1700831729134},{"_id":"public/2023/03/04/添加音乐/index.html","hash":"d48f4056b5c71db0a8bbf1790f3580254e72ee44","modified":1700831729134},{"_id":"public/archives/page/3/index.html","hash":"8e9eb074feedd3596925c0ee61c93b35f7a8db02","modified":1700831729134},{"_id":"public/archives/2023/page/3/index.html","hash":"a3dabacefa51199ed39a4d767ec6c695eda1a0c7","modified":1700831729134},{"_id":"public/archives/2023/01/index.html","hash":"00f9c1ae5e85dfa53eebb13373c1eefc630a18a8","modified":1700831729134},{"_id":"public/archives/2023/02/index.html","hash":"e35753878d298a22d8498d2fb198507205b1c631","modified":1700831729134},{"_id":"public/archives/2023/03/index.html","hash":"e62f61bff4c20f6ae0d68539157236d4b4a5d437","modified":1700831729134},{"_id":"public/archives/2023/04/index.html","hash":"6776113edbb6127aa3510754ba52d3c0b00e6369","modified":1700831729134},{"_id":"public/archives/2023/05/index.html","hash":"392778b0cd17b11ae291273f308fca1fac8fa8f2","modified":1700831729134},{"_id":"public/archives/2023/06/index.html","hash":"82560952301b08ac37a2dc0b480f6bf7ffbc889a","modified":1700831729134},{"_id":"public/archives/2023/07/index.html","hash":"96d40f6c0a7584413948446ec8b79fa005371943","modified":1700831729134},{"_id":"public/categories/PLC/index.html","hash":"c81e3ef4125bbd75e9cd5e5823f1028dd4a7c813","modified":1700831729134},{"_id":"public/categories/随笔/index.html","hash":"0106f7f62ec2c948d44a7ee0eb2e555a33fd3b41","modified":1700831729134},{"_id":"public/categories/实验/index.html","hash":"5cd140f3db29f6da3707772d88d9e444a79adf17","modified":1700831729134},{"_id":"public/categories/搭建环境/index.html","hash":"6da57864e196e2d83056b4cc96ce0a740ca635b3","modified":1700831729134},{"_id":"public/categories/论文整理/index.html","hash":"95821261d4096927420c75a6b997cf3b9de6d94e","modified":1700831729134},{"_id":"public/categories/博客搭建/index.html","hash":"af294648637d7161f633cf1e5d13b52ace1d5b3f","modified":1700831729134},{"_id":"public/tags/yolo基础/index.html","hash":"d0c4045888cf546e4a98315cef170b518479e756","modified":1700831729134},{"_id":"public/tags/打卡/index.html","hash":"a05b2df3e24ae689724ce69576091101b3b5594a","modified":1700831729134},{"_id":"public/tags/三维重建/index.html","hash":"c034a05435a8be8e6867542b0d8c1cecf9c37083","modified":1700831729134},{"_id":"public/tags/aplayer-leancloud/index.html","hash":"6c5c54ad8edcf92dd80dd717e4fcbb0898934821","modified":1700831729134},{"_id":"public/tags/带钢缺陷检测/index.html","hash":"784b9eec59410279619cf4c25293ff46997f5fb8","modified":1700831729134},{"_id":"public/tags/深度学习基础/index.html","hash":"56fd36d440e4ca4dc848a547916f989a180cfb55","modified":1700831729134},{"_id":"public/tags/多标签算法/index.html","hash":"9604e9edd0cb2179f222b0d4753c8220c3be2a5c","modified":1700831729134},{"_id":"public/2023/06/10/基于图像的三维重建——基于图像的立体视觉研究/index.html","hash":"22d5933e350cb0fd469d93f9776f30c2095f2986","modified":1700831729134},{"_id":"public/2023/05/14/论文整理-8/index.html","hash":"7db63a87402c4b48cd4714874e9f53ef0b97bef1","modified":1700831729134},{"_id":"public/2023/04/23/论文整理-5/index.html","hash":"d73be2b96ca78789f55a477d673a919ef35ec1a9","modified":1700831729134},{"_id":"public/2023/04/14/PLC/index.html","hash":"3177866feba5df584cc340dec33ec7ca665459bb","modified":1700831729134},{"_id":"public/2023/04/06/论文整理-4/index.html","hash":"936aa82a9b8cda409cfc2ece31020adfc4010fd0","modified":1700831729134},{"_id":"public/2023/03/16/搭建yolov5框架/index.html","hash":"30733d28726ad65e5ab7ed4f85e428f8ac1fe549","modified":1700831729134},{"_id":"public/2023/03/15/搭建深度学习环境/index.html","hash":"dec3ea2333afc01e604f8b490810b77858c8ccad","modified":1700831729134},{"_id":"public/2023/03/12/论文整理-2/index.html","hash":"5e06f5a570c4e4b2c625b7930d993dad9ed9671c","modified":1700831729134},{"_id":"public/2023/03/04/论文整理/index.html","hash":"1c102bc48503b4b066c3c8d209ca2d83f54e50b5","modified":1700831729134},{"_id":"public/2023/02/28/去看论文吧/index.html","hash":"57eae46c437f04cefd837a2933a1087c009aeb80","modified":1700831729134},{"_id":"public/2023/01/25/hello-world/index.html","hash":"9674eccaaec64e6c012bf775f52d94e5cb990b79","modified":1700831729134},{"_id":"public/archives/index.html","hash":"c605b2599a7c18afb19bb77ac33e87c58e72be58","modified":1700831729134},{"_id":"public/archives/page/2/index.html","hash":"e067bf620e9641c2c5aba4bbf62866f06dbbe6df","modified":1700831729134},{"_id":"public/archives/2023/index.html","hash":"31d76040ae1a1aa4554a6eba076519d735130be7","modified":1700831729134},{"_id":"public/archives/2023/page/2/index.html","hash":"9ae1eb4efe68ab3ea0ea85bb4b3083b9eb538b18","modified":1700831729134},{"_id":"public/index.html","hash":"f4aa55453ec2174f230ca41a0ecf19c2573681d6","modified":1700831729134},{"_id":"public/page/2/index.html","hash":"26ff303225f35d16d44e7e6acf39b78af4621fbd","modified":1700831729134},{"_id":"public/page/3/index.html","hash":"de40d2f128b772de45a44fd69267817638169449","modified":1700831729134},{"_id":"public/dist/APlayer.min.css.map","hash":"c59d2bc9472922cf6ef9a99e052dbee6cc7e6b36","modified":1700831729134},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1700831729134},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1700831729134},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1700831729134},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1700831729134},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1700831729134},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1700831729134},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1700831729134},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1700831729134},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1700831729134},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1700831729134},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1700831729134},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1700831729134},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1700831729134},{"_id":"public/images/music.jpg","hash":"a2e7cc2fdfebf19b6de5a4260b631d090fb3bc25","modified":1700831729134},{"_id":"public/images/music2.jpg","hash":"4a78bdd0a73584ba1ee2597d801c26ec9d8db1cc","modified":1700831729134},{"_id":"public/images/wechatpay.png","hash":"e38612cefdc0c24d2d78bd2c65e0516c7cde2b93","modified":1700831729134},{"_id":"public/lib/canvas-nest/LICENSE","hash":"336611e76f0638d3d8aeca6b1b97138d2a07523f","modified":1700831729134},{"_id":"public/lib/three/LICENSE","hash":"336611e76f0638d3d8aeca6b1b97138d2a07523f","modified":1700831729134},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1700831729134},{"_id":"public/images/wechatpay.jpg","hash":"f7a2fc86255f2a010851618293777a7ccc8b5aad","modified":1700831729134},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1700831729134},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1700831729134},{"_id":"public/dist/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1700831729134},{"_id":"public/dist/music.js","hash":"df31a0e7d51ebe3c19026e5129e80266753ed3ec","modified":1700831729134},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1700831729134},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1700831729134},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1700831729134},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1700831729134},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1700831729134},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1700831729134},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1700831729134},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1700831729134},{"_id":"public/lib/canvas-nest/README.html","hash":"01bbaf2d849273b12f9e43458732193ccae53ab9","modified":1700831729134},{"_id":"public/lib/canvas-nest/canvas-nest-nomobile.min.js","hash":"f0c639f6988f0018bbbdba47af25f025186935a8","modified":1700831729134},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1700831729134},{"_id":"public/lib/three/README.html","hash":"d8faebc08f93684752eeb936400e62117bd59b6e","modified":1700831729134},{"_id":"public/lib/three/renovate.json","hash":"94990e0ad04ce4a7c6f0ac3543318d9e02db1264","modified":1700831729134},{"_id":"public/lib/three/gulpfile.js","hash":"e0e9e7051d9d82a37c2aba1df396d8b3916323c4","modified":1700831729134},{"_id":"public/lib/three/package.json","hash":"3e6a0c56ec47a38c0bf7b404f6e46965ec7d2e3d","modified":1700831729134},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1700831729134},{"_id":"public/lib/three/src/canvas_lines.js","hash":"650310ff6783671f8ceccf01f840b20d9c87b491","modified":1700831729134},{"_id":"public/lib/three/src/canvas_sphere.js","hash":"7614790c67d3e79e3390fe688f6b01afad7e3bb1","modified":1700831729134},{"_id":"public/lib/three/src/three-waves.js","hash":"e98e442f14920e9fb8691846dca3a2225d403048","modified":1700831729134},{"_id":"public/css/main.css","hash":"3af86a23da70ed69f0298ea50c5b162ccb9ed814","modified":1700831729134},{"_id":"public/dist/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1700831729134},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1700831729134},{"_id":"public/lib/three/canvas_lines.min.js","hash":"ae6584edc0418d68731cab82c1494f26bd77c07d","modified":1700831729134},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"186c3bd6ae352d336cdbd0e555ee76a844854c94","modified":1700831729134},{"_id":"public/lib/three/three-waves.min.js","hash":"329483be97cdda030779da9a6cd1e3eae645cf4f","modified":1700831729134},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1700831729134},{"_id":"public/lib/three/lib/CanvasRenderer.js","hash":"cf8e1ce6e884023ad0d692cf30f399862407fb40","modified":1700831729134},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1700831729134},{"_id":"public/lib/three/lib/Projector.js","hash":"1ad16e96cea2a8a9155bb429c83ef9bdd341ce99","modified":1700831729134},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1700831729134},{"_id":"public/dist/APlayer.min.js.map","hash":"31a19da0f0cb6b00ec212eafa847f31af86788df","modified":1700831729134},{"_id":"public/images/background.jpg","hash":"2409adab05a1ca5dd6a2287b742c97f9fc409ea1","modified":1700831729134},{"_id":"public/images/back.jpg","hash":"cb01783caab9279a61bfcf4d4f07317cf528d2ce","modified":1700831729134}],"Category":[{"name":"PLC","_id":"clpcnaqcw0005gkva984539db"},{"name":"随笔","_id":"clpcnaqd5000bgkva48vp6gjk"},{"name":"实验","_id":"clpcnaqd8000igkvab3wlgxs4"},{"name":"搭建环境","_id":"clpcnaqdb000ogkva5td9gk86"},{"name":"博客搭建","_id":"clpcnaqdj0011gkva6tpf37jp"},{"name":"论文整理","_id":"clpcnaqdl0018gkvaa2w8di18"}],"Data":[{"_id":"styles","data":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}],"Page":[{"title":"tags","date":"2023-02-28T08:06:08.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2023-02-28 16:06:08\ntype: \"tags\"\n---\n","updated":"2023-02-28T08:07:23.659Z","path":"tags/index.html","comments":1,"layout":"page","_id":"clpcnaqcd0000gkvageslc4bn","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":""},{"title":"categories","date":"2023-02-28T07:44:04.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2023-02-28 15:44:04\ntype: \"categories\"\n---\n","updated":"2023-03-07T09:24:49.751Z","path":"categories/index.html","comments":1,"layout":"page","_id":"clpcnaqcp0002gkvafzh82rbu","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":""}],"Post":[{"title":"Bilibili&yolo基础","date":"2023-04-26T11:38:32.000Z","_content":"","source":"_posts/Bilibili-yolo基础.md","raw":"---\ntitle: Bilibili&yolo基础\ndate: 2023-04-26 19:38:32\ntags:\n---\n","slug":"Bilibili-yolo基础","published":1,"updated":"2023-04-26T11:38:32.408Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqci0001gkvaa1j6femg","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":""},{"title":"PLC","date":"2023-04-14T12:40:56.000Z","_content":"\n# 参考\n\n1.  [电梯控制PLC编程_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1KP411G7eP/?spm_id_from=333.337.search-card.all.click&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n2.  [【实用】博途S7-1200 如何使用仿真功能 - 水木清扬 - 博客园 (cnblogs.com)](https://www.cnblogs.com/shuimuqingyang/p/14108119.html)\n\n# 准备工作\n\n## 下软件\n\n1. 装虚拟机\n\n2. 在虚拟机上装win10企业版系统\n\n3. 在虚拟机上装博图软件（包括PLC代码编程以及PLC仿真），和电梯仿真软件EET\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-14-20-45-17-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n## 通讯环境的搭建\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-49-33-image.png)\n\n让以太网的IPv4地址与子网掩码和ethernet保持一致\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-50-23-image.png)\n\n    \n","source":"_posts/PLC.md","raw":"---\ntitle: PLC\ndate: 2023-04-14 20:40:56\ntags:\ncategories: PLC\n---\n\n# 参考\n\n1.  [电梯控制PLC编程_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1KP411G7eP/?spm_id_from=333.337.search-card.all.click&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n2.  [【实用】博途S7-1200 如何使用仿真功能 - 水木清扬 - 博客园 (cnblogs.com)](https://www.cnblogs.com/shuimuqingyang/p/14108119.html)\n\n# 准备工作\n\n## 下软件\n\n1. 装虚拟机\n\n2. 在虚拟机上装win10企业版系统\n\n3. 在虚拟机上装博图软件（包括PLC代码编程以及PLC仿真），和电梯仿真软件EET\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-14-20-45-17-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n## 通讯环境的搭建\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-49-33-image.png)\n\n让以太网的IPv4地址与子网掩码和ethernet保持一致\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-50-23-image.png)\n\n    \n","slug":"PLC","published":1,"updated":"2023-04-14T12:53:24.940Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqcq0003gkvago921bgs","content":"<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><p><a href=\"https://www.bilibili.com/video/BV1KP411G7eP/?spm_id_from=333.337.search-card.all.click&vd_source=92031302c770784aa5d8337c1dffa13a\">电梯控制PLC编程_哔哩哔哩_bilibili</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/shuimuqingyang/p/14108119.html\">【实用】博途S7-1200 如何使用仿真功能 - 水木清扬 - 博客园 (cnblogs.com)</a></p>\n</li>\n</ol>\n<h1 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h1><h2 id=\"下软件\"><a href=\"#下软件\" class=\"headerlink\" title=\"下软件\"></a>下软件</h2><ol>\n<li><p>装虚拟机</p>\n</li>\n<li><p>在虚拟机上装win10企业版系统</p>\n</li>\n<li><p>在虚拟机上装博图软件（包括PLC代码编程以及PLC仿真），和电梯仿真软件EET</p>\n</li>\n</ol>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-14-20-45-17-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<h2 id=\"通讯环境的搭建\"><a href=\"#通讯环境的搭建\" class=\"headerlink\" title=\"通讯环境的搭建\"></a>通讯环境的搭建</h2><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-49-33-image.png\"></p>\n<p>让以太网的IPv4地址与子网掩码和ethernet保持一致</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-50-23-image.png\"></p>\n<p>    </p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ol>\n<li><p><a href=\"https://www.bilibili.com/video/BV1KP411G7eP/?spm_id_from=333.337.search-card.all.click&vd_source=92031302c770784aa5d8337c1dffa13a\">电梯控制PLC编程_哔哩哔哩_bilibili</a></p>\n</li>\n<li><p><a href=\"https://www.cnblogs.com/shuimuqingyang/p/14108119.html\">【实用】博途S7-1200 如何使用仿真功能 - 水木清扬 - 博客园 (cnblogs.com)</a></p>\n</li>\n</ol>\n<h1 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h1><h2 id=\"下软件\"><a href=\"#下软件\" class=\"headerlink\" title=\"下软件\"></a>下软件</h2><ol>\n<li><p>装虚拟机</p>\n</li>\n<li><p>在虚拟机上装win10企业版系统</p>\n</li>\n<li><p>在虚拟机上装博图软件（包括PLC代码编程以及PLC仿真），和电梯仿真软件EET</p>\n</li>\n</ol>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-14-20-45-17-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<h2 id=\"通讯环境的搭建\"><a href=\"#通讯环境的搭建\" class=\"headerlink\" title=\"通讯环境的搭建\"></a>通讯环境的搭建</h2><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-49-33-image.png\"></p>\n<p>让以太网的IPv4地址与子网掩码和ethernet保持一致</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-14-20-50-23-image.png\"></p>\n<p>    </p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2023-01-25T09:26:22.817Z","updated":"2023-01-25T09:25:53.676Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqcu0004gkva7ufmfbx5","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"论文整理","date":"2023-07-03T07:46:38.000Z","_content":"\n曲率和法向等微分信息估算是点云数据处理的基础。\n\n三维视觉表面缺陷检测技术和逆向工程技术具有许多共性。\n\n基于点云数据缺陷检测流程图\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-07-03-16-27-45-`PD]GZ7_M0KX8DS}ZELKIGA.png)\n\n数据配准算法：ICP\n","source":"_posts/产品表面缺陷视觉检测数据处理关键技术研究.md","raw":"---\ntitle: 论文整理\ndate: 2023-07-03 15:46:38\ntags:\n---\n\n曲率和法向等微分信息估算是点云数据处理的基础。\n\n三维视觉表面缺陷检测技术和逆向工程技术具有许多共性。\n\n基于点云数据缺陷检测流程图\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-07-03-16-27-45-`PD]GZ7_M0KX8DS}ZELKIGA.png)\n\n数据配准算法：ICP\n","slug":"产品表面缺陷视觉检测数据处理关键技术研究","published":1,"updated":"2023-07-04T13:58:29.725Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqcz0006gkva85ey5h2k","content":"<p>曲率和法向等微分信息估算是点云数据处理的基础。</p>\n<p>三维视觉表面缺陷检测技术和逆向工程技术具有许多共性。</p>\n<p>基于点云数据缺陷检测流程图</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-07-03-16-27-45-`PD]GZ7_M0KX8DS}ZELKIGA.png\"></p>\n<p>数据配准算法：ICP</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>曲率和法向等微分信息估算是点云数据处理的基础。</p>\n<p>三维视觉表面缺陷检测技术和逆向工程技术具有许多共性。</p>\n<p>基于点云数据缺陷检测流程图</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-07-03-16-27-45-`PD]GZ7_M0KX8DS}ZELKIGA.png\"></p>\n<p>数据配准算法：ICP</p>\n"},{"title":"yolov5代码分析","date":"2023-05-18T08:06:44.000Z","_content":"\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-19-10-03-58-image.png\" alt=\"\" data-align=\"inline\">\n\n`class Detect(nn.Module):`\n\n`class Segment(Detect):`\n\n`class BaseModel(nn.Module):`\n\n`class DetectionModel(BaseModel):`\n\n`class SegmentationModel(DetectionModel):`\n\n`class ClassificationModel(BaseModel): `\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-18-16-18-37-image.png\" title=\"\" alt=\"\" data-align=\"left\">\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-30-31-image.png)\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-42-48-image.png)\n","source":"_posts/yolov5代码分析.md","raw":"---\ntitle: yolov5代码分析\ndate: 2023-05-18 16:06:44\ntags: yolo基础\n---\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-19-10-03-58-image.png\" alt=\"\" data-align=\"inline\">\n\n`class Detect(nn.Module):`\n\n`class Segment(Detect):`\n\n`class BaseModel(nn.Module):`\n\n`class DetectionModel(BaseModel):`\n\n`class SegmentationModel(DetectionModel):`\n\n`class ClassificationModel(BaseModel): `\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-18-16-18-37-image.png\" title=\"\" alt=\"\" data-align=\"left\">\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-30-31-image.png)\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-42-48-image.png)\n","slug":"yolov5代码分析","published":1,"updated":"2023-05-19T13:15:36.109Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd00007gkvab0512fl0","content":"<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-19-10-03-58-image.png\" alt=\"\" data-align=\"inline\">\n\n<p><code>class Detect(nn.Module):</code></p>\n<p><code>class Segment(Detect):</code></p>\n<p><code>class BaseModel(nn.Module):</code></p>\n<p><code>class DetectionModel(BaseModel):</code></p>\n<p><code>class SegmentationModel(DetectionModel):</code></p>\n<p><code>class ClassificationModel(BaseModel): </code></p>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-18-16-18-37-image.png\" title=\"\" alt=\"\" data-align=\"left\">\n\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-30-31-image.png\"></p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-42-48-image.png\"></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-19-10-03-58-image.png\" alt=\"\" data-align=\"inline\">\n\n<p><code>class Detect(nn.Module):</code></p>\n<p><code>class Segment(Detect):</code></p>\n<p><code>class BaseModel(nn.Module):</code></p>\n<p><code>class DetectionModel(BaseModel):</code></p>\n<p><code>class SegmentationModel(DetectionModel):</code></p>\n<p><code>class ClassificationModel(BaseModel): </code></p>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-18-16-18-37-image.png\" title=\"\" alt=\"\" data-align=\"left\">\n\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-30-31-image.png\"></p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-18-16-42-48-image.png\"></p>\n"},{"title":"去看论文吧","date":"2023-02-28T07:48:59.000Z","_content":"\n# 一天一篇论文打卡\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| ×   | ×   | ×   | ×   | ×   | √   | ×   |\n| ×   | √   | ×   | ×   | ×   | ×   | ×   |\n\n3.13\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n\n# 2023希望养成的好习惯\n\n## 上厕所不看手机\n\n## 9点后不玩手机\n\n3.13\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n","source":"_posts/去看论文吧.md","raw":"---\ntitle: 去看论文吧\ndate: 2023-02-28 15:48:59\ntags: 打卡\ncategories: 随笔\n---\n\n# 一天一篇论文打卡\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| ×   | ×   | ×   | ×   | ×   | √   | ×   |\n| ×   | √   | ×   | ×   | ×   | ×   | ×   |\n\n3.13\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n\n# 2023希望养成的好习惯\n\n## 上厕所不看手机\n\n## 9点后不玩手机\n\n3.13\n\n| 周一  | 周二  | 周三  | 周四  | 周五  | 周六  | 周日  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n|     |     |     |     |     |     |     |\n","slug":"去看论文吧","published":1,"updated":"2023-03-12T10:13:59.341Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd20008gkva3rfv7t1n","content":"<h1 id=\"一天一篇论文打卡\"><a href=\"#一天一篇论文打卡\" class=\"headerlink\" title=\"一天一篇论文打卡\"></a>一天一篇论文打卡</h1><table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">√</td>\n<td align=\"center\">×</td>\n</tr>\n<tr>\n<td align=\"center\">×</td>\n<td align=\"center\">√</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n</tr>\n</tbody></table>\n<p>3.13</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<h1 id=\"2023希望养成的好习惯\"><a href=\"#2023希望养成的好习惯\" class=\"headerlink\" title=\"2023希望养成的好习惯\"></a>2023希望养成的好习惯</h1><h2 id=\"上厕所不看手机\"><a href=\"#上厕所不看手机\" class=\"headerlink\" title=\"上厕所不看手机\"></a>上厕所不看手机</h2><h2 id=\"9点后不玩手机\"><a href=\"#9点后不玩手机\" class=\"headerlink\" title=\"9点后不玩手机\"></a>9点后不玩手机</h2><p>3.13</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"一天一篇论文打卡\"><a href=\"#一天一篇论文打卡\" class=\"headerlink\" title=\"一天一篇论文打卡\"></a>一天一篇论文打卡</h1><table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">√</td>\n<td align=\"center\">×</td>\n</tr>\n<tr>\n<td align=\"center\">×</td>\n<td align=\"center\">√</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n<td align=\"center\">×</td>\n</tr>\n</tbody></table>\n<p>3.13</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n<h1 id=\"2023希望养成的好习惯\"><a href=\"#2023希望养成的好习惯\" class=\"headerlink\" title=\"2023希望养成的好习惯\"></a>2023希望养成的好习惯</h1><h2 id=\"上厕所不看手机\"><a href=\"#上厕所不看手机\" class=\"headerlink\" title=\"上厕所不看手机\"></a>上厕所不看手机</h2><h2 id=\"9点后不玩手机\"><a href=\"#9点后不玩手机\" class=\"headerlink\" title=\"9点后不玩手机\"></a>9点后不玩手机</h2><p>3.13</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">周一</th>\n<th align=\"center\">周二</th>\n<th align=\"center\">周三</th>\n<th align=\"center\">周四</th>\n<th align=\"center\">周五</th>\n<th align=\"center\">周六</th>\n<th align=\"center\">周日</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n<td align=\"center\"></td>\n</tr>\n</tbody></table>\n"},{"title":"基于图像的三维重建——基于图像的立体视觉研究","date":"2023-06-10T12:27:46.000Z","_content":"\n随着科技的发展，人们生活水平的进步，二维图像已经满足不了人类的需求了，三维立体图像应运而生。\n\n获取三维立体信息，可以根据不同的特性大致分为以下三类\n\n1. 利用三维建模应用软件（GBM），但得先知道场景中事物的具体信息，比如物体的大小，形状，位置等。像3Dmax，AutoCAD等，都是利用一些基本的几何元素（长方体，球等），再经过一系列几何操作来构造出复杂的实物模型。操作复杂，且由于使用的是基本的几何体元素，所以精度和真实感都不高。\n\n2. 通过复杂的三维扫描系统来获取三维立体信息。这种方法是用激光或红外来测量物体与仪器间的距离，从而生成一个深度曲面。精度较高，但设备昂贵。\n\n3. 基于图像的三维重建方法（IBM），这种方法是先采集生活中的二维图像，再根据图像中的信息和已知的参数来重建物体模型。自动化程度高，计算机可以独立完成建模过程中的大部分数据处理工作。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-10-21-18-53-1686403126776.png\" alt=\"\" data-align=\"center\">\n","source":"_posts/基于图像的三维重建——基于图像的立体视觉研究.md","raw":"---\ntitle: 基于图像的三维重建——基于图像的立体视觉研究\ndate: 2023-06-10 20:27:46\ntags: 三维重建\n---\n\n随着科技的发展，人们生活水平的进步，二维图像已经满足不了人类的需求了，三维立体图像应运而生。\n\n获取三维立体信息，可以根据不同的特性大致分为以下三类\n\n1. 利用三维建模应用软件（GBM），但得先知道场景中事物的具体信息，比如物体的大小，形状，位置等。像3Dmax，AutoCAD等，都是利用一些基本的几何元素（长方体，球等），再经过一系列几何操作来构造出复杂的实物模型。操作复杂，且由于使用的是基本的几何体元素，所以精度和真实感都不高。\n\n2. 通过复杂的三维扫描系统来获取三维立体信息。这种方法是用激光或红外来测量物体与仪器间的距离，从而生成一个深度曲面。精度较高，但设备昂贵。\n\n3. 基于图像的三维重建方法（IBM），这种方法是先采集生活中的二维图像，再根据图像中的信息和已知的参数来重建物体模型。自动化程度高，计算机可以独立完成建模过程中的大部分数据处理工作。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-10-21-18-53-1686403126776.png\" alt=\"\" data-align=\"center\">\n","slug":"基于图像的三维重建——基于图像的立体视觉研究","published":1,"updated":"2023-06-10T13:21:14.938Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd4000agkva5y3v8ffy","content":"<p>随着科技的发展，人们生活水平的进步，二维图像已经满足不了人类的需求了，三维立体图像应运而生。</p>\n<p>获取三维立体信息，可以根据不同的特性大致分为以下三类</p>\n<ol>\n<li><p>利用三维建模应用软件（GBM），但得先知道场景中事物的具体信息，比如物体的大小，形状，位置等。像3Dmax，AutoCAD等，都是利用一些基本的几何元素（长方体，球等），再经过一系列几何操作来构造出复杂的实物模型。操作复杂，且由于使用的是基本的几何体元素，所以精度和真实感都不高。</p>\n</li>\n<li><p>通过复杂的三维扫描系统来获取三维立体信息。这种方法是用激光或红外来测量物体与仪器间的距离，从而生成一个深度曲面。精度较高，但设备昂贵。</p>\n</li>\n<li><p>基于图像的三维重建方法（IBM），这种方法是先采集生活中的二维图像，再根据图像中的信息和已知的参数来重建物体模型。自动化程度高，计算机可以独立完成建模过程中的大部分数据处理工作。</p>\n</li>\n</ol>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-10-21-18-53-1686403126776.png\" alt=\"\" data-align=\"center\">\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>随着科技的发展，人们生活水平的进步，二维图像已经满足不了人类的需求了，三维立体图像应运而生。</p>\n<p>获取三维立体信息，可以根据不同的特性大致分为以下三类</p>\n<ol>\n<li><p>利用三维建模应用软件（GBM），但得先知道场景中事物的具体信息，比如物体的大小，形状，位置等。像3Dmax，AutoCAD等，都是利用一些基本的几何元素（长方体，球等），再经过一系列几何操作来构造出复杂的实物模型。操作复杂，且由于使用的是基本的几何体元素，所以精度和真实感都不高。</p>\n</li>\n<li><p>通过复杂的三维扫描系统来获取三维立体信息。这种方法是用激光或红外来测量物体与仪器间的距离，从而生成一个深度曲面。精度较高，但设备昂贵。</p>\n</li>\n<li><p>基于图像的三维重建方法（IBM），这种方法是先采集生活中的二维图像，再根据图像中的信息和已知的参数来重建物体模型。自动化程度高，计算机可以独立完成建模过程中的大部分数据处理工作。</p>\n</li>\n</ol>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-10-21-18-53-1686403126776.png\" alt=\"\" data-align=\"center\">\n"},{"title":"手写数字识别","date":"2023-03-29T03:35:33.000Z","_content":"\n# 参考资料\n\n[基于卷积神经网络的手写数字识别（附数据集+完整代码+操作说明）_卷积神经网络进行手写数字识别_Hurri_cane的博客-CSDN博客](https://blog.csdn.net/ShakalakaPHD/article/details/110694933)\n\n# 主要步骤\n","source":"_posts/基于深度学习的图像识别.md","raw":"---\ntitle: 手写数字识别\ndate: 2023-03-29 11:35:33\ntags: \ncategories: 实验\n---\n\n# 参考资料\n\n[基于卷积神经网络的手写数字识别（附数据集+完整代码+操作说明）_卷积神经网络进行手写数字识别_Hurri_cane的博客-CSDN博客](https://blog.csdn.net/ShakalakaPHD/article/details/110694933)\n\n# 主要步骤\n","slug":"基于深度学习的图像识别","published":1,"updated":"2023-03-29T03:43:25.230Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd5000cgkva6771a7i6","content":"<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://blog.csdn.net/ShakalakaPHD/article/details/110694933\">基于卷积神经网络的手写数字识别（附数据集+完整代码+操作说明）_卷积神经网络进行手写数字识别_Hurri_cane的博客-CSDN博客</a></p>\n<h1 id=\"主要步骤\"><a href=\"#主要步骤\" class=\"headerlink\" title=\"主要步骤\"></a>主要步骤</h1>","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://blog.csdn.net/ShakalakaPHD/article/details/110694933\">基于卷积神经网络的手写数字识别（附数据集+完整代码+操作说明）_卷积神经网络进行手写数字识别_Hurri_cane的博客-CSDN博客</a></p>\n<h1 id=\"主要步骤\"><a href=\"#主要步骤\" class=\"headerlink\" title=\"主要步骤\"></a>主要步骤</h1>"},{"title":"基于结构光扫描的三维点云数据重构算法研究_徐吉轩","date":"2023-06-10T13:22:10.000Z","_content":"","source":"_posts/基于结构光扫描的三维点云数据重构算法研究.md","raw":"---\ntitle: 基于结构光扫描的三维点云数据重构算法研究_徐吉轩\ndate: 2023-06-10 21:22:10\ntags: 三维重建\n---\n","slug":"基于结构光扫描的三维点云数据重构算法研究","published":1,"updated":"2023-06-10T13:33:59.428Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd6000egkvad63r61ok","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":""},{"title":"搭建yolov5框架","date":"2023-03-16T14:11:48.000Z","_content":"\n# 参考资料\n\n[常用conda命令_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1GE411T7rQ/?p=3&spm_id_from=pageDriver&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n[(135条消息) 手把手教你使用YOLOV5训练自己的目标检测模型-口罩检测-视频教程_yolov5模型训练_肆十二的博客-CSDN博客](https://blog.csdn.net/ECHOSON/article/details/121939535)\n\n# 基本步骤\n\n1.在gitee下载yolov5源代码，创建一个专门的文件夹\n\n2.在该文件夹的地址下进行cmd \n\n3.用conda常用命令创建一个专门为yolov5的虚拟环境，然后在这个环境中安装各种依赖包\n\n4.在pycharm中打开yolov5文件夹，然后选择刚才配置好的虚拟环境\n\n5.运行detect.py 并观察结果\n\n# 后续附加\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-22-17-55-59-6e1045938c678b103175169c8cc149e.png)\n","source":"_posts/搭建yolov5框架.md","raw":"---\ntitle: 搭建yolov5框架\ndate: 2023-03-16 22:11:48\ntags: yolo基础\ncategories: 搭建环境\n---\n\n# 参考资料\n\n[常用conda命令_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1GE411T7rQ/?p=3&spm_id_from=pageDriver&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n[(135条消息) 手把手教你使用YOLOV5训练自己的目标检测模型-口罩检测-视频教程_yolov5模型训练_肆十二的博客-CSDN博客](https://blog.csdn.net/ECHOSON/article/details/121939535)\n\n# 基本步骤\n\n1.在gitee下载yolov5源代码，创建一个专门的文件夹\n\n2.在该文件夹的地址下进行cmd \n\n3.用conda常用命令创建一个专门为yolov5的虚拟环境，然后在这个环境中安装各种依赖包\n\n4.在pycharm中打开yolov5文件夹，然后选择刚才配置好的虚拟环境\n\n5.运行detect.py 并观察结果\n\n# 后续附加\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-22-17-55-59-6e1045938c678b103175169c8cc149e.png)\n","slug":"搭建yolov5框架","published":1,"updated":"2023-04-26T11:33:04.064Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd7000hgkva467sh5cb","content":"<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://www.bilibili.com/video/BV1GE411T7rQ/?p=3&spm_id_from=pageDriver&vd_source=92031302c770784aa5d8337c1dffa13a\">常用conda命令_哔哩哔哩_bilibili</a></p>\n<p><a href=\"https://blog.csdn.net/ECHOSON/article/details/121939535\">(135条消息) 手把手教你使用YOLOV5训练自己的目标检测模型-口罩检测-视频教程_yolov5模型训练_肆十二的博客-CSDN博客</a></p>\n<h1 id=\"基本步骤\"><a href=\"#基本步骤\" class=\"headerlink\" title=\"基本步骤\"></a>基本步骤</h1><p>1.在gitee下载yolov5源代码，创建一个专门的文件夹</p>\n<p>2.在该文件夹的地址下进行cmd </p>\n<p>3.用conda常用命令创建一个专门为yolov5的虚拟环境，然后在这个环境中安装各种依赖包</p>\n<p>4.在pycharm中打开yolov5文件夹，然后选择刚才配置好的虚拟环境</p>\n<p>5.运行detect.py 并观察结果</p>\n<h1 id=\"后续附加\"><a href=\"#后续附加\" class=\"headerlink\" title=\"后续附加\"></a>后续附加</h1><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-22-17-55-59-6e1045938c678b103175169c8cc149e.png\"></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p><a href=\"https://www.bilibili.com/video/BV1GE411T7rQ/?p=3&spm_id_from=pageDriver&vd_source=92031302c770784aa5d8337c1dffa13a\">常用conda命令_哔哩哔哩_bilibili</a></p>\n<p><a href=\"https://blog.csdn.net/ECHOSON/article/details/121939535\">(135条消息) 手把手教你使用YOLOV5训练自己的目标检测模型-口罩检测-视频教程_yolov5模型训练_肆十二的博客-CSDN博客</a></p>\n<h1 id=\"基本步骤\"><a href=\"#基本步骤\" class=\"headerlink\" title=\"基本步骤\"></a>基本步骤</h1><p>1.在gitee下载yolov5源代码，创建一个专门的文件夹</p>\n<p>2.在该文件夹的地址下进行cmd </p>\n<p>3.用conda常用命令创建一个专门为yolov5的虚拟环境，然后在这个环境中安装各种依赖包</p>\n<p>4.在pycharm中打开yolov5文件夹，然后选择刚才配置好的虚拟环境</p>\n<p>5.运行detect.py 并观察结果</p>\n<h1 id=\"后续附加\"><a href=\"#后续附加\" class=\"headerlink\" title=\"后续附加\"></a>后续附加</h1><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-22-17-55-59-6e1045938c678b103175169c8cc149e.png\"></p>\n"},{"title":"搭建深度学习环境","date":"2023-03-15T12:55:24.000Z","_content":"\n# 参考文件：\n\n[Python深度学习：安装Anaconda、PyTorch（GPU版）与PyCharm_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n# 主要流程\n\n下载<u>anaconda</u>、<u>pytorch</u>、<u>pycharm</u>\n\n1.anaconda安装最新版本即可\n\n   下载完添加环境变量\n\n   配置 Jupyter notebook\n\n2.pytorch要按照自己电脑的GPU配置，python解[www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a](http://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a))释器的配置选择合适的版本\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-04-cca7116f309501e10e0a6cc5a4af104.png\" alt=\"\" width=\"272\">    <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-36-260e56c82abfac9ca1f48cd2372d335.png\" alt=\"\" width=\"278\">\n\n如图所示，我的电脑cuda版本为11.6   python解释器版本为3.9\n\n> pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n\n⚫ 第 1 个单词 pip3 代表我们是用 pip 下载的，而不是 conda；\n⚫ 第 3 到 5 个单词表示我们下载了 <u>torch</u>、<u>torchvision</u>、<u>torchaudio</u> 三大组件；\n⚫ 接着的 extra-index-url 后面的这个网站是指下载的地址。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-16-42-3c5c975106a104686bf43beb277bbe1.png\" alt=\"\" width=\"160\" data-align=\"center\">\n\n进入cuda 11.6的网站后，依次下载torch、torchvision、torchaudio 三大组件\n\n掌握 Torch 的版本是 1.12.0 后，为了确保之后下载的 torchvision 与 torchaudio\n可以适配这个 Torch，进入网址 https://pytorch.org/get-started/previous-versions/，\n搜索“pip install torch==1.12.0”，\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-24-07-4f6cd7f985c0783f27dc3aea04bcf69.png)\n\n可见适配的vision版本为0.13.0  audio版本为0.12.0\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-25-02-b6585178841196dd45adf31661c2d0c.png)\n\n其中cu116表示cuda版本为11.6         cp39表示python解释器版本为3.19.3\n\n再在cmd中依次输入以下代码进行安装\n\n```\npip install torch-1.12.0+cu116-cp39-cp39-win_amd64.whl\npip install torchaudio-0.12.0+cu116-cp39-cp39-win_amd64.whl\npip install torchvision-0.13.0+cu116-cp39-cp39-win_amd64.whl\n```\n","source":"_posts/搭建深度学习环境.md","raw":"---\ntitle: 搭建深度学习环境\ndate: 2023-03-15 20:55:24\ntags: yolo基础\ncategories: 搭建环境\n---\n\n# 参考文件：\n\n[Python深度学习：安装Anaconda、PyTorch（GPU版）与PyCharm_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a)\n\n# 主要流程\n\n下载<u>anaconda</u>、<u>pytorch</u>、<u>pycharm</u>\n\n1.anaconda安装最新版本即可\n\n   下载完添加环境变量\n\n   配置 Jupyter notebook\n\n2.pytorch要按照自己电脑的GPU配置，python解[www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a](http://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a))释器的配置选择合适的版本\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-04-cca7116f309501e10e0a6cc5a4af104.png\" alt=\"\" width=\"272\">    <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-36-260e56c82abfac9ca1f48cd2372d335.png\" alt=\"\" width=\"278\">\n\n如图所示，我的电脑cuda版本为11.6   python解释器版本为3.9\n\n> pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n\n⚫ 第 1 个单词 pip3 代表我们是用 pip 下载的，而不是 conda；\n⚫ 第 3 到 5 个单词表示我们下载了 <u>torch</u>、<u>torchvision</u>、<u>torchaudio</u> 三大组件；\n⚫ 接着的 extra-index-url 后面的这个网站是指下载的地址。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-16-42-3c5c975106a104686bf43beb277bbe1.png\" alt=\"\" width=\"160\" data-align=\"center\">\n\n进入cuda 11.6的网站后，依次下载torch、torchvision、torchaudio 三大组件\n\n掌握 Torch 的版本是 1.12.0 后，为了确保之后下载的 torchvision 与 torchaudio\n可以适配这个 Torch，进入网址 https://pytorch.org/get-started/previous-versions/，\n搜索“pip install torch==1.12.0”，\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-24-07-4f6cd7f985c0783f27dc3aea04bcf69.png)\n\n可见适配的vision版本为0.13.0  audio版本为0.12.0\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-25-02-b6585178841196dd45adf31661c2d0c.png)\n\n其中cu116表示cuda版本为11.6         cp39表示python解释器版本为3.19.3\n\n再在cmd中依次输入以下代码进行安装\n\n```\npip install torch-1.12.0+cu116-cp39-cp39-win_amd64.whl\npip install torchaudio-0.12.0+cu116-cp39-cp39-win_amd64.whl\npip install torchvision-0.13.0+cu116-cp39-cp39-win_amd64.whl\n```\n","slug":"搭建深度学习环境","published":1,"updated":"2023-04-26T11:33:04.065Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqd9000jgkvabquq340k","content":"<h1 id=\"参考文件：\"><a href=\"#参考文件：\" class=\"headerlink\" title=\"参考文件：\"></a>参考文件：</h1><p><a href=\"https://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a\">Python深度学习：安装Anaconda、PyTorch（GPU版）与PyCharm_哔哩哔哩_bilibili</a></p>\n<h1 id=\"主要流程\"><a href=\"#主要流程\" class=\"headerlink\" title=\"主要流程\"></a>主要流程</h1><p>下载<u>anaconda</u>、<u>pytorch</u>、<u>pycharm</u></p>\n<p>1.anaconda安装最新版本即可</p>\n<p>   下载完添加环境变量</p>\n<p>   配置 Jupyter notebook</p>\n<p>2.pytorch要按照自己电脑的GPU配置，python解<a href=\"http://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a\">www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from&#x3D;333.788.recommend_more_video.0&amp;vd_source&#x3D;92031302c770784aa5d8337c1dffa13a</a>)释器的配置选择合适的版本</p>\n<p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-04-cca7116f309501e10e0a6cc5a4af104.png\" alt=\"\" width=\"272\">    <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-36-260e56c82abfac9ca1f48cd2372d335.png\" alt=\"\" width=\"278\"></p>\n<p>如图所示，我的电脑cuda版本为11.6   python解释器版本为3.9</p>\n<blockquote>\n<p>pip3 install torch torchvision torchaudio –extra-index-url <a href=\"https://download.pytorch.org/whl/cu116\">https://download.pytorch.org/whl/cu116</a></p>\n</blockquote>\n<p>⚫ 第 1 个单词 pip3 代表我们是用 pip 下载的，而不是 conda；<br>⚫ 第 3 到 5 个单词表示我们下载了 <u>torch</u>、<u>torchvision</u>、<u>torchaudio</u> 三大组件；<br>⚫ 接着的 extra-index-url 后面的这个网站是指下载的地址。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-16-42-3c5c975106a104686bf43beb277bbe1.png\" alt=\"\" width=\"160\" data-align=\"center\">\n\n<p>进入cuda 11.6的网站后，依次下载torch、torchvision、torchaudio 三大组件</p>\n<p>掌握 Torch 的版本是 1.12.0 后，为了确保之后下载的 torchvision 与 torchaudio<br>可以适配这个 Torch，进入网址 <a href=\"https://pytorch.org/get-started/previous-versions/%EF%BC%8C\">https://pytorch.org/get-started/previous-versions/，</a><br>搜索“pip install torch&#x3D;&#x3D;1.12.0”，</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-24-07-4f6cd7f985c0783f27dc3aea04bcf69.png\"></p>\n<p>可见适配的vision版本为0.13.0  audio版本为0.12.0</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-25-02-b6585178841196dd45adf31661c2d0c.png\"></p>\n<p>其中cu116表示cuda版本为11.6         cp39表示python解释器版本为3.19.3</p>\n<p>再在cmd中依次输入以下代码进行安装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch-1.12.0+cu116-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install torchaudio-0.12.0+cu116-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install torchvision-0.13.0+cu116-cp39-cp39-win_amd64.whl</span><br></pre></td></tr></table></figure>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"参考文件：\"><a href=\"#参考文件：\" class=\"headerlink\" title=\"参考文件：\"></a>参考文件：</h1><p><a href=\"https://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a\">Python深度学习：安装Anaconda、PyTorch（GPU版）与PyCharm_哔哩哔哩_bilibili</a></p>\n<h1 id=\"主要流程\"><a href=\"#主要流程\" class=\"headerlink\" title=\"主要流程\"></a>主要流程</h1><p>下载<u>anaconda</u>、<u>pytorch</u>、<u>pycharm</u></p>\n<p>1.anaconda安装最新版本即可</p>\n<p>   下载完添加环境变量</p>\n<p>   配置 Jupyter notebook</p>\n<p>2.pytorch要按照自己电脑的GPU配置，python解<a href=\"http://www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from=333.788.recommend_more_video.0&vd_source=92031302c770784aa5d8337c1dffa13a\">www.bilibili.com/video/BV1cD4y1H7Tk/?spm_id_from&#x3D;333.788.recommend_more_video.0&amp;vd_source&#x3D;92031302c770784aa5d8337c1dffa13a</a>)释器的配置选择合适的版本</p>\n<p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-04-cca7116f309501e10e0a6cc5a4af104.png\" alt=\"\" width=\"272\">    <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-08-36-260e56c82abfac9ca1f48cd2372d335.png\" alt=\"\" width=\"278\"></p>\n<p>如图所示，我的电脑cuda版本为11.6   python解释器版本为3.9</p>\n<blockquote>\n<p>pip3 install torch torchvision torchaudio –extra-index-url <a href=\"https://download.pytorch.org/whl/cu116\">https://download.pytorch.org/whl/cu116</a></p>\n</blockquote>\n<p>⚫ 第 1 个单词 pip3 代表我们是用 pip 下载的，而不是 conda；<br>⚫ 第 3 到 5 个单词表示我们下载了 <u>torch</u>、<u>torchvision</u>、<u>torchaudio</u> 三大组件；<br>⚫ 接着的 extra-index-url 后面的这个网站是指下载的地址。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-15-21-16-42-3c5c975106a104686bf43beb277bbe1.png\" alt=\"\" width=\"160\" data-align=\"center\">\n\n<p>进入cuda 11.6的网站后，依次下载torch、torchvision、torchaudio 三大组件</p>\n<p>掌握 Torch 的版本是 1.12.0 后，为了确保之后下载的 torchvision 与 torchaudio<br>可以适配这个 Torch，进入网址 <a href=\"https://pytorch.org/get-started/previous-versions/%EF%BC%8C\">https://pytorch.org/get-started/previous-versions/，</a><br>搜索“pip install torch&#x3D;&#x3D;1.12.0”，</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-24-07-4f6cd7f985c0783f27dc3aea04bcf69.png\"></p>\n<p>可见适配的vision版本为0.13.0  audio版本为0.12.0</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-15-21-25-02-b6585178841196dd45adf31661c2d0c.png\"></p>\n<p>其中cu116表示cuda版本为11.6         cp39表示python解释器版本为3.19.3</p>\n<p>再在cmd中依次输入以下代码进行安装</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install torch-1.12.0+cu116-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install torchaudio-0.12.0+cu116-cp39-cp39-win_amd64.whl</span><br><span class=\"line\">pip install torchvision-0.13.0+cu116-cp39-cp39-win_amd64.whl</span><br></pre></td></tr></table></figure>\n"},{"title":"方向研究","date":"2023-05-19T07:44:38.000Z","_content":"\n开发了一种将双目结构光相机采集的高密度点云数据转化为深度图像的新方法，构建了训练深度学习模型的焊缝表面缺陷深度图像数据集和用于对比的彩色图像数据集\n","source":"_posts/方向研究.md","raw":"---\ntitle: 方向研究\ndate: 2023-05-19 15:44:38\ntags:\n---\n\n开发了一种将双目结构光相机采集的高密度点云数据转化为深度图像的新方法，构建了训练深度学习模型的焊缝表面缺陷深度图像数据集和用于对比的彩色图像数据集\n","slug":"方向研究","published":1,"updated":"2023-05-19T13:11:30.752Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqda000mgkvadcqz98xf","content":"<p>开发了一种将双目结构光相机采集的高密度点云数据转化为深度图像的新方法，构建了训练深度学习模型的焊缝表面缺陷深度图像数据集和用于对比的彩色图像数据集</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>开发了一种将双目结构光相机采集的高密度点云数据转化为深度图像的新方法，构建了训练深度学习模型的焊缝表面缺陷深度图像数据集和用于对比的彩色图像数据集</p>\n"},{"title":"时间序列","date":"2023-05-22T08:41:56.000Z","_content":"\n绝大部分行业场景，尤其是互联网、量化行业，每天都会产生大量的数据。金融领域股票价格随时间的走势；电商行业每日的销售额；旅游行业随着节假日周期变化的机票酒店价格等；\n\n我们称这种不同时间收到的，描述一个或多种特征随着时间发生变化的数据，为时间序列数据（Time Series Data）。\n\n而时间序列预测做的就是**通过多种维度的数据本身内在与时间的关联特性**，**利用历史的数据预测未来**这么一件事情。\n\nTransformer是Google 在 2017 年提出的一种 用于NLP任务的模型，可以用来做时间序列预测。它可以关注到序列的长期依赖信息，且支持并行化计算，但其本身没有序列的概念，需要增加position embedding是网络学习到序列位置信息\n","source":"_posts/时间序列.md","raw":"---\ntitle: 时间序列\ndate: 2023-05-22 16:41:56\ntags:\n---\n\n绝大部分行业场景，尤其是互联网、量化行业，每天都会产生大量的数据。金融领域股票价格随时间的走势；电商行业每日的销售额；旅游行业随着节假日周期变化的机票酒店价格等；\n\n我们称这种不同时间收到的，描述一个或多种特征随着时间发生变化的数据，为时间序列数据（Time Series Data）。\n\n而时间序列预测做的就是**通过多种维度的数据本身内在与时间的关联特性**，**利用历史的数据预测未来**这么一件事情。\n\nTransformer是Google 在 2017 年提出的一种 用于NLP任务的模型，可以用来做时间序列预测。它可以关注到序列的长期依赖信息，且支持并行化计算，但其本身没有序列的概念，需要增加position embedding是网络学习到序列位置信息\n","slug":"时间序列","published":1,"updated":"2023-05-22T11:43:04.272Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdc000qgkva8z6xazb6","content":"<p>绝大部分行业场景，尤其是互联网、量化行业，每天都会产生大量的数据。金融领域股票价格随时间的走势；电商行业每日的销售额；旅游行业随着节假日周期变化的机票酒店价格等；</p>\n<p>我们称这种不同时间收到的，描述一个或多种特征随着时间发生变化的数据，为时间序列数据（Time Series Data）。</p>\n<p>而时间序列预测做的就是<strong>通过多种维度的数据本身内在与时间的关联特性</strong>，<strong>利用历史的数据预测未来</strong>这么一件事情。</p>\n<p>Transformer是Google 在 2017 年提出的一种 用于NLP任务的模型，可以用来做时间序列预测。它可以关注到序列的长期依赖信息，且支持并行化计算，但其本身没有序列的概念，需要增加position embedding是网络学习到序列位置信息</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>绝大部分行业场景，尤其是互联网、量化行业，每天都会产生大量的数据。金融领域股票价格随时间的走势；电商行业每日的销售额；旅游行业随着节假日周期变化的机票酒店价格等；</p>\n<p>我们称这种不同时间收到的，描述一个或多种特征随着时间发生变化的数据，为时间序列数据（Time Series Data）。</p>\n<p>而时间序列预测做的就是<strong>通过多种维度的数据本身内在与时间的关联特性</strong>，<strong>利用历史的数据预测未来</strong>这么一件事情。</p>\n<p>Transformer是Google 在 2017 年提出的一种 用于NLP任务的模型，可以用来做时间序列预测。它可以关注到序列的长期依赖信息，且支持并行化计算，但其本身没有序列的概念，需要增加position embedding是网络学习到序列位置信息</p>\n"},{"title":"添加音乐","date":"2023-03-04T05:43:43.000Z","_content":"\n## 参考文档\n\n[(131条消息) Hexo next主题中添加播放器Aplayer_Stride Max Zz的博客-CSDN博客](https://blog.csdn.net/qq_35324057/article/details/104124723?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-3-104124723-blog-107868192.pc_relevant_landingrelevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-3-104124723-blog-107868192.pc_relevant_landingrelevant&utm_relevant_index=6)\n\n[踩坑记--hexo中加入音乐 - 简书 (jianshu.com)](https://www.jianshu.com/p/3fb29cc7a00b)\n\n## 简单总结\n\n大体操作按照链接一，再按照链接二生成本地音乐外链，替换链接一的音乐链，就可以播放自己喜欢的音乐了。\n","source":"_posts/添加音乐.md","raw":"---\ntitle: 添加音乐\ndate: 2023-03-04 13:43:43\ntags: aplayer leancloud\ncategories: 博客搭建\n---\n\n## 参考文档\n\n[(131条消息) Hexo next主题中添加播放器Aplayer_Stride Max Zz的博客-CSDN博客](https://blog.csdn.net/qq_35324057/article/details/104124723?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-3-104124723-blog-107868192.pc_relevant_landingrelevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-3-104124723-blog-107868192.pc_relevant_landingrelevant&utm_relevant_index=6)\n\n[踩坑记--hexo中加入音乐 - 简书 (jianshu.com)](https://www.jianshu.com/p/3fb29cc7a00b)\n\n## 简单总结\n\n大体操作按照链接一，再按照链接二生成本地音乐外链，替换链接一的音乐链，就可以播放自己喜欢的音乐了。\n","slug":"添加音乐","published":1,"updated":"2023-03-04T06:11:04.523Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdf000tgkva38iiguqf","content":"<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p><a href=\"https://blog.csdn.net/qq_35324057/article/details/104124723?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~ESLANDING~default-3-104124723-blog-107868192.pc_relevant_landingrelevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~ESLANDING~default-3-104124723-blog-107868192.pc_relevant_landingrelevant&utm_relevant_index=6\">(131条消息) Hexo next主题中添加播放器Aplayer_Stride Max Zz的博客-CSDN博客</a></p>\n<p><a href=\"https://www.jianshu.com/p/3fb29cc7a00b\">踩坑记–hexo中加入音乐 - 简书 (jianshu.com)</a></p>\n<h2 id=\"简单总结\"><a href=\"#简单总结\" class=\"headerlink\" title=\"简单总结\"></a>简单总结</h2><p>大体操作按照链接一，再按照链接二生成本地音乐外链，替换链接一的音乐链，就可以播放自己喜欢的音乐了。</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h2 id=\"参考文档\"><a href=\"#参考文档\" class=\"headerlink\" title=\"参考文档\"></a>参考文档</h2><p><a href=\"https://blog.csdn.net/qq_35324057/article/details/104124723?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~ESLANDING~default-3-104124723-blog-107868192.pc_relevant_landingrelevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~ESLANDING~default-3-104124723-blog-107868192.pc_relevant_landingrelevant&utm_relevant_index=6\">(131条消息) Hexo next主题中添加播放器Aplayer_Stride Max Zz的博客-CSDN博客</a></p>\n<p><a href=\"https://www.jianshu.com/p/3fb29cc7a00b\">踩坑记–hexo中加入音乐 - 简书 (jianshu.com)</a></p>\n<h2 id=\"简单总结\"><a href=\"#简单总结\" class=\"headerlink\" title=\"简单总结\"></a>简单总结</h2><p>大体操作按照链接一，再按照链接二生成本地音乐外链，替换链接一的音乐链，就可以播放自己喜欢的音乐了。</p>\n"},{"title":"论文整理","date":"2023-03-12T07:09:22.000Z","_content":"\n# 目标\n\n然而表面缺陷检测的相关技术并未随着图像处理技术的发展而快速发展，其原因是在相关技术转化为生产力的过程中未能更多地结合实际生产的需要做出改进，也不能克服生产环境的变化给这些技术实现带来的障碍。\n\n带钢表面缺陷检测系统的发展不仅是结构构架、缺陷检测和分类能力的提高，更重要\n是能使用合适的方法解决应用过程中出现的问题，问题的解决也能直接推动着系统的优化。\n\n# 任务\n\n- [ ] 了解国内外带钢表面缺陷检测技术的发展情况，并分析未来趋势\n\n- [ ] 了解带钢表面缺陷检测中典型的图像检测分类方法以及这些方法适用的范围\n\n- [ ] 了解带钢表面缺陷样貌，形成机理，位置分布\n\n- [ ] 了解带钢表面缺陷检测系统的结构\n\n# 存在的问题\n\n边界检测\n","source":"_posts/论文整理-2.md","raw":"---\ntitle: 论文整理\ndate: 2023-03-12 15:09:22\ntags: 带钢缺陷检测\ncategories: 论文整理\n---\n\n# 目标\n\n然而表面缺陷检测的相关技术并未随着图像处理技术的发展而快速发展，其原因是在相关技术转化为生产力的过程中未能更多地结合实际生产的需要做出改进，也不能克服生产环境的变化给这些技术实现带来的障碍。\n\n带钢表面缺陷检测系统的发展不仅是结构构架、缺陷检测和分类能力的提高，更重要\n是能使用合适的方法解决应用过程中出现的问题，问题的解决也能直接推动着系统的优化。\n\n# 任务\n\n- [ ] 了解国内外带钢表面缺陷检测技术的发展情况，并分析未来趋势\n\n- [ ] 了解带钢表面缺陷检测中典型的图像检测分类方法以及这些方法适用的范围\n\n- [ ] 了解带钢表面缺陷样貌，形成机理，位置分布\n\n- [ ] 了解带钢表面缺陷检测系统的结构\n\n# 存在的问题\n\n边界检测\n","slug":"论文整理-2","published":1,"updated":"2023-04-26T11:33:04.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdh000xgkva4do9b6vl","content":"<h1 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h1><p>然而表面缺陷检测的相关技术并未随着图像处理技术的发展而快速发展，其原因是在相关技术转化为生产力的过程中未能更多地结合实际生产的需要做出改进，也不能克服生产环境的变化给这些技术实现带来的障碍。</p>\n<p>带钢表面缺陷检测系统的发展不仅是结构构架、缺陷检测和分类能力的提高，更重要<br>是能使用合适的方法解决应用过程中出现的问题，问题的解决也能直接推动着系统的优化。</p>\n<h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解国内外带钢表面缺陷检测技术的发展情况，并分析未来趋势</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷检测中典型的图像检测分类方法以及这些方法适用的范围</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷样貌，形成机理，位置分布</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷检测系统的结构</p>\n</li>\n</ul>\n<h1 id=\"存在的问题\"><a href=\"#存在的问题\" class=\"headerlink\" title=\"存在的问题\"></a>存在的问题</h1><p>边界检测</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h1><p>然而表面缺陷检测的相关技术并未随着图像处理技术的发展而快速发展，其原因是在相关技术转化为生产力的过程中未能更多地结合实际生产的需要做出改进，也不能克服生产环境的变化给这些技术实现带来的障碍。</p>\n<p>带钢表面缺陷检测系统的发展不仅是结构构架、缺陷检测和分类能力的提高，更重要<br>是能使用合适的方法解决应用过程中出现的问题，问题的解决也能直接推动着系统的优化。</p>\n<h1 id=\"任务\"><a href=\"#任务\" class=\"headerlink\" title=\"任务\"></a>任务</h1><ul>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解国内外带钢表面缺陷检测技术的发展情况，并分析未来趋势</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷检测中典型的图像检测分类方法以及这些方法适用的范围</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷样貌，形成机理，位置分布</p>\n</li>\n<li><p><input disabled=\"\" type=\"checkbox\"> \n了解带钢表面缺陷检测系统的结构</p>\n</li>\n</ul>\n<h1 id=\"存在的问题\"><a href=\"#存在的问题\" class=\"headerlink\" title=\"存在的问题\"></a>存在的问题</h1><p>边界检测</p>\n"},{"title":"论文整理","date":"2023-03-23T09:13:49.000Z","_content":"\n# 论文引用\n\n[1]周博. 基于深度学习的铝型材表面缺陷检测技术研究[D].武汉纺织大学,2022.DOI:10.27698/d.cnki.gwhxj.2022.000140.\n\n# 论文结构\n\n1. 阐述了铝型材表面缺陷检测任务背景\n\n2. 研究了铝型材表面缺陷检测装置。对装置的传送部分、图像采集部分、检测部分、分流、打标部分以及整体工作流程进行了介绍；其次，对装置中的主控模块、照明模块、图像采集模块、传动模块进行了详细的说明\n\n3. 改进了Faster-RCNN 网络的铝型材框架表面缺陷检测分类模型\n\n4. 总结与展望\n","source":"_posts/论文整理-3.md","raw":"---\ntitle: 论文整理\ndate: 2023-03-23 17:13:49\ntags:\n\n---\n\n# 论文引用\n\n[1]周博. 基于深度学习的铝型材表面缺陷检测技术研究[D].武汉纺织大学,2022.DOI:10.27698/d.cnki.gwhxj.2022.000140.\n\n# 论文结构\n\n1. 阐述了铝型材表面缺陷检测任务背景\n\n2. 研究了铝型材表面缺陷检测装置。对装置的传送部分、图像采集部分、检测部分、分流、打标部分以及整体工作流程进行了介绍；其次，对装置中的主控模块、照明模块、图像采集模块、传动模块进行了详细的说明\n\n3. 改进了Faster-RCNN 网络的铝型材框架表面缺陷检测分类模型\n\n4. 总结与展望\n","slug":"论文整理-3","published":1,"updated":"2023-04-26T11:33:04.159Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdi000zgkvagcrd8xdu","content":"<h1 id=\"论文引用\"><a href=\"#论文引用\" class=\"headerlink\" title=\"论文引用\"></a>论文引用</h1><p>[1]周博. 基于深度学习的铝型材表面缺陷检测技术研究[D].武汉纺织大学,2022.DOI:10.27698&#x2F;d.cnki.gwhxj.2022.000140.</p>\n<h1 id=\"论文结构\"><a href=\"#论文结构\" class=\"headerlink\" title=\"论文结构\"></a>论文结构</h1><ol>\n<li><p>阐述了铝型材表面缺陷检测任务背景</p>\n</li>\n<li><p>研究了铝型材表面缺陷检测装置。对装置的传送部分、图像采集部分、检测部分、分流、打标部分以及整体工作流程进行了介绍；其次，对装置中的主控模块、照明模块、图像采集模块、传动模块进行了详细的说明</p>\n</li>\n<li><p>改进了Faster-RCNN 网络的铝型材框架表面缺陷检测分类模型</p>\n</li>\n<li><p>总结与展望</p>\n</li>\n</ol>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"论文引用\"><a href=\"#论文引用\" class=\"headerlink\" title=\"论文引用\"></a>论文引用</h1><p>[1]周博. 基于深度学习的铝型材表面缺陷检测技术研究[D].武汉纺织大学,2022.DOI:10.27698&#x2F;d.cnki.gwhxj.2022.000140.</p>\n<h1 id=\"论文结构\"><a href=\"#论文结构\" class=\"headerlink\" title=\"论文结构\"></a>论文结构</h1><ol>\n<li><p>阐述了铝型材表面缺陷检测任务背景</p>\n</li>\n<li><p>研究了铝型材表面缺陷检测装置。对装置的传送部分、图像采集部分、检测部分、分流、打标部分以及整体工作流程进行了介绍；其次，对装置中的主控模块、照明模块、图像采集模块、传动模块进行了详细的说明</p>\n</li>\n<li><p>改进了Faster-RCNN 网络的铝型材框架表面缺陷检测分类模型</p>\n</li>\n<li><p>总结与展望</p>\n</li>\n</ol>\n"},{"title":"论文整理","date":"2023-07-05T03:42:21.000Z","_content":"","source":"_posts/论文整理-10.md","raw":"---\ntitle: 论文整理\ndate: 2023-07-05 11:42:21\ntags:\n---\n","slug":"论文整理-10","published":1,"updated":"2023-07-05T03:42:21.711Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdj0013gkvaexub5w6e","content":"","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":""},{"title":"论文整理","date":"2023-04-06T11:06:12.000Z","_content":"\n# 综述\n\n## NatureDeepReview                                  doi:10.1038/nature14539\n\n深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应该如何改变其内部参数。深度卷积网络在图像、视频、语音和音频处理方面取得了突破，而循环网络则在文本和语音等顺序数据方面大出风头。\n\n机器学习最常见的形式，无论深度与否，都是监督学习。\n\n我们计算一个目标函数来测量输出分数和期望分数模式之间的误差。然后，机器修改其内部可调参数，以减少这种误差。这些可调参数，通常称为权重。\n\n在实践中，大多数从业者使用一种称为随机梯度下降(SGD)的程序。这包括显示几个例子的输入向量，计算输出和误差，计算这些例子的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复这个过程，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本集对所有样本的平均梯度给出了一个有噪声的估计。与复杂得多的优化技术相比，这个简单的程序通常能惊人地快速找到一组良好的权重。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-16-39-5a49a75876192c9a1ea19b821a5521c.png\" alt=\"\" width=\"303\">     <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-17-24-3a42b78e655362bd8b6b86aeab1d0bb.png\" alt=\"\" width=\"314\">\n\nc，用于计算具有两个隐藏层和一个输出层的神经网络中的前向传递的方程，每个输出层构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入z，这是下一层单元输出的加权和。然后对z应用非线性函数f(.)来得到单位的输出。为简单起见，我们省略了偏差项。\n\n神经网络中使用的非线性函数包括近年来常用的整流线性单元(ReLU) f(z) = max(0,z)，以及更传统的sigmoids，如双曲正切，f(z) =(exp(z)−exp(−z))/(exp(z)+exp(−z))和logistic函数logistic, f(z) =1/(1 +exp(−z))。\n\nd，用于计算向后传递的方程。在每个隐藏层，我们计算关于每个单元输出的误差导数，这是关于上面一层中单元的总输入的误差导数的加权和。然后通过乘以f(z)的梯度，将输出的误差导数转换为输入的误差导数。\n\n在输出层，相对于单位输出的误差导数通过对成本函数求导来计算。如果单位l的成本函数为0.5(yl−tl) 2，则得到yl−tl，其中tl是目标值。一旦∂E/∂zk是已知的，从下面一层的单位j开始的连接的权值wjk的误差导数就是yj∂E/∂zk。\n\n使用通用的学习过程自动学习好的特性\n\n隐藏层可以被视为以非线性方式扭曲输入\n\nThere are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.\n\n深度神经网络利用了许多自然信号都是组合层次结构的特性，其中高级特征是通过组合低级特征来获得的。\n\n深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络具有两种不同的指数优势。这两个优点都来自组合的强大功能，并依赖于底层数据生成分布具有适当的组件结构。首先，学习分布式表示能够泛化到学习到的特征值的新组合，而不是在训练中看到的那些(例如，n个二进制特征可以有2^n个组合)。其次，在一个深度网络中组成表示层带来了另一个指数优势(深度指数)的潜力。\n\n# 历史\n\n## Reducing the Dimensionality of Data with Neural Networks\n\n## 28 JULY 2006 VOL 313 SCIENCE www.sciencemag.org\n\n降维有利于高维数据的分类、可视化、通信和存储。一种简单而广泛使用的方法是主成分分析。本文提出了。。。。。\n\n在具有多个隐藏层(2-4层)的非线性自编码器中，权值的优化非常困难。只有初始权重接近一个好的解，梯度下降才能很好地工作，但找到这样的初始权重需要一种非常不同的算法，需要每次学习一层特征。我们介绍了这个用于二进制数据的预训练过程，并将其推广到实值数据，并表明它适用于各种数据集。\n","source":"_posts/论文整理-4.md","raw":"---\ntitle: 论文整理\ndate: 2023-04-06 19:06:12\ntags: 深度学习基础\ncategories: 论文整理\n---\n\n# 综述\n\n## NatureDeepReview                                  doi:10.1038/nature14539\n\n深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应该如何改变其内部参数。深度卷积网络在图像、视频、语音和音频处理方面取得了突破，而循环网络则在文本和语音等顺序数据方面大出风头。\n\n机器学习最常见的形式，无论深度与否，都是监督学习。\n\n我们计算一个目标函数来测量输出分数和期望分数模式之间的误差。然后，机器修改其内部可调参数，以减少这种误差。这些可调参数，通常称为权重。\n\n在实践中，大多数从业者使用一种称为随机梯度下降(SGD)的程序。这包括显示几个例子的输入向量，计算输出和误差，计算这些例子的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复这个过程，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本集对所有样本的平均梯度给出了一个有噪声的估计。与复杂得多的优化技术相比，这个简单的程序通常能惊人地快速找到一组良好的权重。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-16-39-5a49a75876192c9a1ea19b821a5521c.png\" alt=\"\" width=\"303\">     <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-17-24-3a42b78e655362bd8b6b86aeab1d0bb.png\" alt=\"\" width=\"314\">\n\nc，用于计算具有两个隐藏层和一个输出层的神经网络中的前向传递的方程，每个输出层构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入z，这是下一层单元输出的加权和。然后对z应用非线性函数f(.)来得到单位的输出。为简单起见，我们省略了偏差项。\n\n神经网络中使用的非线性函数包括近年来常用的整流线性单元(ReLU) f(z) = max(0,z)，以及更传统的sigmoids，如双曲正切，f(z) =(exp(z)−exp(−z))/(exp(z)+exp(−z))和logistic函数logistic, f(z) =1/(1 +exp(−z))。\n\nd，用于计算向后传递的方程。在每个隐藏层，我们计算关于每个单元输出的误差导数，这是关于上面一层中单元的总输入的误差导数的加权和。然后通过乘以f(z)的梯度，将输出的误差导数转换为输入的误差导数。\n\n在输出层，相对于单位输出的误差导数通过对成本函数求导来计算。如果单位l的成本函数为0.5(yl−tl) 2，则得到yl−tl，其中tl是目标值。一旦∂E/∂zk是已知的，从下面一层的单位j开始的连接的权值wjk的误差导数就是yj∂E/∂zk。\n\n使用通用的学习过程自动学习好的特性\n\n隐藏层可以被视为以非线性方式扭曲输入\n\nThere are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.\n\n深度神经网络利用了许多自然信号都是组合层次结构的特性，其中高级特征是通过组合低级特征来获得的。\n\n深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络具有两种不同的指数优势。这两个优点都来自组合的强大功能，并依赖于底层数据生成分布具有适当的组件结构。首先，学习分布式表示能够泛化到学习到的特征值的新组合，而不是在训练中看到的那些(例如，n个二进制特征可以有2^n个组合)。其次，在一个深度网络中组成表示层带来了另一个指数优势(深度指数)的潜力。\n\n# 历史\n\n## Reducing the Dimensionality of Data with Neural Networks\n\n## 28 JULY 2006 VOL 313 SCIENCE www.sciencemag.org\n\n降维有利于高维数据的分类、可视化、通信和存储。一种简单而广泛使用的方法是主成分分析。本文提出了。。。。。\n\n在具有多个隐藏层(2-4层)的非线性自编码器中，权值的优化非常困难。只有初始权重接近一个好的解，梯度下降才能很好地工作，但找到这样的初始权重需要一种非常不同的算法，需要每次学习一层特征。我们介绍了这个用于二进制数据的预训练过程，并将其推广到实值数据，并表明它适用于各种数据集。\n","slug":"论文整理-4","published":1,"updated":"2023-04-20T12:29:01.822Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdk0014gkvafdsn5uc9","content":"<h1 id=\"综述\"><a href=\"#综述\" class=\"headerlink\" title=\"综述\"></a>综述</h1><h2 id=\"NatureDeepReview-doi-10-1038-x2F-nature14539\"><a href=\"#NatureDeepReview-doi-10-1038-x2F-nature14539\" class=\"headerlink\" title=\"NatureDeepReview                                  doi:10.1038&#x2F;nature14539\"></a>NatureDeepReview                                  doi:10.1038&#x2F;nature14539</h2><p>深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应该如何改变其内部参数。深度卷积网络在图像、视频、语音和音频处理方面取得了突破，而循环网络则在文本和语音等顺序数据方面大出风头。</p>\n<p>机器学习最常见的形式，无论深度与否，都是监督学习。</p>\n<p>我们计算一个目标函数来测量输出分数和期望分数模式之间的误差。然后，机器修改其内部可调参数，以减少这种误差。这些可调参数，通常称为权重。</p>\n<p>在实践中，大多数从业者使用一种称为随机梯度下降(SGD)的程序。这包括显示几个例子的输入向量，计算输出和误差，计算这些例子的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复这个过程，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本集对所有样本的平均梯度给出了一个有噪声的估计。与复杂得多的优化技术相比，这个简单的程序通常能惊人地快速找到一组良好的权重。</p>\n<p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-16-39-5a49a75876192c9a1ea19b821a5521c.png\" alt=\"\" width=\"303\">     <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-17-24-3a42b78e655362bd8b6b86aeab1d0bb.png\" alt=\"\" width=\"314\"></p>\n<p>c，用于计算具有两个隐藏层和一个输出层的神经网络中的前向传递的方程，每个输出层构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入z，这是下一层单元输出的加权和。然后对z应用非线性函数f(.)来得到单位的输出。为简单起见，我们省略了偏差项。</p>\n<p>神经网络中使用的非线性函数包括近年来常用的整流线性单元(ReLU) f(z) &#x3D; max(0,z)，以及更传统的sigmoids，如双曲正切，f(z) &#x3D;(exp(z)−exp(−z))&#x2F;(exp(z)+exp(−z))和logistic函数logistic, f(z) &#x3D;1&#x2F;(1 +exp(−z))。</p>\n<p>d，用于计算向后传递的方程。在每个隐藏层，我们计算关于每个单元输出的误差导数，这是关于上面一层中单元的总输入的误差导数的加权和。然后通过乘以f(z)的梯度，将输出的误差导数转换为输入的误差导数。</p>\n<p>在输出层，相对于单位输出的误差导数通过对成本函数求导来计算。如果单位l的成本函数为0.5(yl−tl) 2，则得到yl−tl，其中tl是目标值。一旦∂E&#x2F;∂zk是已知的，从下面一层的单位j开始的连接的权值wjk的误差导数就是yj∂E&#x2F;∂zk。</p>\n<p>使用通用的学习过程自动学习好的特性</p>\n<p>隐藏层可以被视为以非线性方式扭曲输入</p>\n<p>There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.</p>\n<p>深度神经网络利用了许多自然信号都是组合层次结构的特性，其中高级特征是通过组合低级特征来获得的。</p>\n<p>深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络具有两种不同的指数优势。这两个优点都来自组合的强大功能，并依赖于底层数据生成分布具有适当的组件结构。首先，学习分布式表示能够泛化到学习到的特征值的新组合，而不是在训练中看到的那些(例如，n个二进制特征可以有2^n个组合)。其次，在一个深度网络中组成表示层带来了另一个指数优势(深度指数)的潜力。</p>\n<h1 id=\"历史\"><a href=\"#历史\" class=\"headerlink\" title=\"历史\"></a>历史</h1><h2 id=\"Reducing-the-Dimensionality-of-Data-with-Neural-Networks\"><a href=\"#Reducing-the-Dimensionality-of-Data-with-Neural-Networks\" class=\"headerlink\" title=\"Reducing the Dimensionality of Data with Neural Networks\"></a>Reducing the Dimensionality of Data with Neural Networks</h2><h2 id=\"28-JULY-2006-VOL-313-SCIENCE-www-sciencemag-org\"><a href=\"#28-JULY-2006-VOL-313-SCIENCE-www-sciencemag-org\" class=\"headerlink\" title=\"28 JULY 2006 VOL 313 SCIENCE www.sciencemag.org\"></a>28 JULY 2006 VOL 313 SCIENCE <a href=\"http://www.sciencemag.org/\">www.sciencemag.org</a></h2><p>降维有利于高维数据的分类、可视化、通信和存储。一种简单而广泛使用的方法是主成分分析。本文提出了。。。。。</p>\n<p>在具有多个隐藏层(2-4层)的非线性自编码器中，权值的优化非常困难。只有初始权重接近一个好的解，梯度下降才能很好地工作，但找到这样的初始权重需要一种非常不同的算法，需要每次学习一层特征。我们介绍了这个用于二进制数据的预训练过程，并将其推广到实值数据，并表明它适用于各种数据集。</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"综述\"><a href=\"#综述\" class=\"headerlink\" title=\"综述\"></a>综述</h1><h2 id=\"NatureDeepReview-doi-10-1038-x2F-nature14539\"><a href=\"#NatureDeepReview-doi-10-1038-x2F-nature14539\" class=\"headerlink\" title=\"NatureDeepReview                                  doi:10.1038&#x2F;nature14539\"></a>NatureDeepReview                                  doi:10.1038&#x2F;nature14539</h2><p>深度学习通过使用反向传播算法来发现大型数据集中的复杂结构，以指示机器应该如何改变其内部参数。深度卷积网络在图像、视频、语音和音频处理方面取得了突破，而循环网络则在文本和语音等顺序数据方面大出风头。</p>\n<p>机器学习最常见的形式，无论深度与否，都是监督学习。</p>\n<p>我们计算一个目标函数来测量输出分数和期望分数模式之间的误差。然后，机器修改其内部可调参数，以减少这种误差。这些可调参数，通常称为权重。</p>\n<p>在实践中，大多数从业者使用一种称为随机梯度下降(SGD)的程序。这包括显示几个例子的输入向量，计算输出和误差，计算这些例子的平均梯度，并相应地调整权重。对训练集中的许多小样本集重复这个过程，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本集对所有样本的平均梯度给出了一个有噪声的估计。与复杂得多的优化技术相比，这个简单的程序通常能惊人地快速找到一组良好的权重。</p>\n<p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-16-39-5a49a75876192c9a1ea19b821a5521c.png\" alt=\"\" width=\"303\">     <img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-06-20-17-24-3a42b78e655362bd8b6b86aeab1d0bb.png\" alt=\"\" width=\"314\"></p>\n<p>c，用于计算具有两个隐藏层和一个输出层的神经网络中的前向传递的方程，每个输出层构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入z，这是下一层单元输出的加权和。然后对z应用非线性函数f(.)来得到单位的输出。为简单起见，我们省略了偏差项。</p>\n<p>神经网络中使用的非线性函数包括近年来常用的整流线性单元(ReLU) f(z) &#x3D; max(0,z)，以及更传统的sigmoids，如双曲正切，f(z) &#x3D;(exp(z)−exp(−z))&#x2F;(exp(z)+exp(−z))和logistic函数logistic, f(z) &#x3D;1&#x2F;(1 +exp(−z))。</p>\n<p>d，用于计算向后传递的方程。在每个隐藏层，我们计算关于每个单元输出的误差导数，这是关于上面一层中单元的总输入的误差导数的加权和。然后通过乘以f(z)的梯度，将输出的误差导数转换为输入的误差导数。</p>\n<p>在输出层，相对于单位输出的误差导数通过对成本函数求导来计算。如果单位l的成本函数为0.5(yl−tl) 2，则得到yl−tl，其中tl是目标值。一旦∂E&#x2F;∂zk是已知的，从下面一层的单位j开始的连接的权值wjk的误差导数就是yj∂E&#x2F;∂zk。</p>\n<p>使用通用的学习过程自动学习好的特性</p>\n<p>隐藏层可以被视为以非线性方式扭曲输入</p>\n<p>There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.</p>\n<p>深度神经网络利用了许多自然信号都是组合层次结构的特性，其中高级特征是通过组合低级特征来获得的。</p>\n<p>深度学习理论表明，与不使用分布式表示的经典学习算法相比，深度网络具有两种不同的指数优势。这两个优点都来自组合的强大功能，并依赖于底层数据生成分布具有适当的组件结构。首先，学习分布式表示能够泛化到学习到的特征值的新组合，而不是在训练中看到的那些(例如，n个二进制特征可以有2^n个组合)。其次，在一个深度网络中组成表示层带来了另一个指数优势(深度指数)的潜力。</p>\n<h1 id=\"历史\"><a href=\"#历史\" class=\"headerlink\" title=\"历史\"></a>历史</h1><h2 id=\"Reducing-the-Dimensionality-of-Data-with-Neural-Networks\"><a href=\"#Reducing-the-Dimensionality-of-Data-with-Neural-Networks\" class=\"headerlink\" title=\"Reducing the Dimensionality of Data with Neural Networks\"></a>Reducing the Dimensionality of Data with Neural Networks</h2><h2 id=\"28-JULY-2006-VOL-313-SCIENCE-www-sciencemag-org\"><a href=\"#28-JULY-2006-VOL-313-SCIENCE-www-sciencemag-org\" class=\"headerlink\" title=\"28 JULY 2006 VOL 313 SCIENCE www.sciencemag.org\"></a>28 JULY 2006 VOL 313 SCIENCE <a href=\"http://www.sciencemag.org/\">www.sciencemag.org</a></h2><p>降维有利于高维数据的分类、可视化、通信和存储。一种简单而广泛使用的方法是主成分分析。本文提出了。。。。。</p>\n<p>在具有多个隐藏层(2-4层)的非线性自编码器中，权值的优化非常困难。只有初始权重接近一个好的解，梯度下降才能很好地工作，但找到这样的初始权重需要一种非常不同的算法，需要每次学习一层特征。我们介绍了这个用于二进制数据的预训练过程，并将其推广到实值数据，并表明它适用于各种数据集。</p>\n"},{"title":"基于深度学习的图像多标签分类算法研究_高士慧","date":"2023-04-23T07:19:12.000Z","_content":"\n# 目的：\n\n 对这些类别、内容都不相同的图片进行合理有效的管理\n\n# 研究背景和意义：\n\n对于普通用户来说，平均每个用户的智能手机中大概会存储一千多张照片，并且随着手机与相机拍照功能的增强和各种图片后期处理软件的流行，智能设备中的图片数量也在逐渐增加。如何高效管理这些海量的图片，并且便于用户的查询使用成为一个急需解决的课题。\n\n图像多标签分类，即为每张图像的不同目标分别赋予类别标签，然后以这些标签为依据对图像进行分类，将图片归属到不同的类别中。这样通过类别标签也可以从大量的图像中检索出需要查询的图像。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-15-55-20-image.png\" alt=\"\" data-align=\"center\">\n\n上图可以为图像赋予人、狗和长椅的标签，即可以通过这三个标签将图像分到三个不同的类别中。并且分别以这三个标签为依据也可以检索出这张图像。\n\n# 国内外研究现状\n\n 卷积神经网络由卷积层、激励层、池化层和全连接层构成。卷积层的作用主要是通过卷积操作来提取特征，进而获取特征图。池化层的主要作用是降采样。\n\n基于各种高效的网络模型，学者提出了很多适用于多标签分类的算法。将其分为两类，第一类是基于内容的多标签分类方法，第二类是基于检测的图像多标签分类方法。\n\n第一类：🙏。。。。。。。\n\n第二类：该方法首先训练一个目标检测模型，然后通过知识蒸馏模块将该模型参数移植到\n多标签分类的网络中来完成分类任务。\n\n借鉴目标检测成熟的方案去设计算法和网络结构，并通过语义关系的对应对检测结果进行处理完成图像多标签分类的任务。\n\n图像多标签分类任务存在着较大的挑战\n\n1. 当下此类算法网络结构复杂而庞大，分类准确率低。\n\n2. 在对多个目标进行识别时常常忽略了各个目标之间的关联性，只是对每个目标进行单个识别。不同目标的类别之间具有一定的关联性，基于这种关联性可以提高对目标识别的准确性。\n\n# 本文主要研究内容\n\n- 基于高效Refinedet算法的网络结构，提出了一个用于图像多标签分类的密集连接的细化网络（ＤｅｎｓｅｌｙＣｏｎｎｅｃｔｅｄＲｅｆｉｎｅｍｅｎｔＮｅｔｗｏｒｋ），简称为ＤＣＲＮ。\n\n- 在ＤＣＲＮ的基础上引入了注意力机制，提出了用于多标签分类的ａｔｔｅｎｔｉｏn机制下的ＤＣＲＮ算法。\n\n- 在对算法进行上述优化后，将检测结果进行语义关系的对应，将目标检测的结果转化为图像多标签分类所需要的类别结果。为图像赋予了便于分类和检索的多个标签。\n\n- 在对算法进行研究和训练并得到了更高的准确率后，本文设计并实现了一个基于Ｃ／Ｓ架构的图像多标签分类系统平台，以作者拍摄所得的生活照作为系统的输入，经过上述算法对图像进行测试。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-19-25-34-image.png\" alt=\"\" data-align=\"center\">\n\n# 基于检测&深度学习的图像多标签分类算法\n\n## 深度学习的基础\n\n深度学习的核心思想是<u>神经网络</u>，神经网络的运算主要分为三步：正向传播、反向传播、梯度下降。正向传播的主要目的是计算预测值，主要过程是从输入层输入数据之后，经过一层一层的计算到输出层，得到预测值。反向传播主要是来度量预测值的准确性，用正向传播得到的预测值与采集回来的标签之做比较，两者之间的差距称为损失函数。梯度下降即损失函数最小化的过程，主要目的是对网络结构进行优化。神经网络包含很多种形式，其中深度学习中应用最广泛的是卷积神经网络CNN\n\n<u>卷积神经网络</u>是含有卷积层的神经网络，它由卷积层、激励层、池化层和全连接层构成。\n\nＣＮＮ—般使用的是ReLU激活函数，它的特点是收敛快，求梯度简单。\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-26-10-28-51-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n由于卷积层是线性运算，而线性模型的表达能力不够，所以引入激活函数，即非线性运算，这样可以大大增强模型的表达能力和泛化能力\n\n## 目标检测算法\n\n## 目标检测发展历程\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-26-11-13-44-image.png)\n\n本文重点关注Refinedet和DSOD，在此基础上进行改进\n\n## 图像分类算法特征网络模型\n\n本文将对工作所用到的ＶＧＧＮｅｔ、ＤｅｎｓｅｔＮｅｔ和ＲｅｓＮｅｔ的网络结构进行详细介绍。\n\n🙏。。。。。。。。。。。。。。\n\n# 网络模型的搭建和训练\n\n卷积神经网络一般包括训练和测试两大阶段。训练是把训练数据和神经网络模型用ＧＰＵ提炼出模型参数。测试是把测试数据用训练好的模型运行后查看结果。\n\n本节将对深度学习框架的选择和网络模型的训练优化两个方面进行详细论述。\n\n为了更加方便的对卷积神经网络进行训练，各种开源深度学习框架也层出不穷，\n常见的有Ｃａｆｆｅ，ＣＮＴＫ，ＤｅｅｐＬｅａｍｉｎｇ４ｊ，ＰｙＴｏｒｃｈＫｅｒａｓ，Ｔｈｅａｎｏ、Ｍｘｎｅｔ和ＴｅｎｓｏｒＦｌｏｗ网等。\n本文作者选择Ｍｘｎｅｔ作为训练神经网络的深度学习框架。\n\n本算法使用的主千网络是ＲｅｓＮｅｔ１０１，首先将数据集的相关文件准备好并放到指定的位置，然后按照要求对程序中的参数如ｅｐｏｃｈ、ｂａｔｃｈｓｉｚｅ、ｌｅａｒｎｉｎｇｒａｔｅ等进行调整。将图像输入后，网络采用梯度下降的方法不断更新参数，直到损失函数收敛为止\n","source":"_posts/论文整理-5.md","raw":"---\ntitle: 基于深度学习的图像多标签分类算法研究_高士慧\ndate: 2023-04-23 15:19:12\ntags: 多标签算法\ncategories: 论文整理\n---\n\n# 目的：\n\n 对这些类别、内容都不相同的图片进行合理有效的管理\n\n# 研究背景和意义：\n\n对于普通用户来说，平均每个用户的智能手机中大概会存储一千多张照片，并且随着手机与相机拍照功能的增强和各种图片后期处理软件的流行，智能设备中的图片数量也在逐渐增加。如何高效管理这些海量的图片，并且便于用户的查询使用成为一个急需解决的课题。\n\n图像多标签分类，即为每张图像的不同目标分别赋予类别标签，然后以这些标签为依据对图像进行分类，将图片归属到不同的类别中。这样通过类别标签也可以从大量的图像中检索出需要查询的图像。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-15-55-20-image.png\" alt=\"\" data-align=\"center\">\n\n上图可以为图像赋予人、狗和长椅的标签，即可以通过这三个标签将图像分到三个不同的类别中。并且分别以这三个标签为依据也可以检索出这张图像。\n\n# 国内外研究现状\n\n 卷积神经网络由卷积层、激励层、池化层和全连接层构成。卷积层的作用主要是通过卷积操作来提取特征，进而获取特征图。池化层的主要作用是降采样。\n\n基于各种高效的网络模型，学者提出了很多适用于多标签分类的算法。将其分为两类，第一类是基于内容的多标签分类方法，第二类是基于检测的图像多标签分类方法。\n\n第一类：🙏。。。。。。。\n\n第二类：该方法首先训练一个目标检测模型，然后通过知识蒸馏模块将该模型参数移植到\n多标签分类的网络中来完成分类任务。\n\n借鉴目标检测成熟的方案去设计算法和网络结构，并通过语义关系的对应对检测结果进行处理完成图像多标签分类的任务。\n\n图像多标签分类任务存在着较大的挑战\n\n1. 当下此类算法网络结构复杂而庞大，分类准确率低。\n\n2. 在对多个目标进行识别时常常忽略了各个目标之间的关联性，只是对每个目标进行单个识别。不同目标的类别之间具有一定的关联性，基于这种关联性可以提高对目标识别的准确性。\n\n# 本文主要研究内容\n\n- 基于高效Refinedet算法的网络结构，提出了一个用于图像多标签分类的密集连接的细化网络（ＤｅｎｓｅｌｙＣｏｎｎｅｃｔｅｄＲｅｆｉｎｅｍｅｎｔＮｅｔｗｏｒｋ），简称为ＤＣＲＮ。\n\n- 在ＤＣＲＮ的基础上引入了注意力机制，提出了用于多标签分类的ａｔｔｅｎｔｉｏn机制下的ＤＣＲＮ算法。\n\n- 在对算法进行上述优化后，将检测结果进行语义关系的对应，将目标检测的结果转化为图像多标签分类所需要的类别结果。为图像赋予了便于分类和检索的多个标签。\n\n- 在对算法进行研究和训练并得到了更高的准确率后，本文设计并实现了一个基于Ｃ／Ｓ架构的图像多标签分类系统平台，以作者拍摄所得的生活照作为系统的输入，经过上述算法对图像进行测试。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-19-25-34-image.png\" alt=\"\" data-align=\"center\">\n\n# 基于检测&深度学习的图像多标签分类算法\n\n## 深度学习的基础\n\n深度学习的核心思想是<u>神经网络</u>，神经网络的运算主要分为三步：正向传播、反向传播、梯度下降。正向传播的主要目的是计算预测值，主要过程是从输入层输入数据之后，经过一层一层的计算到输出层，得到预测值。反向传播主要是来度量预测值的准确性，用正向传播得到的预测值与采集回来的标签之做比较，两者之间的差距称为损失函数。梯度下降即损失函数最小化的过程，主要目的是对网络结构进行优化。神经网络包含很多种形式，其中深度学习中应用最广泛的是卷积神经网络CNN\n\n<u>卷积神经网络</u>是含有卷积层的神经网络，它由卷积层、激励层、池化层和全连接层构成。\n\nＣＮＮ—般使用的是ReLU激活函数，它的特点是收敛快，求梯度简单。\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-26-10-28-51-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n由于卷积层是线性运算，而线性模型的表达能力不够，所以引入激活函数，即非线性运算，这样可以大大增强模型的表达能力和泛化能力\n\n## 目标检测算法\n\n## 目标检测发展历程\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-26-11-13-44-image.png)\n\n本文重点关注Refinedet和DSOD，在此基础上进行改进\n\n## 图像分类算法特征网络模型\n\n本文将对工作所用到的ＶＧＧＮｅｔ、ＤｅｎｓｅｔＮｅｔ和ＲｅｓＮｅｔ的网络结构进行详细介绍。\n\n🙏。。。。。。。。。。。。。。\n\n# 网络模型的搭建和训练\n\n卷积神经网络一般包括训练和测试两大阶段。训练是把训练数据和神经网络模型用ＧＰＵ提炼出模型参数。测试是把测试数据用训练好的模型运行后查看结果。\n\n本节将对深度学习框架的选择和网络模型的训练优化两个方面进行详细论述。\n\n为了更加方便的对卷积神经网络进行训练，各种开源深度学习框架也层出不穷，\n常见的有Ｃａｆｆｅ，ＣＮＴＫ，ＤｅｅｐＬｅａｍｉｎｇ４ｊ，ＰｙＴｏｒｃｈＫｅｒａｓ，Ｔｈｅａｎｏ、Ｍｘｎｅｔ和ＴｅｎｓｏｒＦｌｏｗ网等。\n本文作者选择Ｍｘｎｅｔ作为训练神经网络的深度学习框架。\n\n本算法使用的主千网络是ＲｅｓＮｅｔ１０１，首先将数据集的相关文件准备好并放到指定的位置，然后按照要求对程序中的参数如ｅｐｏｃｈ、ｂａｔｃｈｓｉｚｅ、ｌｅａｒｎｉｎｇｒａｔｅ等进行调整。将图像输入后，网络采用梯度下降的方法不断更新参数，直到损失函数收敛为止\n","slug":"论文整理-5","published":1,"updated":"2023-06-05T12:07:49.073Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdm0019gkva6dembnw0","content":"<h1 id=\"目的：\"><a href=\"#目的：\" class=\"headerlink\" title=\"目的：\"></a>目的：</h1><p> 对这些类别、内容都不相同的图片进行合理有效的管理</p>\n<h1 id=\"研究背景和意义：\"><a href=\"#研究背景和意义：\" class=\"headerlink\" title=\"研究背景和意义：\"></a>研究背景和意义：</h1><p>对于普通用户来说，平均每个用户的智能手机中大概会存储一千多张照片，并且随着手机与相机拍照功能的增强和各种图片后期处理软件的流行，智能设备中的图片数量也在逐渐增加。如何高效管理这些海量的图片，并且便于用户的查询使用成为一个急需解决的课题。</p>\n<p>图像多标签分类，即为每张图像的不同目标分别赋予类别标签，然后以这些标签为依据对图像进行分类，将图片归属到不同的类别中。这样通过类别标签也可以从大量的图像中检索出需要查询的图像。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-15-55-20-image.png\" alt=\"\" data-align=\"center\">\n\n<p>上图可以为图像赋予人、狗和长椅的标签，即可以通过这三个标签将图像分到三个不同的类别中。并且分别以这三个标签为依据也可以检索出这张图像。</p>\n<h1 id=\"国内外研究现状\"><a href=\"#国内外研究现状\" class=\"headerlink\" title=\"国内外研究现状\"></a>国内外研究现状</h1><p> 卷积神经网络由卷积层、激励层、池化层和全连接层构成。卷积层的作用主要是通过卷积操作来提取特征，进而获取特征图。池化层的主要作用是降采样。</p>\n<p>基于各种高效的网络模型，学者提出了很多适用于多标签分类的算法。将其分为两类，第一类是基于内容的多标签分类方法，第二类是基于检测的图像多标签分类方法。</p>\n<p>第一类：🙏。。。。。。。</p>\n<p>第二类：该方法首先训练一个目标检测模型，然后通过知识蒸馏模块将该模型参数移植到<br>多标签分类的网络中来完成分类任务。</p>\n<p>借鉴目标检测成熟的方案去设计算法和网络结构，并通过语义关系的对应对检测结果进行处理完成图像多标签分类的任务。</p>\n<p>图像多标签分类任务存在着较大的挑战</p>\n<ol>\n<li><p>当下此类算法网络结构复杂而庞大，分类准确率低。</p>\n</li>\n<li><p>在对多个目标进行识别时常常忽略了各个目标之间的关联性，只是对每个目标进行单个识别。不同目标的类别之间具有一定的关联性，基于这种关联性可以提高对目标识别的准确性。</p>\n</li>\n</ol>\n<h1 id=\"本文主要研究内容\"><a href=\"#本文主要研究内容\" class=\"headerlink\" title=\"本文主要研究内容\"></a>本文主要研究内容</h1><ul>\n<li><p>基于高效Refinedet算法的网络结构，提出了一个用于图像多标签分类的密集连接的细化网络（ＤｅｎｓｅｌｙＣｏｎｎｅｃｔｅｄＲｅｆｉｎｅｍｅｎｔＮｅｔｗｏｒｋ），简称为ＤＣＲＮ。</p>\n</li>\n<li><p>在ＤＣＲＮ的基础上引入了注意力机制，提出了用于多标签分类的ａｔｔｅｎｔｉｏn机制下的ＤＣＲＮ算法。</p>\n</li>\n<li><p>在对算法进行上述优化后，将检测结果进行语义关系的对应，将目标检测的结果转化为图像多标签分类所需要的类别结果。为图像赋予了便于分类和检索的多个标签。</p>\n</li>\n<li><p>在对算法进行研究和训练并得到了更高的准确率后，本文设计并实现了一个基于Ｃ／Ｓ架构的图像多标签分类系统平台，以作者拍摄所得的生活照作为系统的输入，经过上述算法对图像进行测试。</p>\n</li>\n</ul>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-19-25-34-image.png\" alt=\"\" data-align=\"center\">\n\n<h1 id=\"基于检测-amp-深度学习的图像多标签分类算法\"><a href=\"#基于检测-amp-深度学习的图像多标签分类算法\" class=\"headerlink\" title=\"基于检测&amp;深度学习的图像多标签分类算法\"></a>基于检测&amp;深度学习的图像多标签分类算法</h1><h2 id=\"深度学习的基础\"><a href=\"#深度学习的基础\" class=\"headerlink\" title=\"深度学习的基础\"></a>深度学习的基础</h2><p>深度学习的核心思想是<u>神经网络</u>，神经网络的运算主要分为三步：正向传播、反向传播、梯度下降。正向传播的主要目的是计算预测值，主要过程是从输入层输入数据之后，经过一层一层的计算到输出层，得到预测值。反向传播主要是来度量预测值的准确性，用正向传播得到的预测值与采集回来的标签之做比较，两者之间的差距称为损失函数。梯度下降即损失函数最小化的过程，主要目的是对网络结构进行优化。神经网络包含很多种形式，其中深度学习中应用最广泛的是卷积神经网络CNN</p>\n<p><u>卷积神经网络</u>是含有卷积层的神经网络，它由卷积层、激励层、池化层和全连接层构成。</p>\n<p>ＣＮＮ—般使用的是ReLU激活函数，它的特点是收敛快，求梯度简单。</p>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-26-10-28-51-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<p>由于卷积层是线性运算，而线性模型的表达能力不够，所以引入激活函数，即非线性运算，这样可以大大增强模型的表达能力和泛化能力</p>\n<h2 id=\"目标检测算法\"><a href=\"#目标检测算法\" class=\"headerlink\" title=\"目标检测算法\"></a>目标检测算法</h2><h2 id=\"目标检测发展历程\"><a href=\"#目标检测发展历程\" class=\"headerlink\" title=\"目标检测发展历程\"></a>目标检测发展历程</h2><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-26-11-13-44-image.png\"></p>\n<p>本文重点关注Refinedet和DSOD，在此基础上进行改进</p>\n<h2 id=\"图像分类算法特征网络模型\"><a href=\"#图像分类算法特征网络模型\" class=\"headerlink\" title=\"图像分类算法特征网络模型\"></a>图像分类算法特征网络模型</h2><p>本文将对工作所用到的ＶＧＧＮｅｔ、ＤｅｎｓｅｔＮｅｔ和ＲｅｓＮｅｔ的网络结构进行详细介绍。</p>\n<p>🙏。。。。。。。。。。。。。。</p>\n<h1 id=\"网络模型的搭建和训练\"><a href=\"#网络模型的搭建和训练\" class=\"headerlink\" title=\"网络模型的搭建和训练\"></a>网络模型的搭建和训练</h1><p>卷积神经网络一般包括训练和测试两大阶段。训练是把训练数据和神经网络模型用ＧＰＵ提炼出模型参数。测试是把测试数据用训练好的模型运行后查看结果。</p>\n<p>本节将对深度学习框架的选择和网络模型的训练优化两个方面进行详细论述。</p>\n<p>为了更加方便的对卷积神经网络进行训练，各种开源深度学习框架也层出不穷，<br>常见的有Ｃａｆｆｅ，ＣＮＴＫ，ＤｅｅｐＬｅａｍｉｎｇ４ｊ，ＰｙＴｏｒｃｈＫｅｒａｓ，Ｔｈｅａｎｏ、Ｍｘｎｅｔ和ＴｅｎｓｏｒＦｌｏｗ网等。<br>本文作者选择Ｍｘｎｅｔ作为训练神经网络的深度学习框架。</p>\n<p>本算法使用的主千网络是ＲｅｓＮｅｔ１０１，首先将数据集的相关文件准备好并放到指定的位置，然后按照要求对程序中的参数如ｅｐｏｃｈ、ｂａｔｃｈｓｉｚｅ、ｌｅａｒｎｉｎｇｒａｔｅ等进行调整。将图像输入后，网络采用梯度下降的方法不断更新参数，直到损失函数收敛为止</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"目的：\"><a href=\"#目的：\" class=\"headerlink\" title=\"目的：\"></a>目的：</h1><p> 对这些类别、内容都不相同的图片进行合理有效的管理</p>\n<h1 id=\"研究背景和意义：\"><a href=\"#研究背景和意义：\" class=\"headerlink\" title=\"研究背景和意义：\"></a>研究背景和意义：</h1><p>对于普通用户来说，平均每个用户的智能手机中大概会存储一千多张照片，并且随着手机与相机拍照功能的增强和各种图片后期处理软件的流行，智能设备中的图片数量也在逐渐增加。如何高效管理这些海量的图片，并且便于用户的查询使用成为一个急需解决的课题。</p>\n<p>图像多标签分类，即为每张图像的不同目标分别赋予类别标签，然后以这些标签为依据对图像进行分类，将图片归属到不同的类别中。这样通过类别标签也可以从大量的图像中检索出需要查询的图像。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-15-55-20-image.png\" alt=\"\" data-align=\"center\">\n\n<p>上图可以为图像赋予人、狗和长椅的标签，即可以通过这三个标签将图像分到三个不同的类别中。并且分别以这三个标签为依据也可以检索出这张图像。</p>\n<h1 id=\"国内外研究现状\"><a href=\"#国内外研究现状\" class=\"headerlink\" title=\"国内外研究现状\"></a>国内外研究现状</h1><p> 卷积神经网络由卷积层、激励层、池化层和全连接层构成。卷积层的作用主要是通过卷积操作来提取特征，进而获取特征图。池化层的主要作用是降采样。</p>\n<p>基于各种高效的网络模型，学者提出了很多适用于多标签分类的算法。将其分为两类，第一类是基于内容的多标签分类方法，第二类是基于检测的图像多标签分类方法。</p>\n<p>第一类：🙏。。。。。。。</p>\n<p>第二类：该方法首先训练一个目标检测模型，然后通过知识蒸馏模块将该模型参数移植到<br>多标签分类的网络中来完成分类任务。</p>\n<p>借鉴目标检测成熟的方案去设计算法和网络结构，并通过语义关系的对应对检测结果进行处理完成图像多标签分类的任务。</p>\n<p>图像多标签分类任务存在着较大的挑战</p>\n<ol>\n<li><p>当下此类算法网络结构复杂而庞大，分类准确率低。</p>\n</li>\n<li><p>在对多个目标进行识别时常常忽略了各个目标之间的关联性，只是对每个目标进行单个识别。不同目标的类别之间具有一定的关联性，基于这种关联性可以提高对目标识别的准确性。</p>\n</li>\n</ol>\n<h1 id=\"本文主要研究内容\"><a href=\"#本文主要研究内容\" class=\"headerlink\" title=\"本文主要研究内容\"></a>本文主要研究内容</h1><ul>\n<li><p>基于高效Refinedet算法的网络结构，提出了一个用于图像多标签分类的密集连接的细化网络（ＤｅｎｓｅｌｙＣｏｎｎｅｃｔｅｄＲｅｆｉｎｅｍｅｎｔＮｅｔｗｏｒｋ），简称为ＤＣＲＮ。</p>\n</li>\n<li><p>在ＤＣＲＮ的基础上引入了注意力机制，提出了用于多标签分类的ａｔｔｅｎｔｉｏn机制下的ＤＣＲＮ算法。</p>\n</li>\n<li><p>在对算法进行上述优化后，将检测结果进行语义关系的对应，将目标检测的结果转化为图像多标签分类所需要的类别结果。为图像赋予了便于分类和检索的多个标签。</p>\n</li>\n<li><p>在对算法进行研究和训练并得到了更高的准确率后，本文设计并实现了一个基于Ｃ／Ｓ架构的图像多标签分类系统平台，以作者拍摄所得的生活照作为系统的输入，经过上述算法对图像进行测试。</p>\n</li>\n</ul>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-23-19-25-34-image.png\" alt=\"\" data-align=\"center\">\n\n<h1 id=\"基于检测-amp-深度学习的图像多标签分类算法\"><a href=\"#基于检测-amp-深度学习的图像多标签分类算法\" class=\"headerlink\" title=\"基于检测&amp;深度学习的图像多标签分类算法\"></a>基于检测&amp;深度学习的图像多标签分类算法</h1><h2 id=\"深度学习的基础\"><a href=\"#深度学习的基础\" class=\"headerlink\" title=\"深度学习的基础\"></a>深度学习的基础</h2><p>深度学习的核心思想是<u>神经网络</u>，神经网络的运算主要分为三步：正向传播、反向传播、梯度下降。正向传播的主要目的是计算预测值，主要过程是从输入层输入数据之后，经过一层一层的计算到输出层，得到预测值。反向传播主要是来度量预测值的准确性，用正向传播得到的预测值与采集回来的标签之做比较，两者之间的差距称为损失函数。梯度下降即损失函数最小化的过程，主要目的是对网络结构进行优化。神经网络包含很多种形式，其中深度学习中应用最广泛的是卷积神经网络CNN</p>\n<p><u>卷积神经网络</u>是含有卷积层的神经网络，它由卷积层、激励层、池化层和全连接层构成。</p>\n<p>ＣＮＮ—般使用的是ReLU激活函数，它的特点是收敛快，求梯度简单。</p>\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-04-26-10-28-51-image.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<p>由于卷积层是线性运算，而线性模型的表达能力不够，所以引入激活函数，即非线性运算，这样可以大大增强模型的表达能力和泛化能力</p>\n<h2 id=\"目标检测算法\"><a href=\"#目标检测算法\" class=\"headerlink\" title=\"目标检测算法\"></a>目标检测算法</h2><h2 id=\"目标检测发展历程\"><a href=\"#目标检测发展历程\" class=\"headerlink\" title=\"目标检测发展历程\"></a>目标检测发展历程</h2><p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-04-26-11-13-44-image.png\"></p>\n<p>本文重点关注Refinedet和DSOD，在此基础上进行改进</p>\n<h2 id=\"图像分类算法特征网络模型\"><a href=\"#图像分类算法特征网络模型\" class=\"headerlink\" title=\"图像分类算法特征网络模型\"></a>图像分类算法特征网络模型</h2><p>本文将对工作所用到的ＶＧＧＮｅｔ、ＤｅｎｓｅｔＮｅｔ和ＲｅｓＮｅｔ的网络结构进行详细介绍。</p>\n<p>🙏。。。。。。。。。。。。。。</p>\n<h1 id=\"网络模型的搭建和训练\"><a href=\"#网络模型的搭建和训练\" class=\"headerlink\" title=\"网络模型的搭建和训练\"></a>网络模型的搭建和训练</h1><p>卷积神经网络一般包括训练和测试两大阶段。训练是把训练数据和神经网络模型用ＧＰＵ提炼出模型参数。测试是把测试数据用训练好的模型运行后查看结果。</p>\n<p>本节将对深度学习框架的选择和网络模型的训练优化两个方面进行详细论述。</p>\n<p>为了更加方便的对卷积神经网络进行训练，各种开源深度学习框架也层出不穷，<br>常见的有Ｃａｆｆｅ，ＣＮＴＫ，ＤｅｅｐＬｅａｍｉｎｇ４ｊ，ＰｙＴｏｒｃｈＫｅｒａｓ，Ｔｈｅａｎｏ、Ｍｘｎｅｔ和ＴｅｎｓｏｒＦｌｏｗ网等。<br>本文作者选择Ｍｘｎｅｔ作为训练神经网络的深度学习框架。</p>\n<p>本算法使用的主千网络是ＲｅｓＮｅｔ１０１，首先将数据集的相关文件准备好并放到指定的位置，然后按照要求对程序中的参数如ｅｐｏｃｈ、ｂａｔｃｈｓｉｚｅ、ｌｅａｒｎｉｎｇｒａｔｅ等进行调整。将图像输入后，网络采用梯度下降的方法不断更新参数，直到损失函数收敛为止</p>\n"},{"title":"基于深度学习的图像多标签分类算法研究_张荣辉","date":"2023-05-01T10:48:49.000Z","_content":"\n本文以深度学习理论为基础，从图像深度特征提取和多标签分类器两个角度出发，研究深度学习在图像多标签分类方面的具体应用。\n\n1. 利用卷积神经网络模型这种特征学习方法提取深层图像特征\n\n2. 对比分析多种多标签分类算法，最终选择了RAKEL算法训练分类器。\n\n3. 研究CNN和RAKEL的结合原理\n   \n   \n","source":"_posts/论文整理-6.md","raw":"---\ntitle: 基于深度学习的图像多标签分类算法研究_张荣辉\ndate: 2023-05-01 18:48:49\ntags:\n---\n\n本文以深度学习理论为基础，从图像深度特征提取和多标签分类器两个角度出发，研究深度学习在图像多标签分类方面的具体应用。\n\n1. 利用卷积神经网络模型这种特征学习方法提取深层图像特征\n\n2. 对比分析多种多标签分类算法，最终选择了RAKEL算法训练分类器。\n\n3. 研究CNN和RAKEL的结合原理\n   \n   \n","slug":"论文整理-6","published":1,"updated":"2023-06-05T12:07:46.655Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdn001agkva07xncdok","content":"<p>本文以深度学习理论为基础，从图像深度特征提取和多标签分类器两个角度出发，研究深度学习在图像多标签分类方面的具体应用。</p>\n<ol>\n<li><p>利用卷积神经网络模型这种特征学习方法提取深层图像特征</p>\n</li>\n<li><p>对比分析多种多标签分类算法，最终选择了RAKEL算法训练分类器。</p>\n</li>\n<li><p>研究CNN和RAKEL的结合原理</p>\n</li>\n</ol>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>本文以深度学习理论为基础，从图像深度特征提取和多标签分类器两个角度出发，研究深度学习在图像多标签分类方面的具体应用。</p>\n<ol>\n<li><p>利用卷积神经网络模型这种特征学习方法提取深层图像特征</p>\n</li>\n<li><p>对比分析多种多标签分类算法，最终选择了RAKEL算法训练分类器。</p>\n</li>\n<li><p>研究CNN和RAKEL的结合原理</p>\n</li>\n</ol>\n"},{"title":"Periodic Surface Defect Detection in Steel Plates Based on Deep Learning","date":"2023-05-14T11:12:29.000Z","_content":"\n辊印的缺陷对比度较低，很难检测。针对滚动痕迹等周期性缺陷具有较强的时间序列特征，提出了一种基于卷积神经网络(CNN)和长短期记忆(LSTM)的周期性缺陷检测方法。首先通过CNN网络提取缺陷图像的特征，然后将提取的特征向量输入到LSTM网络中进行缺陷识别。\n\n周期性缺陷的形态特征并不是固定的，因此传统的CNN检测率不高。\n\n滚印是一组具有周期性的不均匀缺陷。它们通常是由于轧辊疲劳、硬度不足或轧辊在轧制过程中表面有异物引起的。\n\n同一批次滚印的形态特征稳定且相似;由于钢板的反复轧制，不同批次轧辊痕迹的形态特征会有所不同。滚印存在于印版的上下表面，主要在印版的操作侧和中间位置。它们是肉眼可以观察到的亮点，以及相机捕捉到的图像中的黑点\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-19-41-33-1684064486804.png)\n\n由于不同批次的辊印的形态特征有很大差异，所以传统的cnn通过提取形态学特征对缺陷进行分类的方法就难奏效，但由于滚痕缺陷具有较强的周期性，其时间序列特性适合LSTM处理\n\n通过某些方法生成足够多的样本\n\nCNN主要由三个部分组成:数据层、特征学习网络和分类网络。\n\n基于LSTM的周期性缺陷识别原理图7a显示了CNN + LSTM的总体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将<u>特征向量按时间序列</u>输入到LSTM中，输出O为识别结果。\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-11-57-1684066310922.png)\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-20-32-1684066825319.png)\n\n图7a显示了CNN + LSTM的整体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将特征向量按时间序列输入到LSTM中，输出O为识别结果。\n\n图7b显示了图7a中时刻t的LSTM[16]算法的具体流程。图7b中，Ct−1为前一时刻的信息，包括前一时刻输入网络的所有信息。结合当前时刻输入的信息Xt，先过滤前一时刻的信息，再过滤当前时刻输入的信息。通过过滤，没有缺陷时的信息，如图7a中的X2、Xt，被遗忘，而有缺陷时的信息，如图7a中的X0、Xt−1，被记住。因此，LSTM可以更好地记住缺陷特征，并防止长时间忘记缺陷特征。\n\n将缺陷图像与背景图像周期性排列拼接成一幅长矩形图像，得到周期性缺陷样本。将该样本输入到设计好的CNN中，提取卷积层的输出作为样本的特征向量。然后，将特征向量切成LSTM的输入向量。\n\n本研究使用Tensorflow框架实现LSTM。\n","source":"_posts/论文整理-8.md","raw":"---\ntitle: Periodic Surface Defect Detection in Steel Plates Based on Deep Learning\ndate: 2023-05-14 19:12:29\ntags:\ncategories: 论文整理\n---\n\n辊印的缺陷对比度较低，很难检测。针对滚动痕迹等周期性缺陷具有较强的时间序列特征，提出了一种基于卷积神经网络(CNN)和长短期记忆(LSTM)的周期性缺陷检测方法。首先通过CNN网络提取缺陷图像的特征，然后将提取的特征向量输入到LSTM网络中进行缺陷识别。\n\n周期性缺陷的形态特征并不是固定的，因此传统的CNN检测率不高。\n\n滚印是一组具有周期性的不均匀缺陷。它们通常是由于轧辊疲劳、硬度不足或轧辊在轧制过程中表面有异物引起的。\n\n同一批次滚印的形态特征稳定且相似;由于钢板的反复轧制，不同批次轧辊痕迹的形态特征会有所不同。滚印存在于印版的上下表面，主要在印版的操作侧和中间位置。它们是肉眼可以观察到的亮点，以及相机捕捉到的图像中的黑点\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-19-41-33-1684064486804.png)\n\n由于不同批次的辊印的形态特征有很大差异，所以传统的cnn通过提取形态学特征对缺陷进行分类的方法就难奏效，但由于滚痕缺陷具有较强的周期性，其时间序列特性适合LSTM处理\n\n通过某些方法生成足够多的样本\n\nCNN主要由三个部分组成:数据层、特征学习网络和分类网络。\n\n基于LSTM的周期性缺陷识别原理图7a显示了CNN + LSTM的总体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将<u>特征向量按时间序列</u>输入到LSTM中，输出O为识别结果。\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-11-57-1684066310922.png)\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-20-32-1684066825319.png)\n\n图7a显示了CNN + LSTM的整体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将特征向量按时间序列输入到LSTM中，输出O为识别结果。\n\n图7b显示了图7a中时刻t的LSTM[16]算法的具体流程。图7b中，Ct−1为前一时刻的信息，包括前一时刻输入网络的所有信息。结合当前时刻输入的信息Xt，先过滤前一时刻的信息，再过滤当前时刻输入的信息。通过过滤，没有缺陷时的信息，如图7a中的X2、Xt，被遗忘，而有缺陷时的信息，如图7a中的X0、Xt−1，被记住。因此，LSTM可以更好地记住缺陷特征，并防止长时间忘记缺陷特征。\n\n将缺陷图像与背景图像周期性排列拼接成一幅长矩形图像，得到周期性缺陷样本。将该样本输入到设计好的CNN中，提取卷积层的输出作为样本的特征向量。然后，将特征向量切成LSTM的输入向量。\n\n本研究使用Tensorflow框架实现LSTM。\n","slug":"论文整理-8","published":1,"updated":"2023-05-14T13:08:33.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdp001dgkva3cnf41j9","content":"<p>辊印的缺陷对比度较低，很难检测。针对滚动痕迹等周期性缺陷具有较强的时间序列特征，提出了一种基于卷积神经网络(CNN)和长短期记忆(LSTM)的周期性缺陷检测方法。首先通过CNN网络提取缺陷图像的特征，然后将提取的特征向量输入到LSTM网络中进行缺陷识别。</p>\n<p>周期性缺陷的形态特征并不是固定的，因此传统的CNN检测率不高。</p>\n<p>滚印是一组具有周期性的不均匀缺陷。它们通常是由于轧辊疲劳、硬度不足或轧辊在轧制过程中表面有异物引起的。</p>\n<p>同一批次滚印的形态特征稳定且相似;由于钢板的反复轧制，不同批次轧辊痕迹的形态特征会有所不同。滚印存在于印版的上下表面，主要在印版的操作侧和中间位置。它们是肉眼可以观察到的亮点，以及相机捕捉到的图像中的黑点</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-19-41-33-1684064486804.png\"></p>\n<p>由于不同批次的辊印的形态特征有很大差异，所以传统的cnn通过提取形态学特征对缺陷进行分类的方法就难奏效，但由于滚痕缺陷具有较强的周期性，其时间序列特性适合LSTM处理</p>\n<p>通过某些方法生成足够多的样本</p>\n<p>CNN主要由三个部分组成:数据层、特征学习网络和分类网络。</p>\n<p>基于LSTM的周期性缺陷识别原理图7a显示了CNN + LSTM的总体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将<u>特征向量按时间序列</u>输入到LSTM中，输出O为识别结果。</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-11-57-1684066310922.png\"></p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-20-32-1684066825319.png\"></p>\n<p>图7a显示了CNN + LSTM的整体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将特征向量按时间序列输入到LSTM中，输出O为识别结果。</p>\n<p>图7b显示了图7a中时刻t的LSTM[16]算法的具体流程。图7b中，Ct−1为前一时刻的信息，包括前一时刻输入网络的所有信息。结合当前时刻输入的信息Xt，先过滤前一时刻的信息，再过滤当前时刻输入的信息。通过过滤，没有缺陷时的信息，如图7a中的X2、Xt，被遗忘，而有缺陷时的信息，如图7a中的X0、Xt−1，被记住。因此，LSTM可以更好地记住缺陷特征，并防止长时间忘记缺陷特征。</p>\n<p>将缺陷图像与背景图像周期性排列拼接成一幅长矩形图像，得到周期性缺陷样本。将该样本输入到设计好的CNN中，提取卷积层的输出作为样本的特征向量。然后，将特征向量切成LSTM的输入向量。</p>\n<p>本研究使用Tensorflow框架实现LSTM。</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>辊印的缺陷对比度较低，很难检测。针对滚动痕迹等周期性缺陷具有较强的时间序列特征，提出了一种基于卷积神经网络(CNN)和长短期记忆(LSTM)的周期性缺陷检测方法。首先通过CNN网络提取缺陷图像的特征，然后将提取的特征向量输入到LSTM网络中进行缺陷识别。</p>\n<p>周期性缺陷的形态特征并不是固定的，因此传统的CNN检测率不高。</p>\n<p>滚印是一组具有周期性的不均匀缺陷。它们通常是由于轧辊疲劳、硬度不足或轧辊在轧制过程中表面有异物引起的。</p>\n<p>同一批次滚印的形态特征稳定且相似;由于钢板的反复轧制，不同批次轧辊痕迹的形态特征会有所不同。滚印存在于印版的上下表面，主要在印版的操作侧和中间位置。它们是肉眼可以观察到的亮点，以及相机捕捉到的图像中的黑点</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-19-41-33-1684064486804.png\"></p>\n<p>由于不同批次的辊印的形态特征有很大差异，所以传统的cnn通过提取形态学特征对缺陷进行分类的方法就难奏效，但由于滚痕缺陷具有较强的周期性，其时间序列特性适合LSTM处理</p>\n<p>通过某些方法生成足够多的样本</p>\n<p>CNN主要由三个部分组成:数据层、特征学习网络和分类网络。</p>\n<p>基于LSTM的周期性缺陷识别原理图7a显示了CNN + LSTM的总体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将<u>特征向量按时间序列</u>输入到LSTM中，输出O为识别结果。</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-11-57-1684066310922.png\"></p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-05-14-20-20-32-1684066825319.png\"></p>\n<p>图7a显示了CNN + LSTM的整体流程。通过CNN从样本中提取特征，得到其对应的特征向量x，然后将特征向量按时间序列输入到LSTM中，输出O为识别结果。</p>\n<p>图7b显示了图7a中时刻t的LSTM[16]算法的具体流程。图7b中，Ct−1为前一时刻的信息，包括前一时刻输入网络的所有信息。结合当前时刻输入的信息Xt，先过滤前一时刻的信息，再过滤当前时刻输入的信息。通过过滤，没有缺陷时的信息，如图7a中的X2、Xt，被遗忘，而有缺陷时的信息，如图7a中的X0、Xt−1，被记住。因此，LSTM可以更好地记住缺陷特征，并防止长时间忘记缺陷特征。</p>\n<p>将缺陷图像与背景图像周期性排列拼接成一幅长矩形图像，得到周期性缺陷样本。将该样本输入到设计好的CNN中，提取卷积层的输出作为样本的特征向量。然后，将特征向量切成LSTM的输入向量。</p>\n<p>本研究使用Tensorflow框架实现LSTM。</p>\n"},{"title":"论文整理","date":"2023-05-05T02:39:14.000Z","_content":"\nyolov4\n\n现代检测器通常由两部分组成，在ImageNet上进行预训练的主干和用于预测对象类别和边界框的头部。\n\n近年来发展起来的目标检测器通常在主干和头之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-05-10-45-44-image.png\" alt=\"\" data-align=\"center\">\n\n- Backbone：CSPDarknet53\n- Neck：SPP,PAN\n- Head：YOLOv3\n\n![](https://img-blog.csdnimg.cn/20200512144007178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)\n","source":"_posts/论文整理-7.md","raw":"---\ntitle: 论文整理\ndate: 2023-05-05 10:39:14\ntags:\ncategories: 论文整理\n---\n\nyolov4\n\n现代检测器通常由两部分组成，在ImageNet上进行预训练的主干和用于预测对象类别和边界框的头部。\n\n近年来发展起来的目标检测器通常在主干和头之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-05-10-45-44-image.png\" alt=\"\" data-align=\"center\">\n\n- Backbone：CSPDarknet53\n- Neck：SPP,PAN\n- Head：YOLOv3\n\n![](https://img-blog.csdnimg.cn/20200512144007178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)\n","slug":"论文整理-7","published":1,"updated":"2023-05-05T03:40:51.124Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdq001ggkva91tw54p3","content":"<p>yolov4</p>\n<p>现代检测器通常由两部分组成，在ImageNet上进行预训练的主干和用于预测对象类别和边界框的头部。</p>\n<p>近年来发展起来的目标检测器通常在主干和头之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-05-10-45-44-image.png\" alt=\"\" data-align=\"center\">\n\n<ul>\n<li>Backbone：CSPDarknet53</li>\n<li>Neck：SPP,PAN</li>\n<li>Head：YOLOv3</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200512144007178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center\"></p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<p>yolov4</p>\n<p>现代检测器通常由两部分组成，在ImageNet上进行预训练的主干和用于预测对象类别和边界框的头部。</p>\n<p>近年来发展起来的目标检测器通常在主干和头之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。</p>\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-05-05-10-45-44-image.png\" alt=\"\" data-align=\"center\">\n\n<ul>\n<li>Backbone：CSPDarknet53</li>\n<li>Neck：SPP,PAN</li>\n<li>Head：YOLOv3</li>\n</ul>\n<p><img src=\"https://img-blog.csdnimg.cn/20200512144007178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center\"></p>\n"},{"title":"基于线结构光的物体表面缺陷检测方法与实验研究_杨续昌","date":"2023-06-17T03:13:41.000Z","_content":"\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-17-11-20-56-1686972050686.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n提取光条纹中心线的方法：光强分布拟合法、灰度重心法、方向模板法、Ｈｅｓｓｉａｎ矩阵法。\n","source":"_posts/论文整理-9.md","raw":"---\ntitle: 基于线结构光的物体表面缺陷检测方法与实验研究_杨续昌\ndate: 2023-06-17 11:13:41\ntags:\n---\n\n<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-17-11-20-56-1686972050686.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n提取光条纹中心线的方法：光强分布拟合法、灰度重心法、方向模板法、Ｈｅｓｓｉａｎ矩阵法。\n","slug":"论文整理-9","published":1,"updated":"2023-06-17T07:50:16.492Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqds001igkvabqy2azxm","content":"<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-17-11-20-56-1686972050686.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<p>提取光条纹中心线的方法：光强分布拟合法、灰度重心法、方向模板法、Ｈｅｓｓｉａｎ矩阵法。</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-06-17-11-20-56-1686972050686.png\" title=\"\" alt=\"\" data-align=\"center\">\n\n<p>提取光条纹中心线的方法：光强分布拟合法、灰度重心法、方向模板法、Ｈｅｓｓｉａｎ矩阵法。</p>\n"},{"title":"论文整理","date":"2023-03-04T06:39:56.000Z","_content":"\n# 总结\n\nyolo将目标检测定为一个回归问题，可以直接根据检测性能进行端到端优化，因此相较于其他检测系统yolo速度较快。与最先进的检测系统相比，yolo会产生更多的定位误差且在小目标检测上表现较差，但很少在背景上出错，而且对比其他检测方法，yolo的泛化能力更好。\n\n# 目标\n\n人类瞥一眼图像就能知道图像中有什么物体，物体的位置以及他们是如何交互的。而目标检测系统要做的就是不断接近人眼检测的效果，在精度，召回率，速度上统一协调进步。\n\n# 与其他检测系统的比较\n\nR-CNN使用区域建议方法，首先在图像上生成潜在的边界框，然后在这些推荐的框上运行分类器。分类后再进行后处理用于细化边界框，消除重复检测，再根据场景中的其他物体重新预测检测框。这种检测很难优化，因为每个单独的组件都必须单独训练。\n\n对比而言，yolo将目标检测定义为一个单独的回归问题，只需一次就可以在图像中预测物体的存在和位置。\n\n## yolo的优点：\n\n1.速度快\n2.与基于滑动窗口和区域建议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。\n\n快速R-CNN是一种顶级检测方法，它将图像中的背景块误认为是对象，因为它看不到更大的上下文。与Fast R-CNN相比，YOLO产生的背景错误不到一半。\n\n3.泛化能力好，因此在应用于新领域或意外输入时，它不太可能崩溃。\n\n## 缺点：\n\n1.YOLO在精度方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的对象，但它很难精确定位某些对象，尤其是小对象。\n\n# 网络设计\n\n我们用卷积神经网络来实现该模型，并在PASCAL VOC检测数据集上进行评估。网络的初始卷积层提取特征，全连接层输出概率和坐标。我们的网络架构受到图像分类的Googlenet模型的启发，我们的网络有24个卷基层和2个全连接层。\n\n与Lin等人类似，我们只使用1×1缩减层，然后使用3×3卷积层，而不是GoogLeNet使用的初始模块。整个网络如图3所示。\n\n我们还训练了YOLO的快速版本，旨在突破快速目标检测的界限。快速YOLO使用具有较少卷积层（9而不是24）和较少滤波器的神经网络。除了网络的大小，YOLO和快速YOLO之间的所有训练和测试参数都是相同的。\n\n# 训练\n\n我们在ImageNet 1000类竞争数据集上预处理卷积层[29]。对于预训练，我们使用图3中的前20个卷积层，然后是平均池化层和完全连接层。我们对该网络进行了大约一周的培训，并在ImageNet 2012验证集上实现了88%的单作物前5名准确率，与Caffe模型动物园中的GoogLeNet模型相当[24]。\n\n然后我们转换模型以执行检测。Ren等人表明，将卷积层和连接层添加到预训练的网络可以提高性能[28]。\n\n根据他们的示例，我们添加了四个卷积层和两个具有随机初始化权重的完全连接层。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224提高到448×448。\n\n# yolo算法出现的问题与优化：\n\n 1.问题：它将定位误差与可能不理想的分类误差同等加权，并不完全符合我们最大化平均精度的目标。此外，在每个图像中，许多网格单元都不包含任何对象。这将这些单元格的“置信度”分数推向零，通常会超过包含对象的单元格的梯度。这可能会导致模型不稳定，导致训练早期出现分歧。\n\n   解决：增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数λcoord和λnoobj来实现这一点。我们将λcoord设置为5，λnoobj设置为.5。\n\n平方和误差也对大小框中的误差进行加权。我们的误差度量应该反映出大框中的小偏差比小框中的重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。\n\n2.如果以高学习率开始，模型往往会因为不稳定的梯度而偏离。因此可以将学习率缓慢提高。\n\n3.针对过度拟合问题，使用丢弃和大量数据扩充。🙏还未深入研究\n\n损失函数：\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-07-16-53-37-image.png)\n\n# 推论inference：\n\n大的物体或者是靠经多个检测框边缘的物体能够被多个检测框定位到，为了消除这些重复检测，可以使用非最大抑制，可以增加23%的map。\n\nyolo的局限性：\n\n1.由于yolo检测将图片分成了若干个网格，每个网格只能预测两个框，并且只能有一个类。这种空间约束大大限制了模型预测附近物体的数量，比如大规模的鸟，成群的牛羊等等。\n\n2.由于我们的模型会从数据中预测边界框，数据中的一些新的或不寻常的纵横比或规格的图像就很难被采纳。我们的模型只会依靠一般的特征去分辨物体。\n\n3.损失函数对待小边界框和大边界框的错误是相同的，但是按道理说，同一个尺寸的错误，在大边界框是无所谓的，小边界框上却是致命的，因此应该追加权重处理。\n\n# 与其他检测系统的比较\n\n目标检测通道一般是\n\n1.从输入图像中提取一组鲁棒特征\n2.使用分类器或定位器以滑动窗口的方式在整个图像或图像中的一些区域子集上运行。\n\nDPM:\n\nR-CNN:\n\n。。。🙏这部分暂时略过，有缘再写。。。。。\n\n# 实验\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-00-image.png\" alt=\"\" width=\"244\">   <img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-51-image.png\" title=\"\" alt=\"\" width=\"300\">\n\n从上面两张图可以看出，fast R-cnn总体精度较高，但检测背景图错误率较高，而yolo在定位上犯的错误较多。可以将二者结合起来，在fast R-cnn上应用yolo消除部分背景误差，来达到提高精度的目的。🙏此部分在论文中有写，暂时略过。。。。\n\n通过艺术品数据集的训练效果来看，yolo>DPM>Faster R-cnn\n\n在连接摄像头时，yolo的表现也很好，可以实时检测图像，这更像是一种追踪系统，在物体移动和变化时检测物体，由此产生的系统增加了互动性和体验感。\n","source":"_posts/论文整理.md","raw":"---\ntitle: 论文整理\ndate: 2023-03-04 14:39:56\ntags: yolo基础\ncategories: 论文整理\n---\n\n# 总结\n\nyolo将目标检测定为一个回归问题，可以直接根据检测性能进行端到端优化，因此相较于其他检测系统yolo速度较快。与最先进的检测系统相比，yolo会产生更多的定位误差且在小目标检测上表现较差，但很少在背景上出错，而且对比其他检测方法，yolo的泛化能力更好。\n\n# 目标\n\n人类瞥一眼图像就能知道图像中有什么物体，物体的位置以及他们是如何交互的。而目标检测系统要做的就是不断接近人眼检测的效果，在精度，召回率，速度上统一协调进步。\n\n# 与其他检测系统的比较\n\nR-CNN使用区域建议方法，首先在图像上生成潜在的边界框，然后在这些推荐的框上运行分类器。分类后再进行后处理用于细化边界框，消除重复检测，再根据场景中的其他物体重新预测检测框。这种检测很难优化，因为每个单独的组件都必须单独训练。\n\n对比而言，yolo将目标检测定义为一个单独的回归问题，只需一次就可以在图像中预测物体的存在和位置。\n\n## yolo的优点：\n\n1.速度快\n2.与基于滑动窗口和区域建议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。\n\n快速R-CNN是一种顶级检测方法，它将图像中的背景块误认为是对象，因为它看不到更大的上下文。与Fast R-CNN相比，YOLO产生的背景错误不到一半。\n\n3.泛化能力好，因此在应用于新领域或意外输入时，它不太可能崩溃。\n\n## 缺点：\n\n1.YOLO在精度方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的对象，但它很难精确定位某些对象，尤其是小对象。\n\n# 网络设计\n\n我们用卷积神经网络来实现该模型，并在PASCAL VOC检测数据集上进行评估。网络的初始卷积层提取特征，全连接层输出概率和坐标。我们的网络架构受到图像分类的Googlenet模型的启发，我们的网络有24个卷基层和2个全连接层。\n\n与Lin等人类似，我们只使用1×1缩减层，然后使用3×3卷积层，而不是GoogLeNet使用的初始模块。整个网络如图3所示。\n\n我们还训练了YOLO的快速版本，旨在突破快速目标检测的界限。快速YOLO使用具有较少卷积层（9而不是24）和较少滤波器的神经网络。除了网络的大小，YOLO和快速YOLO之间的所有训练和测试参数都是相同的。\n\n# 训练\n\n我们在ImageNet 1000类竞争数据集上预处理卷积层[29]。对于预训练，我们使用图3中的前20个卷积层，然后是平均池化层和完全连接层。我们对该网络进行了大约一周的培训，并在ImageNet 2012验证集上实现了88%的单作物前5名准确率，与Caffe模型动物园中的GoogLeNet模型相当[24]。\n\n然后我们转换模型以执行检测。Ren等人表明，将卷积层和连接层添加到预训练的网络可以提高性能[28]。\n\n根据他们的示例，我们添加了四个卷积层和两个具有随机初始化权重的完全连接层。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224提高到448×448。\n\n# yolo算法出现的问题与优化：\n\n 1.问题：它将定位误差与可能不理想的分类误差同等加权，并不完全符合我们最大化平均精度的目标。此外，在每个图像中，许多网格单元都不包含任何对象。这将这些单元格的“置信度”分数推向零，通常会超过包含对象的单元格的梯度。这可能会导致模型不稳定，导致训练早期出现分歧。\n\n   解决：增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数λcoord和λnoobj来实现这一点。我们将λcoord设置为5，λnoobj设置为.5。\n\n平方和误差也对大小框中的误差进行加权。我们的误差度量应该反映出大框中的小偏差比小框中的重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。\n\n2.如果以高学习率开始，模型往往会因为不稳定的梯度而偏离。因此可以将学习率缓慢提高。\n\n3.针对过度拟合问题，使用丢弃和大量数据扩充。🙏还未深入研究\n\n损失函数：\n\n![](C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-07-16-53-37-image.png)\n\n# 推论inference：\n\n大的物体或者是靠经多个检测框边缘的物体能够被多个检测框定位到，为了消除这些重复检测，可以使用非最大抑制，可以增加23%的map。\n\nyolo的局限性：\n\n1.由于yolo检测将图片分成了若干个网格，每个网格只能预测两个框，并且只能有一个类。这种空间约束大大限制了模型预测附近物体的数量，比如大规模的鸟，成群的牛羊等等。\n\n2.由于我们的模型会从数据中预测边界框，数据中的一些新的或不寻常的纵横比或规格的图像就很难被采纳。我们的模型只会依靠一般的特征去分辨物体。\n\n3.损失函数对待小边界框和大边界框的错误是相同的，但是按道理说，同一个尺寸的错误，在大边界框是无所谓的，小边界框上却是致命的，因此应该追加权重处理。\n\n# 与其他检测系统的比较\n\n目标检测通道一般是\n\n1.从输入图像中提取一组鲁棒特征\n2.使用分类器或定位器以滑动窗口的方式在整个图像或图像中的一些区域子集上运行。\n\nDPM:\n\nR-CNN:\n\n。。。🙏这部分暂时略过，有缘再写。。。。。\n\n# 实验\n\n<img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-00-image.png\" alt=\"\" width=\"244\">   <img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-51-image.png\" title=\"\" alt=\"\" width=\"300\">\n\n从上面两张图可以看出，fast R-cnn总体精度较高，但检测背景图错误率较高，而yolo在定位上犯的错误较多。可以将二者结合起来，在fast R-cnn上应用yolo消除部分背景误差，来达到提高精度的目的。🙏此部分在论文中有写，暂时略过。。。。\n\n通过艺术品数据集的训练效果来看，yolo>DPM>Faster R-cnn\n\n在连接摄像头时，yolo的表现也很好，可以实时检测图像，这更像是一种追踪系统，在物体移动和变化时检测物体，由此产生的系统增加了互动性和体验感。\n","slug":"论文整理","published":1,"updated":"2023-04-26T11:33:04.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clpcnaqdv001mgkva18tc8kgx","content":"<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>yolo将目标检测定为一个回归问题，可以直接根据检测性能进行端到端优化，因此相较于其他检测系统yolo速度较快。与最先进的检测系统相比，yolo会产生更多的定位误差且在小目标检测上表现较差，但很少在背景上出错，而且对比其他检测方法，yolo的泛化能力更好。</p>\n<h1 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h1><p>人类瞥一眼图像就能知道图像中有什么物体，物体的位置以及他们是如何交互的。而目标检测系统要做的就是不断接近人眼检测的效果，在精度，召回率，速度上统一协调进步。</p>\n<h1 id=\"与其他检测系统的比较\"><a href=\"#与其他检测系统的比较\" class=\"headerlink\" title=\"与其他检测系统的比较\"></a>与其他检测系统的比较</h1><p>R-CNN使用区域建议方法，首先在图像上生成潜在的边界框，然后在这些推荐的框上运行分类器。分类后再进行后处理用于细化边界框，消除重复检测，再根据场景中的其他物体重新预测检测框。这种检测很难优化，因为每个单独的组件都必须单独训练。</p>\n<p>对比而言，yolo将目标检测定义为一个单独的回归问题，只需一次就可以在图像中预测物体的存在和位置。</p>\n<h2 id=\"yolo的优点：\"><a href=\"#yolo的优点：\" class=\"headerlink\" title=\"yolo的优点：\"></a>yolo的优点：</h2><p>1.速度快<br>2.与基于滑动窗口和区域建议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。</p>\n<p>快速R-CNN是一种顶级检测方法，它将图像中的背景块误认为是对象，因为它看不到更大的上下文。与Fast R-CNN相比，YOLO产生的背景错误不到一半。</p>\n<p>3.泛化能力好，因此在应用于新领域或意外输入时，它不太可能崩溃。</p>\n<h2 id=\"缺点：\"><a href=\"#缺点：\" class=\"headerlink\" title=\"缺点：\"></a>缺点：</h2><p>1.YOLO在精度方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的对象，但它很难精确定位某些对象，尤其是小对象。</p>\n<h1 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h1><p>我们用卷积神经网络来实现该模型，并在PASCAL VOC检测数据集上进行评估。网络的初始卷积层提取特征，全连接层输出概率和坐标。我们的网络架构受到图像分类的Googlenet模型的启发，我们的网络有24个卷基层和2个全连接层。</p>\n<p>与Lin等人类似，我们只使用1×1缩减层，然后使用3×3卷积层，而不是GoogLeNet使用的初始模块。整个网络如图3所示。</p>\n<p>我们还训练了YOLO的快速版本，旨在突破快速目标检测的界限。快速YOLO使用具有较少卷积层（9而不是24）和较少滤波器的神经网络。除了网络的大小，YOLO和快速YOLO之间的所有训练和测试参数都是相同的。</p>\n<h1 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h1><p>我们在ImageNet 1000类竞争数据集上预处理卷积层[29]。对于预训练，我们使用图3中的前20个卷积层，然后是平均池化层和完全连接层。我们对该网络进行了大约一周的培训，并在ImageNet 2012验证集上实现了88%的单作物前5名准确率，与Caffe模型动物园中的GoogLeNet模型相当[24]。</p>\n<p>然后我们转换模型以执行检测。Ren等人表明，将卷积层和连接层添加到预训练的网络可以提高性能[28]。</p>\n<p>根据他们的示例，我们添加了四个卷积层和两个具有随机初始化权重的完全连接层。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224提高到448×448。</p>\n<h1 id=\"yolo算法出现的问题与优化：\"><a href=\"#yolo算法出现的问题与优化：\" class=\"headerlink\" title=\"yolo算法出现的问题与优化：\"></a>yolo算法出现的问题与优化：</h1><p> 1.问题：它将定位误差与可能不理想的分类误差同等加权，并不完全符合我们最大化平均精度的目标。此外，在每个图像中，许多网格单元都不包含任何对象。这将这些单元格的“置信度”分数推向零，通常会超过包含对象的单元格的梯度。这可能会导致模型不稳定，导致训练早期出现分歧。</p>\n<p>   解决：增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数λcoord和λnoobj来实现这一点。我们将λcoord设置为5，λnoobj设置为.5。</p>\n<p>平方和误差也对大小框中的误差进行加权。我们的误差度量应该反映出大框中的小偏差比小框中的重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。</p>\n<p>2.如果以高学习率开始，模型往往会因为不稳定的梯度而偏离。因此可以将学习率缓慢提高。</p>\n<p>3.针对过度拟合问题，使用丢弃和大量数据扩充。🙏还未深入研究</p>\n<p>损失函数：</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-07-16-53-37-image.png\"></p>\n<h1 id=\"推论inference：\"><a href=\"#推论inference：\" class=\"headerlink\" title=\"推论inference：\"></a>推论inference：</h1><p>大的物体或者是靠经多个检测框边缘的物体能够被多个检测框定位到，为了消除这些重复检测，可以使用非最大抑制，可以增加23%的map。</p>\n<p>yolo的局限性：</p>\n<p>1.由于yolo检测将图片分成了若干个网格，每个网格只能预测两个框，并且只能有一个类。这种空间约束大大限制了模型预测附近物体的数量，比如大规模的鸟，成群的牛羊等等。</p>\n<p>2.由于我们的模型会从数据中预测边界框，数据中的一些新的或不寻常的纵横比或规格的图像就很难被采纳。我们的模型只会依靠一般的特征去分辨物体。</p>\n<p>3.损失函数对待小边界框和大边界框的错误是相同的，但是按道理说，同一个尺寸的错误，在大边界框是无所谓的，小边界框上却是致命的，因此应该追加权重处理。</p>\n<h1 id=\"与其他检测系统的比较-1\"><a href=\"#与其他检测系统的比较-1\" class=\"headerlink\" title=\"与其他检测系统的比较\"></a>与其他检测系统的比较</h1><p>目标检测通道一般是</p>\n<p>1.从输入图像中提取一组鲁棒特征<br>2.使用分类器或定位器以滑动窗口的方式在整个图像或图像中的一些区域子集上运行。</p>\n<p>DPM:</p>\n<p>R-CNN:</p>\n<p>。。。🙏这部分暂时略过，有缘再写。。。。。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-00-image.png\" alt=\"\" width=\"244\">   <img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-51-image.png\" title=\"\" alt=\"\" width=\"300\"></p>\n<p>从上面两张图可以看出，fast R-cnn总体精度较高，但检测背景图错误率较高，而yolo在定位上犯的错误较多。可以将二者结合起来，在fast R-cnn上应用yolo消除部分背景误差，来达到提高精度的目的。🙏此部分在论文中有写，暂时略过。。。。</p>\n<p>通过艺术品数据集的训练效果来看，yolo&gt;DPM&gt;Faster R-cnn</p>\n<p>在连接摄像头时，yolo的表现也很好，可以实时检测图像，这更像是一种追踪系统，在物体移动和变化时检测物体，由此产生的系统增加了互动性和体验感。</p>\n","site":{"data":{"styles":"body {\n  background: url(\"/images/background.jpg\");\n  background-size: cover;\n  background-repeat: no-repeat;\n  background-attachment: fixed;\n  background-position: 50% 50%;\n}\n.content-wrap {\n  opacity: 0.9;\n}\n.sidebar {\n  opacity: 0.9;\n}\n.header-inner {\n  background: rgba(255,255,255,0.9);\n}\n.popup {\n  opacity: 0.9;\n}\n"}},"excerpt":"","more":"<h1 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h1><p>yolo将目标检测定为一个回归问题，可以直接根据检测性能进行端到端优化，因此相较于其他检测系统yolo速度较快。与最先进的检测系统相比，yolo会产生更多的定位误差且在小目标检测上表现较差，但很少在背景上出错，而且对比其他检测方法，yolo的泛化能力更好。</p>\n<h1 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h1><p>人类瞥一眼图像就能知道图像中有什么物体，物体的位置以及他们是如何交互的。而目标检测系统要做的就是不断接近人眼检测的效果，在精度，召回率，速度上统一协调进步。</p>\n<h1 id=\"与其他检测系统的比较\"><a href=\"#与其他检测系统的比较\" class=\"headerlink\" title=\"与其他检测系统的比较\"></a>与其他检测系统的比较</h1><p>R-CNN使用区域建议方法，首先在图像上生成潜在的边界框，然后在这些推荐的框上运行分类器。分类后再进行后处理用于细化边界框，消除重复检测，再根据场景中的其他物体重新预测检测框。这种检测很难优化，因为每个单独的组件都必须单独训练。</p>\n<p>对比而言，yolo将目标检测定义为一个单独的回归问题，只需一次就可以在图像中预测物体的存在和位置。</p>\n<h2 id=\"yolo的优点：\"><a href=\"#yolo的优点：\" class=\"headerlink\" title=\"yolo的优点：\"></a>yolo的优点：</h2><p>1.速度快<br>2.与基于滑动窗口和区域建议的技术不同，YOLO在训练和测试期间看到整个图像，因此它隐式地编码了关于类及其外观的上下文信息。</p>\n<p>快速R-CNN是一种顶级检测方法，它将图像中的背景块误认为是对象，因为它看不到更大的上下文。与Fast R-CNN相比，YOLO产生的背景错误不到一半。</p>\n<p>3.泛化能力好，因此在应用于新领域或意外输入时，它不太可能崩溃。</p>\n<h2 id=\"缺点：\"><a href=\"#缺点：\" class=\"headerlink\" title=\"缺点：\"></a>缺点：</h2><p>1.YOLO在精度方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的对象，但它很难精确定位某些对象，尤其是小对象。</p>\n<h1 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h1><p>我们用卷积神经网络来实现该模型，并在PASCAL VOC检测数据集上进行评估。网络的初始卷积层提取特征，全连接层输出概率和坐标。我们的网络架构受到图像分类的Googlenet模型的启发，我们的网络有24个卷基层和2个全连接层。</p>\n<p>与Lin等人类似，我们只使用1×1缩减层，然后使用3×3卷积层，而不是GoogLeNet使用的初始模块。整个网络如图3所示。</p>\n<p>我们还训练了YOLO的快速版本，旨在突破快速目标检测的界限。快速YOLO使用具有较少卷积层（9而不是24）和较少滤波器的神经网络。除了网络的大小，YOLO和快速YOLO之间的所有训练和测试参数都是相同的。</p>\n<h1 id=\"训练\"><a href=\"#训练\" class=\"headerlink\" title=\"训练\"></a>训练</h1><p>我们在ImageNet 1000类竞争数据集上预处理卷积层[29]。对于预训练，我们使用图3中的前20个卷积层，然后是平均池化层和完全连接层。我们对该网络进行了大约一周的培训，并在ImageNet 2012验证集上实现了88%的单作物前5名准确率，与Caffe模型动物园中的GoogLeNet模型相当[24]。</p>\n<p>然后我们转换模型以执行检测。Ren等人表明，将卷积层和连接层添加到预训练的网络可以提高性能[28]。</p>\n<p>根据他们的示例，我们添加了四个卷积层和两个具有随机初始化权重的完全连接层。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224提高到448×448。</p>\n<h1 id=\"yolo算法出现的问题与优化：\"><a href=\"#yolo算法出现的问题与优化：\" class=\"headerlink\" title=\"yolo算法出现的问题与优化：\"></a>yolo算法出现的问题与优化：</h1><p> 1.问题：它将定位误差与可能不理想的分类误差同等加权，并不完全符合我们最大化平均精度的目标。此外，在每个图像中，许多网格单元都不包含任何对象。这将这些单元格的“置信度”分数推向零，通常会超过包含对象的单元格的梯度。这可能会导致模型不稳定，导致训练早期出现分歧。</p>\n<p>   解决：增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数λcoord和λnoobj来实现这一点。我们将λcoord设置为5，λnoobj设置为.5。</p>\n<p>平方和误差也对大小框中的误差进行加权。我们的误差度量应该反映出大框中的小偏差比小框中的重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。</p>\n<p>2.如果以高学习率开始，模型往往会因为不稳定的梯度而偏离。因此可以将学习率缓慢提高。</p>\n<p>3.针对过度拟合问题，使用丢弃和大量数据扩充。🙏还未深入研究</p>\n<p>损失函数：</p>\n<p><img src=\"C:\\Users\\HP\\AppData\\Roaming\\marktext\\images\\2023-03-07-16-53-37-image.png\"></p>\n<h1 id=\"推论inference：\"><a href=\"#推论inference：\" class=\"headerlink\" title=\"推论inference：\"></a>推论inference：</h1><p>大的物体或者是靠经多个检测框边缘的物体能够被多个检测框定位到，为了消除这些重复检测，可以使用非最大抑制，可以增加23%的map。</p>\n<p>yolo的局限性：</p>\n<p>1.由于yolo检测将图片分成了若干个网格，每个网格只能预测两个框，并且只能有一个类。这种空间约束大大限制了模型预测附近物体的数量，比如大规模的鸟，成群的牛羊等等。</p>\n<p>2.由于我们的模型会从数据中预测边界框，数据中的一些新的或不寻常的纵横比或规格的图像就很难被采纳。我们的模型只会依靠一般的特征去分辨物体。</p>\n<p>3.损失函数对待小边界框和大边界框的错误是相同的，但是按道理说，同一个尺寸的错误，在大边界框是无所谓的，小边界框上却是致命的，因此应该追加权重处理。</p>\n<h1 id=\"与其他检测系统的比较-1\"><a href=\"#与其他检测系统的比较-1\" class=\"headerlink\" title=\"与其他检测系统的比较\"></a>与其他检测系统的比较</h1><p>目标检测通道一般是</p>\n<p>1.从输入图像中提取一组鲁棒特征<br>2.使用分类器或定位器以滑动窗口的方式在整个图像或图像中的一些区域子集上运行。</p>\n<p>DPM:</p>\n<p>R-CNN:</p>\n<p>。。。🙏这部分暂时略过，有缘再写。。。。。</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><p><img title=\"\" src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-00-image.png\" alt=\"\" width=\"244\">   <img src=\"file:///C:/Users/HP/AppData/Roaming/marktext/images/2023-03-04-14-44-51-image.png\" title=\"\" alt=\"\" width=\"300\"></p>\n<p>从上面两张图可以看出，fast R-cnn总体精度较高，但检测背景图错误率较高，而yolo在定位上犯的错误较多。可以将二者结合起来，在fast R-cnn上应用yolo消除部分背景误差，来达到提高精度的目的。🙏此部分在论文中有写，暂时略过。。。。</p>\n<p>通过艺术品数据集的训练效果来看，yolo&gt;DPM&gt;Faster R-cnn</p>\n<p>在连接摄像头时，yolo的表现也很好，可以实时检测图像，这更像是一种追踪系统，在物体移动和变化时检测物体，由此产生的系统增加了互动性和体验感。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"clpcnaqcq0003gkvago921bgs","category_id":"clpcnaqcw0005gkva984539db","_id":"clpcnaqd6000dgkva7pydbij4"},{"post_id":"clpcnaqd20008gkva3rfv7t1n","category_id":"clpcnaqd5000bgkva48vp6gjk","_id":"clpcnaqdb000ngkvaep2zbxps"},{"post_id":"clpcnaqd5000cgkva6771a7i6","category_id":"clpcnaqd8000igkvab3wlgxs4","_id":"clpcnaqdg000ugkva8jdyde9h"},{"post_id":"clpcnaqd7000hgkva467sh5cb","category_id":"clpcnaqdb000ogkva5td9gk86","_id":"clpcnaqdj0012gkvac9gl01sq"},{"post_id":"clpcnaqd9000jgkvabquq340k","category_id":"clpcnaqdb000ogkva5td9gk86","_id":"clpcnaqdl0016gkvag1586bm0"},{"post_id":"clpcnaqdf000tgkva38iiguqf","category_id":"clpcnaqdj0011gkva6tpf37jp","_id":"clpcnaqdp001cgkva5cjza3cb"},{"post_id":"clpcnaqdh000xgkva4do9b6vl","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdu001kgkva53zo49fm"},{"post_id":"clpcnaqdp001dgkva3cnf41j9","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdv001ngkvabyb4g7ie"},{"post_id":"clpcnaqdq001ggkva91tw54p3","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdw001ogkva9ecp371b"},{"post_id":"clpcnaqdk0014gkvafdsn5uc9","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdx001qgkvaesedfxzk"},{"post_id":"clpcnaqdv001mgkva18tc8kgx","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdx001sgkvafugr0tqj"},{"post_id":"clpcnaqdm0019gkva6dembnw0","category_id":"clpcnaqdl0018gkvaa2w8di18","_id":"clpcnaqdx001tgkva06vkguk8"}],"PostTag":[{"post_id":"clpcnaqd00007gkvab0512fl0","tag_id":"clpcnaqd30009gkvahhncalsw","_id":"clpcnaqd7000ggkva14boav21"},{"post_id":"clpcnaqd7000hgkva467sh5cb","tag_id":"clpcnaqd30009gkvahhncalsw","_id":"clpcnaqda000lgkvahpkcf25m"},{"post_id":"clpcnaqd20008gkva3rfv7t1n","tag_id":"clpcnaqd7000fgkva9woga2nq","_id":"clpcnaqdc000pgkva44eh4a5q"},{"post_id":"clpcnaqd9000jgkvabquq340k","tag_id":"clpcnaqd30009gkvahhncalsw","_id":"clpcnaqdd000sgkvaanrecxa6"},{"post_id":"clpcnaqd4000agkva5y3v8ffy","tag_id":"clpcnaqda000kgkvahr1pc185","_id":"clpcnaqdg000wgkva9wa429v3"},{"post_id":"clpcnaqd6000egkvad63r61ok","tag_id":"clpcnaqda000kgkvahr1pc185","_id":"clpcnaqdj0010gkva0vom3q7c"},{"post_id":"clpcnaqdf000tgkva38iiguqf","tag_id":"clpcnaqdi000ygkva0xj316ik","_id":"clpcnaqdl0017gkva0pyxevk7"},{"post_id":"clpcnaqdh000xgkva4do9b6vl","tag_id":"clpcnaqdl0015gkva8esm6tnv","_id":"clpcnaqdq001egkvaciv2haud"},{"post_id":"clpcnaqdk0014gkvafdsn5uc9","tag_id":"clpcnaqdo001bgkva2t06cm0d","_id":"clpcnaqdu001lgkva7batfnms"},{"post_id":"clpcnaqdm0019gkva6dembnw0","tag_id":"clpcnaqds001hgkva5kn1gx9t","_id":"clpcnaqdx001pgkvac1rva3ou"},{"post_id":"clpcnaqdv001mgkva18tc8kgx","tag_id":"clpcnaqd30009gkvahhncalsw","_id":"clpcnaqdx001rgkva49qv0p8p"}],"Tag":[{"name":"yolo基础","_id":"clpcnaqd30009gkvahhncalsw"},{"name":"打卡","_id":"clpcnaqd7000fgkva9woga2nq"},{"name":"三维重建","_id":"clpcnaqda000kgkvahr1pc185"},{"name":"aplayer leancloud","_id":"clpcnaqdi000ygkva0xj316ik"},{"name":"带钢缺陷检测","_id":"clpcnaqdl0015gkva8esm6tnv"},{"name":"深度学习基础","_id":"clpcnaqdo001bgkva2t06cm0d"},{"name":"多标签算法","_id":"clpcnaqds001hgkva5kn1gx9t"}]}}